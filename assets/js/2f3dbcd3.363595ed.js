"use strict";(self.webpackChunkdrkapichu=self.webpackChunkdrkapichu||[]).push([[1261],{3905:function(e,n,t){t.d(n,{Zo:function(){return c},kt:function(){return m}});var i=t(7294);function s(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){s(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,s=function(e,n){if(null==e)return{};var t,i,s={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(s[t]=e[t]);return s}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var o=i.createContext({}),u=function(e){var n=i.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},c=function(e){var n=u(e.components);return i.createElement(o.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},d=i.forwardRef((function(e,n){var t=e.components,s=e.mdxType,a=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=u(t),m=s,g=d["".concat(o,".").concat(m)]||d[m]||p[m]||a;return t?i.createElement(g,r(r({ref:n},c),{},{components:t})):i.createElement(g,r({ref:n},c))}));function m(e,n){var t=arguments,s=n&&n.mdxType;if("string"==typeof e||s){var a=t.length,r=new Array(a);r[0]=d;var l={};for(var o in n)hasOwnProperty.call(n,o)&&(l[o]=n[o]);l.originalType=e,l.mdxType="string"==typeof e?e:s,r[1]=l;for(var u=2;u<a;u++)r[u]=t[u];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}d.displayName="MDXCreateElement"},953:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return l},contentTitle:function(){return o},metadata:function(){return u},assets:function(){return c},toc:function(){return p},default:function(){return m}});var i=t(3117),s=t(102),a=(t(7294),t(3905)),r=["components"],l={slug:"Des-biais-cognitifs-aux-biais-ethiques",title:"Comment nos pr\xe9jug\xe9s deviennent un probl\xe8me \xe9thique pour une intelligence artificielle ?",author:"St\xe9phanie Lehuger",author_title:"Thinker et entrepreneuse",author_url:"mailto:contact@datacraft.paris",header_image_url:"img/blog/pikachu.png",tags:["tag1","tag2","tag3",4],description:"Here goes a description",keywords:["keyword1","keyword2","keyword3"]},o=void 0,u={permalink:"/blog/Des-biais-cognitifs-aux-biais-ethiques",editUrl:"https://github.com/DrKapichu/drkapichu.github.io/tree/main/blog/blog/2022-04-08-AIBiases.md",source:"@site/blog/2022-04-08-AIBiases.md",title:"Comment nos pr\xe9jug\xe9s deviennent un probl\xe8me \xe9thique pour une intelligence artificielle ?",description:"Here goes a description",date:"2022-04-08T00:00:00.000Z",formattedDate:"April 8, 2022",tags:[{label:"tag1",permalink:"/blog/tags/tag-1"},{label:"tag2",permalink:"/blog/tags/tag-2"},{label:"tag3",permalink:"/blog/tags/tag-3"},{label:"4",permalink:"/blog/tags/4"}],readingTime:6.8,truncated:!0,authors:[{name:"St\xe9phanie Lehuger",title:"Thinker et entrepreneuse",url:"mailto:contact@datacraft.paris"}],nextItem:{title:"Fairness in AI - How a benchathon unlocked our knowledge",permalink:"/blog/AI-Fairness-et-benchathon2"}},c={authorsImageUrls:[void 0]},p=[{value:"Pourquoi les biais posent des probl\xe8mes \xe9thiques en intelligence artificielle ?",id:"pourquoi-les-biais-posent-des-probl\xe8mes-\xe9thiques-en-intelligence-artificielle-",children:[{value:"Nous avons tous des biais",id:"nous-avons-tous-des-biais",children:[],level:3},{value:"Pourquoi les biais d\u2019une IA sont un probl\xe8me ?",id:"pourquoi-les-biais-dune-ia-sont-un-probl\xe8me-",children:[],level:3},{value:"Personne n\u2019est \xe0 l\u2019abri de risques \xe9thiques",id:"personne-nest-\xe0-labri-de-risques-\xe9thiques",children:[],level:3},{value:"La solution propos\xe9e par la Commission europ\xe9enne aux probl\xe8mes d\u2019\xe9thique de l\u2019IA",id:"la-solution-propos\xe9e-par-la-commission-europ\xe9enne-aux-probl\xe8mes-d\xe9thique-de-lia",children:[],level:3},{value:"Le travail de datacraft",id:"le-travail-de-datacraft",children:[],level:3}],level:2}],d={toc:p};function m(e){var n=e.components,t=(0,s.Z)(e,r);return(0,a.kt)("wrapper",(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("hr",null),(0,a.kt)("h1",{id:"comment-nos-pr\xe9jug\xe9s-deviennent-un-probl\xe8me-\xe9thique-pour-une-intelligence-artificielle-"},"Comment nos pr\xe9jug\xe9s deviennent un probl\xe8me \xe9thique pour une intelligence artificielle ?"),(0,a.kt)("h2",{id:"pourquoi-les-biais-posent-des-probl\xe8mes-\xe9thiques-en-intelligence-artificielle-"},"Pourquoi les biais posent des probl\xe8mes \xe9thiques en intelligence artificielle ?"),(0,a.kt)("h3",{id:"nous-avons-tous-des-biais"},"Nous avons tous des biais"),(0,a.kt)("p",null,"Imaginez que vous fa\xeetes vos courses avec une personne aveugle. Vous arrivez devant un rayon de bananes et la personne vous demande ce que vous voyez. Qu\u2019allez-vous lui d\xe9crire ? Il y a des bananes ? Il y a des bananes avec de petits autocollants dessus ? Il y a environ 30 bananes ? Il y a peu de chances pour que vous disiez qu\u2019il y a des bananes qui sont de couleur jaune. Le jaune est une information typique pour une banane et on a tendance \xe0 ne pas mentionner les \xe9vidences. C'est une forme de biais, ou d'angle mort cognitif. Et nous avons tous des ",(0,a.kt)("a",{parentName:"p",href:"https://fr.wikipedia.org/wiki/Biais_cognitif"},"biais cognitifs"),", c\u2019est humain."),(0,a.kt)("h3",{id:"pourquoi-les-biais-dune-ia-sont-un-probl\xe8me-"},"Pourquoi les biais d\u2019une IA sont un probl\xe8me ?"),(0,a.kt)("p",null,"Nos biais humains sont un probl\xe8me quand on les transmet \xe0 une intelligence artificielle. C\u2019est un probl\xe8me aussi bien \xe9thique qu\u2019en termes de performance du logiciel bas\xe9 sur ces biais. Concr\xe8tement, en utilisant la technique de l\u2019apprentissage machine, on va entra\xeener l\u2019intelligence artificielle \xe0 reconna\xeetre des ",(0,a.kt)("a",{parentName:"p",href:"https://www.blog.google/technology/ai/new-course-teach-people-about-fairness-machine-learning/"},"bananes")," en lui montrant des images de bananes. Si toutes les images que je lui montre sont des bananes jaunes, le jour o\xf9 l\u2019intelligence artificielle va voir une banane verte, elle ne va pas penser que c\u2019est une banane. Il y a des chances qu\u2019elle l\u2019associe plut\xf4t \xe0 une autre chose verte qu\u2019elle a d\xe9j\xe0 vue, comme un concombre par exemple."),(0,a.kt)("p",null,"\xc9thiquement, c\u2019est un probl\xe8me. Parce que, quand on parle de bananes, cela para\xeet anodin. Mais cela peut parfois conduire \xe0 des cons\xe9quences graves. En effet, m\xeame un humain bienveillant a des pr\xe9jug\xe9s inconscients. Parfois, sans s\u2019en rendre compte, des data scientists peuvent transmettre leurs biais au travers des donn\xe9es qu\u2019ils s\xe9lectionnent pour construire des logiciels. Prenons l\u2019exemple d\u2019un syst\xe8me de vid\xe9osurveillance automatis\xe9e utilis\xe9 par la police. La Chine est friande de ces syst\xe8mes pr\xe9dictifs pour interpeler des suspects, comme l\u2019a d\xe9nonc\xe9 ",(0,a.kt)("a",{parentName:"p",href:"https://www.hrw.org/news/2021/11/24/mass-surveillance-fuels-oppression-uyghurs-and-palestinians"},"Human Rights Watch")," concernant l\u2019usage du syst\xe8me de surveillance de masse IJOP (Integrated Joint Operation Platform). Imaginons de mani\xe8re caricaturale que le logiciel a appris \xe0 reconna\xeetre des personnes \xe0 partir d\u2019un jeu de donn\xe9es compos\xe9 \xe0 90% de portraits d'hommes \xe0 la peau blanche. L\u2019algorithme n\u2019\xe9tant pas entra\xeen\xe9 \xe0 reconna\xeetre des femmes \xe0 la peau noire, il sera alors compl\xe8tement inefficace sur cette population et pourrait mener \xe0 des erreurs judiciaires s\u2019il \xe9tait utilis\xe9. Ainsi, le ",(0,a.kt)("a",{parentName:"p",href:"https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm"},"logiciel de pr\xe9diction des r\xe9cidives")," COMPAS utilis\xe9 par des juges am\xe9ricains pour les assister dans leur verdict a surestim\xe9 syst\xe9matiquement le risque de r\xe9cidive des d\xe9tenus afro-am\xe9ricains tandis que celui des blancs a \xe9t\xe9 tr\xe8s sous-estim\xe9."),(0,a.kt)("p",null,"Un algorithme fonctionne comme une recette de cuisine : avec des ingr\xe9dients, les donn\xe9es, et une recette, le code. M\xeame si c\u2019est la partie la plus visible, la r\xe9ussite de la recette ne d\xe9pend pas tant du code que de la qualit\xe9 des ingr\xe9dients utilis\xe9s, les donn\xe9es. S\u2019ils sont de mauvaise qualit\xe9, quelle que soit la recette, le succ\xe8s ne pourra pas \xeatre au rendez-vous. Les r\xe9sultats des algorithmes ne d\xe9pendent donc pas que de la mani\xe8re dont les programmeurs les ont \xe9crits. Les biais ont des origines diverses. Les biais d'acquisition de donn\xe9es par exemple sont li\xe9s \xe0 la mani\xe8re dont les donn\xe9es sont collect\xe9es. Par exemple, si on estime la moyenne mensuelle de clients d\u2019un h\xf4tel dans une station de ski en se basant uniquement sur les chiffres des mois d\u2019hiver, on va surestimer le r\xe9sultat. Le contexte est essentiel dans l\u2019acquisition de donn\xe9es parce que, si la p\xe9riode de l'ann\xe9e \xe0 laquelle on collecte des donn\xe9es n'est pas repr\xe9sentative de l'ann\xe9e enti\xe8re, le r\xe9sultat sera fauss\xe9. Nos biais repr\xe9sentent un probl\xe8me quand ils entra\xeenent des cons\xe9quences discriminatoires mais ils sont le plus souvent positifs. En effet, dans le cas normal, nos biais sont aussi ce qui nous permet d'avancer et de prendre des d\xe9cisions rapides en environnement inconnu, transmettant ainsi des informations b\xe9n\xe9fiques aux algorithmes."),(0,a.kt)("p",null,"C\u2019est donc \xe0 la fois un probl\xe8me \xe9thique et un probl\xe8me de performance des algorithmes. Imaginons que la police d\xe9cide du quartier o\xf9 elle patrouille sur les suggestions d\u2019une intelligence artificielle. Si le logiciel dispose de l\u2019historique d\u2019actes de d\xe9linquance de deux quartiers, l\u2018un avec un petit peu plus de d\xe9linquance que l\u2019autre, il va orienter les policiers vers le quartier o\xf9 il y en a eu un peu plus dans le pass\xe9. En arrivant dans ce quartier, si les policiers constatent une infraction, ils vont approvisionner la base d\u2019apprentissage de nouvelles donn\xe9es. Alors que, s\u2019ils avaient \xe9t\xe9 dans l\u2019autre quartier, ils auraient peut-\xeatre aussi constat\xe9 une infraction. Mais maintenant, l\u2019intelligence artificielle est biais\xe9e parce qu\u2019elle consid\xe8re que ce quartier conna\xeet plus de d\xe9linquance. Avec ce biais, les algorithmes peuvent ainsi former ce qu\u2019on appelle des \u201cboucles de r\xe9troaction\u201d par lesquelles st\xe9r\xe9otypes, ",(0,a.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"},"discriminations")," et in\xe9galit\xe9s se renforcent mutuellement, contribuant ainsi \xe0 cristalliser durablement des situations d\u2019in\xe9galit\xe9. En plus de ne pas \xeatre performante, l\u2019intelligence artificielle n\u2019est alors pas non plus \xe9thique. C\u2019est ainsi qu\u2019un ",(0,a.kt)("a",{parentName:"p",href:"https://www.courthousenews.com/audit-finds-lapd-predictive-policing-programs-lack-oversight/"},"audit critique")," du logiciel de police pr\xe9dictive de la ville de Los Angeles, PredPol, a amen\xe9 \xe0 abandonner son usage."),(0,a.kt)("h3",{id:"personne-nest-\xe0-labri-de-risques-\xe9thiques"},"Personne n\u2019est \xe0 l\u2019abri de risques \xe9thiques"),(0,a.kt)("p",null,"Les m\xe9dias rapportent les cas les plus sensationnalistes, comme celui d\u2019Amazon par exemple. Un ",(0,a.kt)("a",{parentName:"p",href:"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G"},"programme informatique")," discriminait les femmes \xe0 l'embauche parce que les informations sur lesquelles l'intelligence artificielle avait bas\xe9 son apprentissage \xe9taient des effectifs historiquement masculins de d\xe9veloppeurs informatiques."),(0,a.kt)("p",null,"Il existe un risque pour les professionnels qui cr\xe9ent des intelligences artificielles de se sentir \xe9loign\xe9s de ces sujets sensationnalistes et, ne se sentant pas concern\xe9s, de ne pas faire attention aux biais dans leur travail. Par exemple, les \xe9quipes de Twitter qui ont travaill\xe9 sur l\u2019algorithme qui choisit quelle partie d\u2019une photo s\u2019affiche en aper\xe7u sur Twitter n\u2019ont pas bien contr\xf4l\xe9 les biais dans leur travail. Leur logiciel recadrait simplement l\u2019image d\u2019un utilisateur pour donner un aper\xe7u pertinent de l'image. Pourtant, les ",(0,a.kt)("a",{parentName:"p",href:"https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm"},"utilisateurs ont remarqu\xe9")," que, sur une photo avec plusieurs personnes, ce recadrage se focalisait plus sur les personnes blanches que les personnes noires. En effet, quand on se base sur les images pr\xe9sentes sur internet, les personnes blanches sont en moyenne plus mises en avant que les personnes noires.",(0,a.kt)("br",{parentName:"p"}),"\n","Il est donc recommand\xe9 \xe0 toute personne travaillant sur une intelligence artificielle de toujours s\u2019interroger sur les biais possibles qu\u2019elle est susceptible d\u2019y introduire, aussi peu risqu\xe9 que le projet puisse para\xeetre."),(0,a.kt)("h3",{id:"la-solution-propos\xe9e-par-la-commission-europ\xe9enne-aux-probl\xe8mes-d\xe9thique-de-lia"},"La solution propos\xe9e par la Commission europ\xe9enne aux probl\xe8mes d\u2019\xe9thique de l\u2019IA"),(0,a.kt)("p",null,"Aujourd\u2019hui, une des rares entit\xe9s qui r\xe9gule l\u2019\xe9thique est la Commission europ\xe9enne avec une ",(0,a.kt)("a",{parentName:"p",href:"https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai"},"proposition de loi qui r\xe9glemente l\u2019intelligence artificielle"),". A l\u2019instar de la mise en \u0153uvre du r\xe8glement g\xe9n\xe9ral sur la protection des donn\xe9es (RGPD), de nombreuses entreprises se plaignent que la Commission europ\xe9enne complique la vie des ing\xe9nieurs parce qu\u2019elle ne comprend pas les implications techniques de ce qu\u2019elle ordonne. Du point de vue du citoyen toutefois, la r\xe9glementation prot\xe8ge la vie priv\xe9e mieux qu\u2019avant. D\u2019autant que, ces derni\xe8res ann\xe9es, minimiser le risque d'un bad buzz li\xe9 \xe0 une violation de la vie priv\xe9e est peut-\xeatre devenu plus important pour les soci\xe9t\xe9s que les contraintes techniques que le RGPD engendre et a \xe9t\xe9 le catalyseur de bonnes pratiques pour \xe9viter ce type de risque."),(0,a.kt)("h3",{id:"le-travail-de-datacraft"},"Le travail de datacraft"),(0,a.kt)("p",null,"Des membres du Club ",(0,a.kt)("a",{parentName:"p",href:"https://www.linkedin.com/company/datacraft-paris/"},"datacraft")," se r\xe9unissent chaque mois pour apporter des r\xe9ponses concr\xe8tes sur les questions d\u2019IA \xe9thique. Ces data scientists issus d\u2019entreprises, de labos de recherche, ou freelance en r\xe9sidence datacraft (Antoine Isnardy de Danone, Th\xe9o Alves Da Costa d\u2019Ekimetrics, Nathan Noiry de T\xe9l\xe9com Paris, Eliot Moll de l\u2019Inria, Yann Girard d\u2019HephIA\u2026) ont par exemple formalis\xe9 les fondements d\u2019une IA de confiance et mis un accent particulier sur sa dimension \xe9thique. C\u2019est ainsi qu\u2019ils ont abouti \xe0 la r\xe9daction d\u2019une ",(0,a.kt)("a",{parentName:"p",href:"https://datacraft.paris/project/trustworthy-ai-charter/"},"charte \xe9thique"),". Le groupe a travaill\xe9 et travaille encore sur la ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/datacraft-paris/ethical-ai-toolkit"},"cartographie")," des outils open source du march\xe9 pour limiter les biais dans les mod\xe8les d\u2019intelligence artificielle (tels que, entre autres, les biais sociaux comme les biais sur le genre, l\u2019\xe2ge, \u2026). Il est possible de contribuer \xe0 cette r\xe9flexion en ",(0,a.kt)("a",{parentName:"p",href:"https://datacraft.paris/join-us/"},"rejoignant datacraft"),"."))}m.isMDXComponent=!0}}]);