"use strict";(self.webpackChunkdrkapichu=self.webpackChunkdrkapichu||[]).push([[477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"draft","metadata":{"permalink":"/blog/draft","editUrl":"https://github.com/facebook/docusaurus/edit/master/website/blog/blog/2022-02-04-MindshakeTime.md","source":"@site/blog/2022-02-04-MindshakeTime.md","title":"Few Shot Learning - application de la m\xe9thode iPET","description":"Draft of the first blog","date":"2022-02-04T00:00:00.000Z","formattedDate":"February 4, 2022","tags":[{"label":"Few Shot Learning","permalink":"/blog/tags/few-shot-learning"},{"label":"PET","permalink":"/blog/tags/pet"},{"label":"iPET","permalink":"/blog/tags/i-pet"},{"label":"semi-supervised","permalink":"/blog/tags/semi-supervised"}],"readingTime":7.95,"truncated":true,"authors":[{"name":"Kapichu","title":"astrophysicist","url":"mailto:julien.guyot@datacraft.paris"}],"nextItem":{"title":"My first GitHub blog! (strongly inspired by Ekimetrics\'...)","permalink":"/blog/welcome"}},"content":"\x3c!--truncate--\x3e\\n\\n\\n([source](https://numenta.com/blog/2019/08/30/case-for-sparsity-in-neural-networks-part-1-pruning) de l\'image de pr\xe9sentaion)\\n\\n\\n## Qu\u2019est ce que le Few Shot Learning (FSL) ? - titre alternatif : Sujet du jour : le Few Shot Learning (FSL)\\n\\nIl est bien connu que la puissance des m\xe9thodes de Machine Learning supervis\xe9es, et plus particuli\xe8rement de Deep Learning avec les r\xe9seaux de neurones, depuis le d\xe9but des ann\xe9es 2000, a repos\xe9 sur la constitution de **grands jeux de donn\xe9es labellis\xe9s**. Deux \xe9l\xe9ments sont importants ici : \u2018grands\u2019 et \u2018labellis\xe9s\u2019.\\n\\nPour le premier point, \xe7a repr\xe9sente par exemple des milliers, voire des millions d\u2019images pour la Computer Vision et des millions d\u2019ensembles de phrases pour le NLP. Concernant le second point, il signifie qu\u2019au cours de son apprentissage, l\u2019ordinateur compare son \xe9valuation des donn\xe9es avec le label qu\u2019un intervenant humain a associ\xe9 \xe0 chaque donn\xe9e.\\n\\nDans le cas du **Few Shot Learning (FSL)**, les chercheurs veulent cr\xe9er des m\xe9thodes capables d\u2019apprendre avec peu de donn\xe9es, i.e. des dizaines ou des centaines, ce qui repr\xe9sente un gain de temps et d\u2019\xe9nergie, tout en conservant des performances \xe9quivalentes aux mod\xe8les traditionnels bien s\xfbr. C\u2019est pourquoi en fran\xe7ais on parle d\u2019**apprentissage frugal**. Toutefois, en pratique les m\xe9thodes de FSL prennent un mod\xe8le traditionnel, pr\xe9-entra\xeen\xe9 sur un grand nombre de donn\xe9es, et elles le sp\xe9cialisent sur le cas d\u2019usage via une courte phase d\u2019apprentissage sur le petit jeu de donn\xe9es \xe0 disposition ; c\u2019est du fine-tuning. Mais en plus, le Few Shot c\u2019est une m\xe9thode qui va au-del\xe0 des m\xe9thodes traditionnelles, elle permet de faire du semi-supervis\xe9, c\u2019est ce qu\u2019on va voir avec le cas d\u2019usage.\\n\\n\\n## Le cas d\u2019usage - titre alternatif : C\u2019est quoi le probl\xe8me ?!\\n\\nEkimetrics s\u2019est int\xe9ress\xe9 \xe0 l\u2019apprentissage frugal pour exploiter les \xe9normes jeux de donn\xe9es des petits commentaires quotidiens sur internet, avec une probl\xe9matique de gain de temps\u2026 De la frugalit\xe9 avec des \xe9normes jeux de donn\xe9es ? On vous explique !\\n\\nMieux que le seul nombre d\u2019\xe9toiles d\u2019un restaurant ou d\u2019un h\xf4tel, il s\u2019agit de prendre en compte les avis dans les tweets, les posts, les br\xe8ves\u2026 qui sont par essence des donn\xe9es non labellis\xe9es et de les exploiter. L\u2019annotation humaine de ces avis est inenvisageable. \xc7a co\xfbterait trop cher, \xe7a prendrait trop de temps, et il faudrait recommencer tous les jours pour suivre l\u2019\xe9volution du sentiment. En l\'occurrence, pour la recherche d\u2019Ekimetrics, le sujet d\u2019\xe9tude porte sur des commentaires de restaurants.\\n\\nMais si la machine \xe9tait capable d\u2019\xe9valuer les commentaires, \xe0 2 Gigahertz, tout de suite le probl\xe8me serait r\xe9gl\xe9. C\u2019est l\xe0 que le Few Shot, en utilisant la m\xe9thode PET, peut devenir utile.\\n\\nDans la suite, nous vous pr\xe9sentons la m\xe9thode PET, comment l\u2019utiliser dans le cadre du FSL et enfin, comment Ekimetrics l\u2019utilise sur les avis des consommateurs.\\n\\n\\n## PET qu\u2019est ce que c\u2019est ?\\n\\n**PET** est l\u2019acronyme de \u2018**Pattern Exploiting Training**\u2019. La m\xe9thode repose sur un ensemble fixe et pr\xe9d\xe9fini de **patterns** et de **verbalizers** et un **Pre-trained Language Model** a.k.a. **PLM**. Les patterns sont les phrases \xe0 trou (\u201cIt was\u2026\u201d, \u201cJust\u2026!\u201d, \u201cAll in all, it was\u2026\u201d, \u201cIn summary, the restaurant is\u2026\u201d) et les verbalizers sont les mots qui peuvent compl\xe9ter ces phrases et auxquels sont associ\xe9es des notes chiffr\xe9es. On commence \xe0 retrouver les nombres que l\u2019ordinateur aime tant !\\n\\nConcr\xe8tement, reprenons notre exemple des \xe9valuations des restaurants, la m\xe9thode consiste \xe0 :\\n - prendre un commentaire,\\n - y associer al\xe9atoirement un pattern,\\n - soumettre le tout au PLM qui va le compl\xe9ter en choisissant un verbalizer.\\n\\n![](./img/2022-02-04-MindshakeTime/PET.png)\\n\\nPar exemple (voir Fig. 1), avec le commentaire \u201cBest pizza ever!\u201d, on construit la phrase \xe0 trou : \u201cBest pizza ever! It was \u2026 .\u201d que le PLM va compl\xe9ter avec \u2018great\u2019 avec une confiance de 0.8, sachant que ce mot est not\xe9 +1.\\n\\n\\n## FSL + PET : premi\xe8re application aux avis internet\\n\\nRevenons \xe0 la masse brute des avis des consommateurs sur internet. **PET est la m\xe9thode** pour associer une note \xe0 un commentaire, le **FSL est le moyen** de traiter automatiquement tout le jeu de donn\xe9es, et le travail de l\u2019algorithme se fait en deux \xe9tapes.\\n\\nDans un premier temps, on labellise un petit nombre de commentaires, une centaine par exemple, ce qui signifie qu\u2019on associe une paire pattern plus verbalizer \xe0 ces commentaires, et on finetune le PLM avec cette centaine. Puis, une fois le PLM sp\xe9cialis\xe9, on le laisse labelliser tout le reste du jeu de donn\xe9es, automatiquement. \xc7a en fait une m\xe9thode semi-supervis\xe9e d\u2019analyse de sentiment des commentaires.\\n\\nCependant, cette application basique pr\xe9sente des limites. D\u2019une part, le verbalizer donn\xe9 par le PLM peut ne pas \xeatre le plus adapt\xe9 au commentaire et, d\u2019autre part, c\u2019est tr\xe8s ambitieux de sp\xe9cialiser le PLM une fois sur une centaine d\u2019exemples pour ensuite en traiter des dizaines de milliers ou plus. C\u2019est pourquoi les chercheurs ont d\xe9velopp\xe9 une m\xe9thode de distillation qui augmente la robustesse de PET, c\u2019est la m\xe9thode **iPET : iterative PET**.\\n\\n\\n## i(terative)PET : une m\xe9thode de distillation astucieuse\\n\\nUne image peut valoir mille mots\u2026\\n\\n![](./img/2022-02-04-MindshakeTime/iPET.png)\\n\\n\u2026 Mais quelques mots seront quand m\xeame n\xe9cessaires pour expliquer cette image !\\n\\nTout d\u2019abord, le sch\xe9ma de gauche sur la figure pr\xe9sente l\u2019adaptation de PET qui permet d\u2019obtenir le label le plus adapt\xe9 au commentaire\u2026 en moyenne. En effet, il s\u2019agit \u2018simplement\u2019 de **faire travailler des m\xe9thodes PET ind\xe9pendantes en parall\xe8le** (trois sur le sch\xe9ma). Les trois cellules ont le m\xeame PLM au d\xe9part, et elles travaillent sur les m\xeames commentaires, mais avec des patterns diff\xe9rents. Dans la phase d\u2019entra\xeenement sur les donn\xe9es labellis\xe9es, les PLMs se sp\xe9cialisent diff\xe9remment. Puis, durant la phase de travail, pour un m\xeame commentaire ils produisent des **paires pattern-verbalizers** (appel\xe9es **PVPs** sur le sch\xe9ma) ind\xe9pendamment les uns des autres ; possiblement les m\xeames, mais pas avec les m\xeames probas. Enfin, **en sortie** ces (trois) labels sont utilis\xe9s pour calculer **un soft-label**, i.e. un **label moyen**.\\n\\nEnsuite, sur la droite est pr\xe9sent\xe9 le caract\xe8re it\xe9ratif de la m\xe9thode iPET. Elle consiste \xe0 diviser le jeu labellis\xe9s sur plusieurs it\xe9rations (indiqu\xe9es par les exposants allant de 0 \xe0 k) et \xe0 diviser encore \xe0 chaque it\xe9ration entre plusieurs m\xe9thodes parall\xe8les (indiqu\xe9es par les indices allant de 0 \xe0 4). Mais attention, chacun des quatre mod\xe8les ici fait du soft-labelling comme pr\xe9sent\xe9 \xe0 gauche de la figure, c\u2019est-\xe0-dire qu\u2019ils contiennent plusieurs m\xe9thodes en parall\xe8le.\\n\\nDonc, si l\u2019on suppose que l\u2019on part pour trois it\xe9rations, l\u2019information labellis\xe9e est distill\xe9e de la mani\xe8re suivante. \xc0 l\u2019it\xe9ration 0 sur le sch\xe9ma, on prend un tiers des donn\xe9es labellis\xe9es, et on fournit un quart de ces donn\xe9es \xe0 chaque mod\xe8le pour le finetuner, avant de prendre un tiers des donn\xe9es \xe0 labelliser et d\u2019en fournir un quart \xe0 chaque mod\xe8le pour soft-labellisation. Ce qui constitue la fin de la premi\xe8re it\xe9ration.\\n\\n\xc0 la deuxi\xe8me it\xe9ration - it\xe9ration 1 sur le sch\xe9ma, on commence \xe0 nouveau par un phase de fine-tuning, mais avec un jeu de donn\xe9es labellis\xe9es constitu\xe9 pour partie des donn\xe9es annot\xe9es par un \xeatre humain (le deuxi\xe8me tiers), et pour partie de donn\xe9es soft-labellis\xe9es. Toutefois, on fait attention \xe0 ce qu\u2019un mod\xe8le ne s\u2019entra\xeene pas avec des donn\xe9es qu\u2019il a lui-m\xeame soft-labellis\xe9, pour \xe9viter qu\u2019il renforce ses biais\u2026 on distille ! Par exemple sur le sch\xe9ma, \xe0 l\u2019it\xe9ration 1, le jeu d\u2019entra\xeenement T fourni au mod\xe8le 4, i.e. T14, est constitu\xe9 de donn\xe9es soft-labellis\xe9es par les mod\xe8les 1 et 2, en plus des donn\xe9es annot\xe9es par l\u2019humain. Puis on prend le deuxi\xe8me tiers de donn\xe9es \xe0 annoter, on en fournit un quart \xe0 chaque mod\xe8le pour soft-labellisation et on finit la deuxi\xe8me it\xe9ration.\\n\\nPour la troisi\xe8me it\xe9ration, vous avez compris le principe je pense\u2026   \\n\\n\xc0 la fin, les millions de commentaires sont plut\xf4t bien soft-labellis\xe9s, \xe0 la vitesse de la machine et au co\xfbt de l\u2019\xe9lectricit\xe9, tout est pr\xeat pour un classifieur sur le sch\xe9ma d\u2019Ekimetrics et je vous ai expliqu\xe9 tous les termes entour\xe9s sur la figure et pr\xe9sent\xe9 toutes les \xe9tapes. \\n\\n\\n## Avantages, inconv\xe9nients, limites et am\xe9liorations.\\n\\nNous avons d\xe9j\xe0 vu certains des avantages. Internet est une place sur laquelle il y a pl\xe9thore d\u2019avis en tout genre : films, restaurants, h\xf4tels, produits de grande consommation, lieux divers\u2026 Annoter ces donn\xe9es serait un travail co\xfbteux et sans fin, nous l\u2019avons dit. L\u2019approche iPET permet d\u2019automatiser cette \xe9tape, \xe0 la vitesse de l\u2019ordinateur et quel que soit le cas d\u2019\xe9tude.\\n\\nDu point de vue des performances, Ekimetrics a indiqu\xe9 avoir une pr\xe9cision de 88% avec seulement 50 donn\xe9es labellis\xe9es au d\xe9part, et m\xeame 84% avec 10 donn\xe9es labellis\xe9es !! En comparaison, les mod\xe8les supervis\xe9s peuvent atteindre des pr\xe9cisions de 99%, mais au prix d\u2019un \xe9norme travail de pr\xe9-traitement. C\u2019est donc un pas conceptuel de plus dans la r\xe9duction de la supervision.\\n\\nToutefois, le domaine d\u2019application se restreint \xe0 des donn\xe9es textuelles assez courtes d\u2019une part. Et d\u2019autre part, la charge de travail est d\xe9plac\xe9e vers une bonne conceptualisation du cas d\u2019\xe9tude. Les r\xe9sultats sont extr\xeamement d\xe9pendants de la formulation des patterns et des choix de verbalizers (i.e. choix du prompting). Ceux-ci impliquent une grande variabilit\xe9 qui n\u2019est pas ma\xeetris\xe9e. De plus le PLM utilis\xe9 - un mod\xe8le BERT dans le cas d\u2019Ekimetrics, cache des inconnues sur le corpus qui a servi \xe0 son entra\xeenement, son domaine d\u2019applicabilit\xe9, ses param\xe8tres. On touche l\xe0 \xe0 une limite dans laquelle l\u2019IA n\u2019est plus tout \xe0 fait de l\u2019open science.\\n\\n\\n---\\n\\n# Notes de Xavier que je n\'ai pas mises\\n\\n## LIMITES et PISTES D\'AM\xc9LIORATIONS\\n\\nPLM ou Foundation mod\xe8le avec quelles donn\xe9es a-t-il \xe9t\xe9 entra\xeen\xe9 ???\\n\\nQue donnerait l\u2019utilisation de plusieurs PLM ?\\n\\nune am\xe9lioration de ces approches est propos\xe9 dans le papier [https://arxiv.org/pdf/2103.11955.pdf](https://arxiv.org/pdf/2103.11955.pdf)."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/facebook/docusaurus/edit/master/website/blog/blog/2021-10-15-welcome.md","source":"@site/blog/2021-10-15-welcome.md","title":"My first GitHub blog! (strongly inspired by Ekimetrics\'...)","description":"Welcome to the first post! Short text, but sexy... People are eager to know more.","date":"2021-10-15T00:00:00.000Z","formattedDate":"October 15, 2021","tags":[{"label":"datacraft","permalink":"/blog/tags/datacraft"},{"label":"cute","permalink":"/blog/tags/cute"},{"label":"I\'m sexy and I know it","permalink":"/blog/tags/im-sexy-and-i-know-it"}],"readingTime":1.145,"truncated":true,"authors":[{"name":"Kapichu","title":"astrophysicist","url":"mailto:julien.guyot@datacraft.paris"}],"prevItem":{"title":"Few Shot Learning - application de la m\xe9thode iPET","permalink":"/blog/draft"}},"content":"\x3c!--truncate--\x3e\\n\\n# And here is the content of my first blog...\\n\\n## Welcome to our technology website!\\nWe have been working in the Data Science industry for 15 years and are now the biggest pure player in Europe with 250+ data profiles. We have been benefiting from the open source community for a while, and we want to give back to the community by sharing insights on what we\'ve learned over the years:\\n- [Blog](/blog) - read articles on various Data topics: from industrialization on cloud platforms to exotic Deep Learning algorithms\\n- [Best practices & convictions](/docs) - discover our programming best practices, our tech convictions and preferred technologies\\n- [Open source contributions](/opensource) - browse our own open source contributions (Python libraries, code snippets)\\n\\n\ud83d\udc8c After reading behind the scenes of the Data Science Company, feel free to [send us a email](mailto:inno@ekimetrics.com) for any questions or feedbacks! \\n\\n## About Ekimetrics\\n\\nEkimetrics is the first pure player in Data Science in Europe. We operate from Paris, London, New York and Hong Kong with 250+ Data Scientists, Data Engineers, Full Stack Developers, strategy consultants and UX designers. \\n\\nWe help companies steer their data opportunity, build data capabilities, and deploy actionable solutions, to power up marketing and operational performance, as well as (re)energizing business models. Our primary focus is to deliver immediate business gains, while guaranteeing sustainable data capital for our clients."}]}')}}]);