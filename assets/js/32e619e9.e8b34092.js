"use strict";(self.webpackChunkdrkapichu=self.webpackChunkdrkapichu||[]).push([[5377],{3905:function(e,n,s){s.d(n,{Zo:function(){return d},kt:function(){return m}});var t=s(7294);function i(e,n,s){return n in e?Object.defineProperty(e,n,{value:s,enumerable:!0,configurable:!0,writable:!0}):e[n]=s,e}function r(e,n){var s=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),s.push.apply(s,t)}return s}function a(e){for(var n=1;n<arguments.length;n++){var s=null!=arguments[n]?arguments[n]:{};n%2?r(Object(s),!0).forEach((function(n){i(e,n,s[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(s)):r(Object(s)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(s,n))}))}return e}function o(e,n){if(null==e)return{};var s,t,i=function(e,n){if(null==e)return{};var s,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)s=r[t],n.indexOf(s)>=0||(i[s]=e[s]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)s=r[t],n.indexOf(s)>=0||Object.prototype.propertyIsEnumerable.call(e,s)&&(i[s]=e[s])}return i}var l=t.createContext({}),u=function(e){var n=t.useContext(l),s=n;return e&&(s="function"==typeof e?e(n):a(a({},n),e)),s},d=function(e){var n=u(e.components);return t.createElement(l.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},p=t.forwardRef((function(e,n){var s=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),p=u(s),m=i,f=p["".concat(l,".").concat(m)]||p[m]||c[m]||r;return s?t.createElement(f,a(a({ref:n},d),{},{components:s})):t.createElement(f,a({ref:n},d))}));function m(e,n){var s=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=s.length,a=new Array(r);a[0]=p;var o={};for(var l in n)hasOwnProperty.call(n,l)&&(o[l]=n[l]);o.originalType=e,o.mdxType="string"==typeof e?e:i,a[1]=o;for(var u=2;u<r;u++)a[u]=s[u];return t.createElement.apply(null,a)}return t.createElement.apply(null,s)}p.displayName="MDXCreateElement"},9622:function(e,n,s){s.r(n),s.d(n,{assets:function(){return d},contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return o},metadata:function(){return u},toc:function(){return c}});var t=s(7462),i=s(3366),r=(s(7294),s(3905)),a=["components"],o={slug:"biais-humains-et-algorithmes",title:"Comment veiller \xe0 ce que les biais humains n\u2019impr\xe8gnent pas les algorithmes ?",author:"St\xe9phanie Lehuger",author_title:"Thinker et entrepreneuse",author_url:"mailto:contact@datacraft.paris",header_image_url:"img/blog/UsberetRica.jpg",tags:["biais","algorithmes","IA","philosophie"],description:"Article publi\xe9 dans Usbek & Rica le 18 f\xe9vrier 2022",keywords:["biais humain","algorithmes","IA","philosophie"]},l=void 0,u={permalink:"/blog/biais-humains-et-algorithmes",editUrl:"https://github.com/DrKapichu/drkapichu.github.io/tree/main/blog/blog/2022-02-18-usbeketrica.md",source:"@site/blog/2022-02-18-usbeketrica.md",title:"Comment veiller \xe0 ce que les biais humains n\u2019impr\xe8gnent pas les algorithmes ?",description:"Article publi\xe9 dans Usbek & Rica le 18 f\xe9vrier 2022",date:"2022-02-18T00:00:00.000Z",formattedDate:"February 18, 2022",tags:[{label:"biais",permalink:"/blog/tags/biais"},{label:"algorithmes",permalink:"/blog/tags/algorithmes"},{label:"IA",permalink:"/blog/tags/ia"},{label:"philosophie",permalink:"/blog/tags/philosophie"}],readingTime:8.9,truncated:!0,authors:[{name:"St\xe9phanie Lehuger",title:"Thinker et entrepreneuse",url:"mailto:contact@datacraft.paris"}],prevItem:{title:"This is a test for the actions",permalink:"/blog/testou"},nextItem:{title:"Few Shot Learning - application de la m\xe9thode iPET",permalink:"/blog/draft"}},d={authorsImageUrls:[void 0]},c=[],p={toc:c};function m(e){var n=e.components,s=(0,i.Z)(e,a);return(0,r.kt)("wrapper",(0,t.Z)({},p,s,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("hr",null),(0,r.kt)("div",{style:{marginTop:"1em",marginBottom:"2em"}},(0,r.kt)("div",{className:"warning",style:{fontSize:"28px",color:"#69337A",padding:"1.0em"}},"Stephanie Lehuger, Thinker et Entrepreneur, membre du ",(0,r.kt)("u",null,"club datacraft"),", qui r\xe9fl\xe9chit aux questions \xe9thiques soulev\xe9es par l\u2019IA, nous explique dans cette tribune de quels outils et m\xe9thodes nous disposons actuellement pour qu\u2019un pr\xe9jug\xe9 humain ne se retrouve pas dans l\u2019algorithme d\u2019une intelligence artificielle.")),(0,r.kt)("p",null,"Tous les humains ont des biais cognitifs, c\u2019est in\xe9vitable. Et les data scientists sont des humains, donc ils sont fatalement sujets aux biais, comme tout le monde. Pour \xe9tablir des connaissances, les data scientists analysent des donn\xe9es. Et ces donn\xe9es, si elles sont mal choisies, donnent de mauvais r\xe9sultats. Ainsi, un biais cognitif se transforme en biais de donn\xe9e qui se transforme ensuite en biais algorithmique."),(0,r.kt)("p",null,"On peut dire qu\u2019un algorithme fonctionne comme une recette de cuisine o\xf9 les ingr\xe9dients seraient les donn\xe9es et la recette le code : si les ingr\xe9dients (les donn\xe9es) sont de mauvaise qualit\xe9, avec des biais par exemple, le r\xe9sultat ne peut qu\u2019\xeatre d\xe9cevant. La plupart du temps, les biais proviennent des donn\xe9es et cela se produit de deux mani\xe8res."),(0,r.kt)("p",null,"En premier lieu, ils peuvent \xeatre le r\xe9sultat d\u2019une mauvaise collecte. Imaginons par exemple qu\u2019on cherche \xe0 d\xe9terminer le loyer moyen que paient les gens qui louent leur logement. Si les data scientists sont parisiens et r\xe9cup\xe8rent la base de donn\xe9es de leur ville, ils vont obtenir un r\xe9sultat \xe9lev\xe9 par rapport \xe0 la moyenne nationale. Il sera biais\xe9 par les loyers de Paris."),(0,r.kt)("p",null,"La transmission d\u2019un biais s\u2019effectue donc au travers des donn\xe9es choisies (la \xab data \xbb). Si les data scientists n\u2019ont pas conscience que les loyers sont moins \xe9lev\xe9s dans les villes de taille moyenne et en zone rurale que dans les grandes villes et qu\u2019ils entra\xeenent un algorithme \xe0 pr\xe9dire le prix du loyer sur ces donn\xe9es-l\xe0, alors ses pr\xe9dictions seront biais\xe9es aussi. Le biais d\u2019une IA peut provenir \xe0 l\u2019origine d\u2019un biais cognitif humain, qui se transmet dans les donn\xe9es choisies qui sont biais\xe9es, puis elles influencent ensuite les r\xe9sultats en s\u2019\xe9tant transform\xe9es en un biais algorithmique."),(0,r.kt)("div",{style:{marginTop:"1em",marginBottom:"2em"}},(0,r.kt)("div",{className:"warning",style:{fontSize:"24px",color:"#69337A",borderLeft:"solid #805AD5 4px",padding:"0.7em"}},"S\u2019il existe une discrimination des femmes dans une entreprise, se baser sur les donn\xe9es pass\xe9es pour \xe9valuer le potentiel d\u2019une candidate, m\xeame plus brillante qu\u2019un concurrent masculin, lui sera d\xe9favorable")),(0,r.kt)("p",null,"En deuxi\xe8me lieu, les biais peuvent \xe9maner d\u2019une situation d\xe9j\xe0 biais\xe9e et qu\u2019un algorithme pourrait amplifier. Comme une intelligence artificielle qui baserait son apprentissage sur des donn\xe9es historiquement biais\xe9es. Si, depuis toujours, il existe une discrimination des femmes dans une entreprise, se baser sur les donn\xe9es pass\xe9es pour \xe9valuer le potentiel d\u2019une candidate, m\xeame plus brillante qu\u2019un concurrent masculin, lui sera d\xe9favorable. Si, historiquement, les femmes sont peu repr\xe9sent\xe9es, l\u2019algorithme pourra en d\xe9duire de mani\xe8re erron\xe9e qu\u2019elles ont un profil moins d\xe9sirable."),(0,r.kt)("h2",null," Quelques exemples de biais communs "),(0,r.kt)("p",null,"Un biais typique qu\u2019il faut tenter d\u2019\xe9viter est le biais des survivants. Par exemple, quand on constate que des b\xe2timents de plus de cent ans sont encore debout, on a l\u2019impression que la \xab construction d\u2019antan \xbb \xe9tait de meilleure qualit\xe9 qu\u2019aujourd\u2019hui. Pourtant, quand on y r\xe9fl\xe9chit, la quasi-totalit\xe9 de ce qui a \xe9t\xe9 construit depuis l\u2019invention de la construction s\u2019est en fait \xe9croul\xe9e ou a \xe9t\xe9 d\xe9molie, donc ces b\xe2timents \xab survivants \xbb sont des exceptions."),(0,r.kt)("div",{style:{marginTop:"1em",marginBottom:"2em"}},(0,r.kt)("div",{className:"warning",style:{fontSize:"24px",color:"#69337A",borderLeft:"solid #805AD5 4px",padding:"0.7em"}},"Les data scientists doivent \xe9viter le biais du survivant qui consisterait \xe0 tirer des conclusions sur la base d\u2019une population incompl\xe8te")),(0,r.kt)("p",null,"Lors de l\u2019\xe9tude de donn\xe9es par des data scientists, il leur faut \xe9viter le biais du survivant qui consisterait \xe0 tirer des conclusions sur la base d\u2019une population incompl\xe8te, comportant uniquement les \xe9l\xe9ments ayant \xab surv\xe9cu \xbb, qui sont en fait des exceptions, plut\xf4t que des cas repr\xe9sentatifs."),(0,r.kt)("p",null,"En France, des \xab antivax \xbb ont \xe9t\xe9 victimes du paradoxe de Simpson. Ils ont assur\xe9 \xe0 tort sur les r\xe9seaux sociaux que les non-vaccin\xe9s ne saturent pas les services de r\xe9animation du pays, en s\u2019appuyant sur des donn\xe9es de la Drees mal interpr\xe9t\xe9es. Leur erreur principale est d\u2019avoir regard\xe9 les chiffres bruts au lieu des pourcentages. Il y a en effet neuf fois plus de vaccin\xe9s que de non-vaccin\xe9s en France. "),(0,r.kt)("p",null,"Alors que les non-vaccin\xe9s sont tr\xe8s minoritaires, ils sont surrepr\xe9sent\xe9s \xe0 l\u2019h\xf4pital, avec 63 % des admissions en soins critiques. Si on regarde les chiffres absolus, on peut avoir l\u2019impression que les deux populations sont en nombre \xe9quilibr\xe9s dans les h\xf4pitaux, mais ce serait oublier de regarder la proportion de chacune dans la population g\xe9n\xe9rale. C\u2019est ainsi que plusieurs enqu\xeates r\xe9centes men\xe9es sur des \xe9chantillons d\u2019h\xf4pitaux ont conclu que les non-vaccin\xe9s repr\xe9sentaient entre 70 % et 90 % dans les services de r\xe9animation."),(0,r.kt)("h2",null," Comment les data scientists corrigent les biais ? "),(0,r.kt)("p",null,"Une fois qu\u2019on a identifi\xe9 pourquoi et comment les biais posent un probl\xe8me aux data scientists, on va s\u2019int\xe9resser \xe0 ce que les data scientists font, ne font pas, et devraient faire pour limiter les risques li\xe9s \xe0 ces biais."),(0,r.kt)("h3",null," 1. Prendre conscience du probl\xe8me et se poser les bonnes questions "),(0,r.kt)("p",null,"Pour prendre conscience du probl\xe8me des biais cognitifs, les data scientists ont acc\xe8s \xe0 diff\xe9rents types de ressources. Ils peuvent par exemple commencer \xe0 s\u2019informer par le biais d\u2019une charte, comme la charte \xe9thique \xe9labor\xe9e au sein de datacraft. Ils peuvent par ailleurs analyser le contenu des r\xe9f\xe9rentiels d\u2019\xe9valuation de l\u2019IA de confiance, comme celui de Labelia Labs (ex-Substra Foundation) ou celui du LNE. Les serments \xe9tablissent \xe9galement une liste pertinente de crit\xe8res d\u2019une IA responsable, comme le font Tech pledge et Holberton-Turing Oath. Enfin, il existe des outils pratiques comme la checklist de Data Science \xe9thique deon, accessible en ligne de commande."),(0,r.kt)("p",null,"Il est important d\u2019avoir un esprit critique sur son propre travail quand on est data scientist. Si on ne devait choisir que 3 questions \xe0 se poser absolument, voici ce que je propose :"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"S\u2019engager \xe0 faire une pause pour s\u2019interroger sur toutes les cons\xe9quences de son travail, qu\u2019elles soient voulues ou non;"),(0,r.kt)("li",{parentName:"ul"},"Contr\xf4ler les cons\xe9quences de son travail dans le temps;"),(0,r.kt)("li",{parentName:"ul"},"Tendre vers l\u2019autor\xe9gulation \xe0 l\u2019aide de r\xe9f\xe9rentiel d\u2019\xe9valuation, de certification avec audit), en compl\xe9ment des \xab 7 points de vigilance \xbb soulign\xe9s par la Commission europ\xe9enne.")),(0,r.kt)("h3",null," 2. Mesurer les biais "),(0,r.kt)("p",null,"Apr\xe8s avoir pris conscience de la potentielle existence de biais, la seconde \xe9tape consiste \xe0 d\xe9finir des m\xe9triques appropri\xe9es afin de les mesurer convenablement. Le choix des m\xe9triques d\xe9pend alors essentiellement de ce que l\u2019on cherche \xe0 contr\xf4ler. Aequitas est une bo\xeete \xe0 outils open source pour auditer les biais, cr\xe9\xe9e par le Center for Data Science and Public Policy de l\u2019Universit\xe9 de Chicago. "),(0,r.kt)("p",null,"Elle permet de v\xe9rifier les pr\xe9dictions des outils d\u2019\xe9valuation des risques bas\xe9s sur l\u2019apprentissage automatique afin de comprendre les diff\xe9rents types de biais et de prendre des d\xe9cisions \xe9clair\xe9es sur le d\xe9veloppement et le d\xe9ploiement de ces syst\xe8mes. Le \xab fairness tree \xbb aide \xe0 choisir la bonne m\xe9trique. L\xe0 comme ailleurs, il convient d\u2019\xeatre attentif aux choix r\xe9alis\xe9s puisqu\u2019il existe un nouveau biais possible. En effet, il faut avoir conscience que, en choisissant une m\xe9trique, on \xe9carte toutes les autres."),(0,r.kt)("div",{style:{marginTop:"1em",marginBottom:"2em"}},(0,r.kt)("div",{className:"warning",style:{fontSize:"24px",color:"#69337A",borderLeft:"solid #805AD5 4px",padding:"0.7em"}},"Tout choix concernant une population \xe9tudi\xe9e devient moral en data science")),(0,r.kt)("p",null,"Des m\xe9triques faciles d\u2019acc\xe8s pour les data scientists sont \xe9galement mises en \u0153uvre. Une \xe9quipe au sein de datacraft a r\xe9alis\xe9 une cartographie de 5 \xab fairness open source libraries \xbb lors d\u2019un benchathon : AIF360, Shapash, Aequitas, What if tool, Fairlearn."),(0,r.kt)("p",null,"Tout choix concernant une population \xe9tudi\xe9e devient moral en data science. En dehors des tr\xe8s grands groupes qui ont conscience des risques r\xe9putationnels forts auxquels ils sont soumis s\u2019ils ne font pas attention aux biais et \xe0 leurs cons\xe9quences, cela reste principalement une question qui tient du ressort individuel dans les autres entreprises."),(0,r.kt)("p",null,"M\xeame si ce n\u2019est pas obligatoire l\xe9galement, il revient donc aux data scientists d\u2019\xeatre moralement critiques sur leurs choix de donn\xe9es. Tout comme il est n\xe9cessaire d\u2019\xeatre prudent pour ne pas introduire de biais dans les algorithmes qu\u2019ils d\xe9veloppent."),(0,r.kt)("h3",null," 3. Limiter les risques de biais "),(0,r.kt)("p",null,"Il existe de nombreuses m\xe9thodes pour r\xe9duire les biais, que l\u2019on peut diviser en trois grandes familles selon que l\u2019intervention du praticien se situe avant, pendant ou apr\xe8s l\u2019entra\xeenement de l\u2019algorithme. Avant l\u2019entra\xeenement, ces m\xe9thodes consistent \xe0 transformer les donn\xe9es \xe0 disposition, par exemple en les repond\xe9rant."),(0,r.kt)("p",null,"Concr\xe8tement, on peut revoir la pond\xe9ration du nombre de personnes dans un jeu de donn\xe9es pour s\u2019assurer qu\u2019il y a autant d\u2019hommes que de femmes et ainsi \xe9viter un biais de genre. Pendant l\u2019entra\xeenement, il s\u2019agit d\u2019incorporer des contraintes d\u2019\xe9quit\xe9 \xe0 satisfaire, en compl\xe9ment des objectifs de performance classique. Enfin, les m\xe9thodes dites de post-traitement consistent \xe0 modifier les d\xe9cisions des algorithmes, par exemple en favorisant les sous-groupes discrimin\xe9s."),(0,r.kt)("div",{style:{marginTop:"1em",marginBottom:"2em"}},(0,r.kt)("div",{className:"warning",style:{fontSize:"24px",color:"#69337A",borderLeft:"solid #805AD5 4px",padding:"0.7em"}},"Pour qu\u2019une entreprise soit \xe0 la fois juste et profitable, toutes les parties prenantes doivent \xe9changer pour parvenir \xe0 des compromis acceptables")),(0,r.kt)("p",null,"En diminuant les biais d\u2019un c\xf4t\xe9, on diminue g\xe9n\xe9ralement la performance des algorithmes de l\u2019autre : on parle du fairness-accuracy tradeoff. Pour qu\u2019une entreprise soit \xe0 la fois juste et profitable, toutes les parties prenantes doivent \xe9changer pour parvenir \xe0 des compromis acceptables. Les data scientists, les d\xe9cideurs business ou encore les \xe9quipes de gouvernance sont impliqu\xe9s dans ce processus complexe afin d\u2019aboutir \xe0 un arbitrage. Une fois la d\xe9cision prise, l\u2019algorithme d\xe9ploy\xe9 et mis en production, il est indispensable de mettre en place une politique de supervision en temps r\xe9el (monitoring) afin de d\xe9tecter de possibles changements de comportement du mod\xe8le."),(0,r.kt)("p",null,"Pour r\xe9sumer, afin d\u2019\xe9viter de transmettre des biais humains \xe0 une intelligence artificielle, les data scientists doivent faire preuve d\u2019esprit critique vis-\xe0-vis de leurs possibles pr\xe9jug\xe9s inconscients quand ils s\xe9lectionnent leurs donn\xe9es et construisent leurs algorithmes. Il n\u2019y a malheureusement pas de m\xe9thode miracle qui marche \xe0 tous les coups. Un mod\xe8le repose sur des hypoth\xe8ses d\xe9pendantes d\u2019un contexte, elles seront donc diff\xe9rentes pour chaque probl\xe8me, sans qu\u2019un mod\xe8le magique fonctionne pour tous."))}m.isMDXComponent=!0}}]);