<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://drkapichu.github.io/blog</id>
    <title>datacraft blog Blog</title>
    <updated>2022-04-21T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://drkapichu.github.io/blog"/>
    <subtitle>datacraft blog Blog</subtitle>
    <icon>https://drkapichu.github.io/img/datacraft_logo.png</icon>
    <entry>
        <title type="html"><![CDATA[Un petit mémo à propos de Kubernetes]]></title>
        <id>sense4data-kubernetes</id>
        <link href="https://drkapichu.github.io/blog/sense4data-kubernetes"/>
        <updated>2022-04-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Here goes a short description of this post]]></summary>
        <content type="html"><![CDATA[<hr/><h1>Pourquoi Kubernetes ?</h1><p>Sorti pour la première fois en 2015, <a href="https://en.wikipedia.org/wiki/Kubernetes">Kubernetes</a> est devenu la solution d’orchestration de conteneurs privilégiée par les entreprises. Pourquoi cela ?<br/>
<!-- -->Imaginez une application que vous mettez en ligne. A mesure qu’elle devient populaire, elle doit pouvoir être ‘requêtée’ de manière massive et, dans son ensemble, elle traite des volumes de données de plus en plus gros. Une tension apparaît alors sur deux caractéristiques de la machine :</p><ul><li>Vitesse de calcul : d’une part le CPU ne peut pas gérer toutes les demandes des utilisateurs dans un temps raisonnable,</li><li>Capacité mémoire : d’autre part la RAM du CPU ne peut pas supporter l’ensemble des données de tous les utilisateurs. @fig1:descr</li></ul><p>Il n’existe que deux façons de gérer ce problème de capacité (cf <a href="#Figure1">Figure 1</a>).<br/>
<!-- -->Avant la généralisation des technologies de conteneurs, la solution la plus adoptée était la <strong>scalabilité verticale</strong> : on prenait une machine plus puissante. C&#x27;est-à-dire avec un CPU plus performant, voire un GPU, et une RAM plus grande (e.g. 32Gb, 64Gb, jusqu’à 1Tb). Mais c’est une stratégie du “toujours plus puissant” qui peut rapidement atteindre ses limites, dont la première est le coût qui grandit de manière quasi-exponentielle (voir graphe à droite de la <a href="#Figure1">Figure 1</a>).<a name="Figure1"></a><br/>
<!-- -->Pour remédier à ça, on peut pratiquer la <strong>scalabilité horizontale</strong> : on met deux machines en parallèle et on répartit le travail sur chacune. Ça a aussi un coût financier, mais passée une certaine limite il est moindre que celui de la scalabilité verticale. Par contre il faut maintenir plusieurs machines : les mettre à jour et installer les librairies deux fois, i.e. s’assurer qu’elles offrent le même environnement fonctionnel à l’application. Pire, si on veut avoir des redondances pour pallier l’arrêt imprévu d’une machine, voire des deux, on doit s’occuper de quatre machines à minima !</p><p>Kubernetes, également appelé k8s, fait partie des technologies qui ont rendu la scalabilité horizontale très facilement applicable.</p><p><img src="./img/2022-04-21-Kubernetes/Figure1-scalabilites.png" alt="Figure1" title="Illustration des scalabilités verticale et horizontale."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Figure 1 : Illustration des scalabilités verticale et horizontale.</div><h1>Principes de base de k8s.</h1><p>Le fonctionnement de Kubernetes repose sur une <strong>stratégie de microservices conteneurisés</strong>.<br/>
<!-- -->Pour préciser les choses, nous allons maintenant préciser quelques fonctionnalités de notre application. Imaginez qu’elle prend des informations clients qu’elle compare à une base de données pour des programmes de fidélité, et, en même temps, elle dispose d’un algorithme de recommandation qui doit proposer un contenu adapté à l’utilisateur selon les produits disponibles d’après une autre base de données. Voilà déjà une application assez complexe.</p><p>Auparavant, une telle application était construite de façon dite ‘monolithique’ (voir Figure 2, panneau de gauche). C’est à dire que les différentes fonctionnalités de l’application n’étaient pas forcément indépendantes et surtout elles tournaient toutes sur la même machine ; celle que l’on voulait toujours plus puissante.
La stratégie de micro-services consiste à construire chacune des fonctionnalités - ou chacun des services - de manière indépendante des autres, par une “petite” équipe autonome. Les équipes n’ont qu’à s’accorder sur un standard de communication entre les services et éventuellement à le faire évoluer progressivement. Chaque micro-service tourne ensuite sur un serveur dédié et peut être indépendamment scalable et déployable.  </p><p>:::tip Vocabulaire  </p><div><div value="Indépendamment scalable" label="Indépendamment scalable">S’il y a une trop grande tension pour accéder à la base de données des utilisateurs par exemple et que ce service devient trop lent, on peut augmenter le nombre de machines dédiées à ce service seulement et répartir la charge sans affecter les autres.</div><div value="Indépendamment déployable" label="Indépendamment déployable">Si on améliore l’algorithme de recommandation de contenu, on peut redéployer ce service seulement puis arrêter les serveurs qui font tourner l’ancien algorithme progressivement ; comme ce service est indépendant des autres, la seule précaution à prendre est de s’assurer que leur standard de communication est toujours valide.</div></div> ::: <p>:::note</p><h2>class: dropdown</h2><p>The note body will be hidden!
:::</p><p>:::{admonition} Click here!
:class: tip, dropdown
Here&#x27;s what&#x27;s inside!
:::</p><p>Cette stratégie de microservices représente la scalabilité horizontale. Toutefois, faire tourner le même service sur plusieurs serveurs, ça implique de maintenir toutes ces machines et de s’assurer qu’elles fournissent toutes le même environnement au service en question, i.e. mêmes librairies avec les mêmes versions. Et c’est là que la conteneurisation devient un atout majeur. Pour chaque service il suffit d’écrire toutes les exigences de librairies dans un fichier, puis de lancer un conteneur Docker à partir de ce fichier.<br/>
<!-- -->Au passage cela règle un autre problème présenté sur le panneau de droite de la Figure 2. Différentes applications installées sur un même serveur pouvaient avoir besoin des mêmes librairies mais avec des versions différentes, qui elles-mêmes pouvaient appeler différentes versions d’autres librairies. Et les équipes techniques devaient trouver les moyens de les installer toutes de manière concurrente et de s’assurer que les bonnes versions étaient appelées au bon moment. La conteneurisation enlève tout simplement ce problème.  </p><p>En conclusion, <strong>k8s</strong> est un moyen unique et simple de gérer une myriade de micro-services interconnectés de manière conteneurisée, c’est un <strong>orchestrateur de conteneurs</strong>.</p><p><img src="./img/2022-04-21-Kubernetes/Figure2-conteneurisation.png" alt="Figure 2" title="Figure 2"/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Figure 2 : Write some text here... This fig is too small, make it different.</div>]]></content>
        <author>
            <name>Julien Guyot</name>
            <uri>mailto:julien.guyot@datacraft.paris</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in AI - How a benchathon unlocked our knowledge]]></title>
        <id>AI-Fairness-et-benchathon2</id>
        <link href="https://drkapichu.github.io/blog/AI-Fairness-et-benchathon2"/>
        <updated>2022-04-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[IA et biais (2/3) - Quels outils pour mesurer et atténuer ?]]></summary>
        <content type="html"><![CDATA[<hr/><h1>Fairness in AI - How a benchathon unlocked our knowledge</h1><p>Let&#x27;s all board on a journey to the land of AI fairness that we, a group of private players (Danone, Ekimetrics, datacraft), researchers (Telecom Paris, Inria), and students (Université de Cergy), partnered to uncover fairness &amp; ethics in Artificial Intelligence from a practical standpoint. Through this journey the group tried to tackle the following challenge: “how should a Data Scientist concretely react when exposed to fairness concerns?”.  </p><p>We started with a series of workshops to discuss the existing tools and methods, assess the needs and define an <a href="https://datacraft-paris.github.io/trustworthyai/">ethical charter</a>. This led to the writing of a first popularisation article, in French, which you can find <a href="./biais-humains-et-algorithmes">here</a>.<br/>
<!-- -->Then we continued our journey with a benchathon - definition below - in which the group participated to get a quick and documented opinion of an already rich fairness/ethical ecosystem. It first explains how the concept of benchathon accelerated our practical grasp of the topic, and then explores the first conclusions drawn about the fairness ecosystem. You can find the code developped during this benchathon on the <a href="https://github.com/datacraft-paris/Fairness-Benchathon">datacraft&#x27;s GitHub repo</a>.</p><h2>Benchathon as an innovation catalyst</h2><p>At this stage of our “fairness journey”, we had a decent high level understanding of what fairness could imply in real life. It was then the right time to start acting concrete: try and derive a pragmatic methodology, even if it implied implementing our own routines.</p><p>To that end, a very first step was to make sure we’d not reinvent the wheel, and we’d plainly benefit from existing open source contributions. This happened during a one-day benchathon. If you work in tech, you may already be familiar with the following two notions:</p><ul><li>Benchmark: <em>gathering and comparing qualitative information about how an activity is conducted through people, processes, and technology</em> (Source: <a href="https://www.apqc.org/blog/what-are-four-types-benchmarking">https://www.apqc.org/blog/what-are-four-types-benchmarking</a>)  </li><li>Hackathon: <em>[short]<!-- --> event <!-- -->[...]<!-- --> in which computer programmers and others involved in software development <!-- -->[...]<!-- --> collaborate intensively on software projects</em> (Source: <a href="https://en.wikipedia.org/wiki/Hackathon">https://en.wikipedia.org/wiki/Hackathon</a>)  </li></ul><p>Hackathons usually involve teams that compete on the “same topic” for 2 to 3 days. Because we were limited in time - 1 day, rather than focusing all on the same “thing”, we decided to make the most out of the presence of 9 data scientists: we shared and split between us the technical analysis of several fairness open source libraries - <a href="https://github.com/Trusted-AI/AIF360">AIF360</a>, <a href="https://github.com/MAIF/shapash">Shapash</a>, <a href="https://github.com/dssg/aequitas">Aequitas</a>, <a href="https://pair-code.github.io/what-if-tool/">What if tool</a>, <a href="https://fairlearn.org/">Fairlearn</a>. Hence the concept of benchathon.</p><p>Even though all of us were entitled as “Data Scientists”, we all came from different structures, different backgrounds, and different (coding) habits. That diversity definitely triggered (and still does) great discussions and perspectives along the initiative. Still, an important step during the benchathon was to settle on an interpretation grid that would make the outcome as reusable and general as possible, and as unbiased as possible - in line with the topic then :). A few criteria were identified:</p><table><tr><th>   <strong>Criteria</strong>                  </th><th>    <strong>Description</strong>                                                                                                             </th><th>   <strong>Scale</strong>           </th>  </tr><tr><td align="left" bgcolor="white"> Installation  </td><td align="left" bgcolor="white"> How easy is it to get started?                                                                               </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Usability     </td><td align="left" bgcolor="white"> How easy to use is the API?                                                                                  </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Documentation </td><td align="left" bgcolor="white"> How well documented is the library?                                                                          </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Completeness  </td><td align="left" bgcolor="white"> Does the library perform everything it is supposed to?                                                       </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Reliability   </td><td align="left" bgcolor="white"> Does the library seem reliable? (code quality, tests, …)                                                     </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Legitimacy    </td><td align="left" bgcolor="white"> Is the library popular within the community? (number of stars on GitHub, latest commit, number of issues, …) </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Future        </td><td align="left" bgcolor="white"> Gut instinct - would you trust it and use it in real projects?                                               </td><td align="left" bgcolor="white"> Y/N </td>  </tr><tr><td align="left" bgcolor="white"> Weaknesses    </td><td align="left" bgcolor="white"> What is currently missing?                                                                                   </td><td align="left" bgcolor="white"> N/A </td>  </tr></table><p>That being set, what was important was also to pace the day, so that despite the fact that small groups worked independently, we always kept an overall coherence and dynamics. It meant:</p><ul><li>Mini sprints of 1,5 hours</li><li>At the end of each mini sprint, a quick roundtable to share insights or blocking points, and get challenged by the whole group</li><li>Lunch break altogether: everyone brought something to share. This was a great moment of conviviality. It would even seem that a new datacraft initiative was born at this very moment, stay tuned!</li><li>At the end of the day, wrap up session during which each group made a demo of the library it spent the day on, and made sure to fill out above-mentioned criteria. The latter was especially important because this is what helps us today to have a concrete reference that every one can refer to.</li></ul><p>Taking a step back, below are a few takeaways:</p><ul><li>This benchathon was extremely productive: in the matter of only a day, our practical grasp of the fairness/ethical ecosystem clearly passed a milestone (see next section).</li><li>All people around the table had a developer background, and the same objective - namely, uncovering the fairness topic from a technical &amp; practical standpoint. It helped to get started fairly quickly, and proved that this format was a great fit for that audience and purpose.</li><li>One mistake we made was not to invest enough time beforehand in mapping the main open source libraries available in the AI community. It turns out we missed what would become our GO TO in the future: <a href="https://github.com/ModelOriented/DALEX">Dalex</a>.</li></ul><h2>Highlight of 6 ethical/fairness libraries</h2><p>If you are further interested in the exhaustive findings on the five libraries that were studied during the benchathon, we summarized our conclusions in this <a href="https://github.com/datacraft-paris/Fairness-Benchathon/blob/master/docs/Benchathon-summary.pdf">file</a>. The following section aims at providing a (subjective) summary of these libraries, in increasing relevance order, with respect to fairness / ethics.</p><h3>Shapash</h3><p><a href="https://github.com/MAIF/shapash">Open source library</a> developed by MAIF - a French insurance actor, and Quantmetry - a French AI consultancy, that mainly focuses on interpretability (no built-in fairness-oriented feature). It acts as a layer on top of the usual interpretability toolbox (feature importance, SHAP values, …). It comes with a very decent web interface, high quality code, and a great community/documentation. It also provides an audit report of the project (from data prep to modeling, to exploratory analysis).</p><p>In a nutshell: great project, but not that relevant (yet?) for fairness topics.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Shapash.jpg" alt="Screenshot of Shapash." title="Screenshot of Shapash."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of Shapash. <a target="_blank" rel="noopener noreferrer" href="https://github.com/MAIF/shapash">Credits</a>.</div><h3>What if tool</h3><p><a href="https://pair-code.github.io/what-if-tool/">Open source interface</a> developed by Google. It mainly aims at conducting counterfactual analysis (“what would be the machine learning model prediction if we changed the value of that particular attribute, like the sex e.g.?”). It comes with a decent web interface, especially to deal with unstructured data like images. Documentation is however not handy to deal with.</p><p>In a nutshell: great interface. However, counterfactual analysis is only one (important) feature among the different aspects related to fairness, which in turn does not justify a lock-in with that specific tool.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/What_if_tool.jpg" alt="Screenshot of What-if tool." title="Screenshot of What-if tool."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of What-if tool. <a target="_blank" rel="noopener noreferrer" href="https://github.com/pair-code/what-if-tool">Credits</a>.</div><h3>Aequitas</h3><p><a href="http://aequitas.dssg.io/">Aequitas</a> is a bias and audit toolkit developed by Carnegie Mellon University. It aims at spotting unfair allocation compared to population repartition or wrong decisions about certain groups of people. It comes with a web interface (which we could not make work) and a Python library to help compute fairness metrics. Documentation is decent, especially their representation of the <a href="http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/">&quot;fairness tree&quot;</a>, which helps to navigate the (many and ambiguous) fairness metrics, depending on the use case.</p><p>In a nutshell: Aequitas is a tool that has been available for quite some time now, but that does not benefit from a living community. To be kept under the radar (or contribute to!).</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Aequitas.jpg" alt="Screenshot of Aequitas web application." title="Screenshot of Aequitas web application."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of Aequitas web application. <a target="_blank" rel="noopener noreferrer" href="http://aequitas.dssg.io/">Credits</a>.</div><h3>Fairlearn</h3><p><a href="https://fairlearn.org/">Fairlearn</a> is an open source library maintained by diverse contributors (from Microsoft, Zalando, …). It aims at tackling each step of the fairness value chain. It implements fairness metrics, of which you have a summary below:</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Fairlearn-1.jpg" alt="Figure complemented by datacraft&#x27;s initiative" title="Figure complemented by datacraft&#x27;s initiative"/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Credits: <a href="#reference">[1]</a> - complemented by datacraft’s ethical initiative.</div><p>Fairlearn implements mitigation techniques:</p><ul><li>Pre-processing methods: alter a training set before training a model (example in fairlearn: removing sensitive correlations)</li><li>In-processing method: train a model (or a sequence of models) accounting for fairness constraints (example in fairlearn: exponentiated gradient <a href="#reference">[2]</a>)</li><li>Post-processing methods: alter predictions to account for fairness constraints, once a model is trained (example in fairlearn: threshold optimization post processing algorithm <a href="#reference">[3]</a>)</li></ul><p>It also tries to go beyond the usual binary classification problem, which is the usual go-to when uncovering the fairness topic (e.g. giving a try at regression). However, making our way through the “get started” procedure or the documentation - yet well designed and appealing, was no easy task. Note that the library also comes with nice dashboards that allow, among others, model comparison.</p><p>In a nutshell: promising and active library for fairness topics. Accessibility could be improved. To definitely keep an eye on (or contribute to!).</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Fairlearn-2.jpg" alt="Screenshot of the Fairlearn dashboard." title="Screenshot of the Fairlearn dashboard."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of the Fairlearn dashboard. <a target="_blank" rel="noopener noreferrer" href="https://opendatascience.com/how-to-assess-ai-systems-fairness-and-mitigate-any-observed-unfairness-issues/">Credits</a>.</div><h3>AIF360</h3><p><a href="http://aif360.mybluemix.net/">AIF360</a> is an open source library developed by IBM. From our perspective, and before doing this initiative, this library was considered as the go-to for tackling fairness topics. It comes with an online tool, implements a wide range of mitigation techniques:</p><ul><li>Pre-processing methods among which reweighting <a href="#reference">[4]</a>, or learning fair representations <a href="#reference">[5]</a></li><li>In-Processing methods: grid search reduction <a href="#reference">[6, 7]</a></li><li>Post-processing methods: equalized odds postprocessing <a href="#reference">[8, 9]</a></li></ul><p>It also benefits from a wide community, and comes with a user-friendly web interface. </p><p>However rich in terms of features / mitigation techniques, the documentation is quite poor (it is not unusual to go and directly look into the source code to get answers). Besides, (useful) snippets of code are disseminated in various Jupyter notebooks, which slows down the appropriation. Last, some choices regarding data representation (formatting) and/or object declaration/instantiation (like the main explainer object, which is quite verbose), led us to troubles when trying to get used to the library.</p><p>In a nutshell: AIF360 is a very rich and mature ecosystem. Accessibility is however currently an obstacle to its full exploitation.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/AIF360.jpg" alt="Screenshot of AIF360." title="Screenshot of AIF360."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of AIF360. <a target="_blank" rel="noopener noreferrer" href="https://aif360.mybluemix.net/">Credits</a>.</div><h2>Conclusion</h2><p>At this point in time, we had discovered very interesting libraries, some of them backed by great communities and capabilities. Still, some open points remained that we thought would be worth investing time on:</p><ul><li>There was no clear winner: each library came with pros and cons. An ideal tool should be able to combine the best of each.</li><li>All those tools were very much focused (and still are) on the tooling, namely implementing a wide set of mitigation techniques or fairness dashboards. However, we were still missing a systematic framework for tackling fairness topics, that not only would make practical tools available, but that would also provide the associated reasoning: what question should a data scientist ask themselves? In which situation? Who should take part in this or that sensitive decision with respect to the model, …?</li></ul><p>This is what will be tackled in the third article of this series. We’ll introduce Dalex, another library that will be used as a foundation to derive (our interpretation of) the whole reasoning when exposed to fairness / ethical concerns.</p><h2>Reference</h2><p>[1]<!-- --> - Credits: Data Robot – Trusted AI 102: A Guide to Building Fair and Unbiased AI Systems<br/>
<!-- -->[2]<!-- --> - <a href="https://arxiv.org/abs/1803.02453">Agarwal et al. (2018)  A Reductions Approach to Fair Classification</a><br/>
<!-- -->[3]<!-- --> - <a href="https://arxiv.org/pdf/1610.02413.pdf">M. Hardt, E. Price, N. Srebro (2018) - Equality of Opportunity in Supervised Learning</a><br/>
<!-- -->[4]<!-- --> - F. Kamiran and T. Calders, &quot;Data Preprocessing Techniques for Classification without Discrimination,&quot; Knowledge and Information Systems, 2012<br/>
<!-- -->[5]<!-- --> - R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, &quot;Learning Fair Representations.&quot; International Conference on Machine Learning, 2013<br/>
<!-- -->[6]<!-- --> - <a href="https://arxiv.org/abs/1803.02453">A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach (2018) - A Reductions Approach to Fair Classification - International Conference on Machine Learning</a><br/>
<!-- -->[7]<!-- --> - <a href="https://arxiv.org/abs/1905.12843">A. Agarwal, M. Dudik, and Z. Wu (2019) - Fair Regression: Quantitative Definitions and Reduction-based Algorithms - International Conference on Machine Learning</a><br/>
<!-- -->[8]<!-- --> - M. Hardt, E. Price, and N. Srebro, &quot;Equality of Opportunity in Supervised Learning&quot; Conference on Neural Information Processing Systems, 2016.<br/>
<!-- -->[9]<!-- --> - G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, &quot;On Fairness and Calibration,&quot; Conference on Neural Information Processing Systems, 2017.</p>]]></content>
        <author>
            <name>Antoine Isnardy</name>
            <uri>mailto:antoine.isnardy@danone.com</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ?]]></title>
        <id>Des-biais-cognitifs-aux-biais-des-algorithmes</id>
        <link href="https://drkapichu.github.io/blog/Des-biais-cognitifs-aux-biais-des-algorithmes"/>
        <updated>2022-04-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[IA et biais (1/3) - L'humain, une source du problème.]]></summary>
        <content type="html"><![CDATA[<hr/><h1>Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ?</h1><h2>Pourquoi les biais posent des problèmes éthiques en intelligence artificielle ?</h2><h3>Nous avons tous des biais</h3><p>Imaginez que vous faîtes vos courses avec une personne aveugle. Vous arrivez devant un rayon de bananes et la personne vous demande ce que vous voyez. Qu’allez-vous lui décrire ? Il y a des bananes ? Il y a des bananes avec de petits autocollants dessus ? Il y a environ 30 bananes ? Il y a peu de chances pour que vous disiez qu’il y a des bananes qui sont de couleur jaune. Le jaune est une information typique pour une banane et on a tendance à ne pas mentionner les évidences. C&#x27;est une forme de biais, ou d&#x27;angle mort cognitif. Et nous avons tous des <a href="https://fr.wikipedia.org/wiki/Biais_cognitif">biais cognitifs</a>, c’est humain.</p><h3>Pourquoi les biais d’une IA sont un problème ?</h3><p>Nos biais humains sont un problème quand on les transmet à une intelligence artificielle. C’est un problème aussi bien éthique qu’en termes de performance du logiciel basé sur ces biais. Concrètement, en utilisant la technique de l’apprentissage machine, on va entraîner l’intelligence artificielle à reconnaître des <a href="https://www.blog.google/technology/ai/new-course-teach-people-about-fairness-machine-learning/">bananes</a> en lui montrant des images de bananes. Si toutes les images que je lui montre sont des bananes jaunes, le jour où l’intelligence artificielle va voir une banane verte, elle ne va pas penser que c’est une banane. Il y a des chances qu’elle l’associe plutôt à une autre chose verte qu’elle a déjà vue, comme un concombre par exemple.</p><p>Éthiquement, c’est un problème. Parce que, quand on parle de bananes, cela paraît anodin. Mais cela peut parfois conduire à des conséquences graves. En effet, même un humain bienveillant a des préjugés inconscients. Parfois, sans s’en rendre compte, des data scientists peuvent transmettre leurs biais au travers des données qu’ils sélectionnent pour construire des logiciels. Prenons l’exemple d’un système de vidéosurveillance automatisée utilisé par la police. La Chine est friande de ces systèmes prédictifs pour interpeler des suspects, comme l’a dénoncé <a href="https://www.hrw.org/news/2021/11/24/mass-surveillance-fuels-oppression-uyghurs-and-palestinians">Human Rights Watch</a> concernant l’usage du système de surveillance de masse IJOP (Integrated Joint Operation Platform). Imaginons de manière caricaturale que le logiciel a appris à reconnaître des personnes à partir d’un jeu de données composé à 90% de portraits d&#x27;hommes à la peau blanche. L’algorithme n’étant pas entraîné à reconnaître des femmes à la peau noire, il sera alors complètement inefficace sur cette population et pourrait mener à des erreurs judiciaires s’il était utilisé. Ainsi, le <a href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">logiciel de prédiction des récidives</a> COMPAS utilisé par des juges américains pour les assister dans leur verdict a surestimé systématiquement le risque de récidive des détenus afro-américains tandis que celui des blancs a été très sous-estimé.</p><p>Un algorithme fonctionne comme une recette de cuisine : avec des ingrédients, les données, et une recette, le code. Même si c’est la partie la plus visible, la réussite de la recette ne dépend pas tant du code que de la qualité des ingrédients utilisés, les données. S’ils sont de mauvaise qualité, quelle que soit la recette, le succès ne pourra pas être au rendez-vous. Les résultats des algorithmes ne dépendent donc pas que de la manière dont les programmeurs les ont écrits. Les biais ont des origines diverses. Les biais d&#x27;acquisition de données par exemple sont liés à la manière dont les données sont collectées. Par exemple, si on estime la moyenne mensuelle de clients d’un hôtel dans une station de ski en se basant uniquement sur les chiffres des mois d’hiver, on va surestimer le résultat. Le contexte est essentiel dans l’acquisition de données parce que, si la période de l&#x27;année à laquelle on collecte des données n&#x27;est pas représentative de l&#x27;année entière, le résultat sera faussé. Nos biais représentent un problème quand ils entraînent des conséquences discriminatoires mais ils sont le plus souvent positifs. En effet, dans le cas normal, nos biais sont aussi ce qui nous permet d&#x27;avancer et de prendre des décisions rapides en environnement inconnu, transmettant ainsi des informations bénéfiques aux algorithmes.</p><p>C’est donc à la fois un problème éthique et un problème de performance des algorithmes. Imaginons que la police décide du quartier où elle patrouille sur les suggestions d’une intelligence artificielle. Si le logiciel dispose de l’historique d’actes de délinquance de deux quartiers, l‘un avec un petit peu plus de délinquance que l’autre, il va orienter les policiers vers le quartier où il y en a eu un peu plus dans le passé. En arrivant dans ce quartier, si les policiers constatent une infraction, ils vont approvisionner la base d’apprentissage de nouvelles données. Alors que, s’ils avaient été dans l’autre quartier, ils auraient peut-être aussi constaté une infraction. Mais maintenant, l’intelligence artificielle est biaisée parce qu’elle considère que ce quartier connaît plus de délinquance. Avec ce biais, les algorithmes peuvent ainsi former ce qu’on appelle des “boucles de rétroaction” par lesquelles stéréotypes, <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">discriminations</a> et inégalités se renforcent mutuellement, contribuant ainsi à cristalliser durablement des situations d’inégalité. En plus de ne pas être performante, l’intelligence artificielle n’est alors pas non plus éthique. C’est ainsi qu’un <a href="https://www.courthousenews.com/audit-finds-lapd-predictive-policing-programs-lack-oversight/">audit critique</a> du logiciel de police prédictive de la ville de Los Angeles, PredPol, a amené à abandonner son usage.</p><h3>Personne n’est à l’abri de risques éthiques</h3><p>Les médias rapportent les cas les plus sensationnalistes, comme celui d’Amazon par exemple. Un <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">programme informatique</a> discriminait les femmes à l&#x27;embauche parce que les informations sur lesquelles l&#x27;intelligence artificielle avait basé son apprentissage étaient des effectifs historiquement masculins de développeurs informatiques.</p><p>Il existe un risque pour les professionnels qui créent des intelligences artificielles de se sentir éloignés de ces sujets sensationnalistes et, ne se sentant pas concernés, de ne pas faire attention aux biais dans leur travail. Par exemple, les équipes de Twitter qui ont travaillé sur l’algorithme qui choisit quelle partie d’une photo s’affiche en aperçu sur Twitter n’ont pas bien contrôlé les biais dans leur travail. Leur logiciel recadrait simplement l’image d’un utilisateur pour donner un aperçu pertinent de l&#x27;image. Pourtant, les <a href="https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm">utilisateurs ont remarqué</a> que, sur une photo avec plusieurs personnes, ce recadrage se focalisait plus sur les personnes blanches que les personnes noires. En effet, quand on se base sur les images présentes sur internet, les personnes blanches sont en moyenne plus mises en avant que les personnes noires.<br/>
<!-- -->Il est donc recommandé à toute personne travaillant sur une intelligence artificielle de toujours s’interroger sur les biais possibles qu’elle est susceptible d’y introduire, aussi peu risqué que le projet puisse paraître.</p><h3>La solution proposée par la Commission européenne aux problèmes d’éthique de l’IA</h3><p>Aujourd’hui, une des rares entités qui régule l’éthique est la Commission européenne avec une <a href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai">proposition de loi qui réglemente l’intelligence artificielle</a>. A l’instar de la mise en œuvre du règlement général sur la protection des données (RGPD), de nombreuses entreprises se plaignent que la Commission européenne complique la vie des ingénieurs parce qu’elle ne comprend pas les implications techniques de ce qu’elle ordonne. Du point de vue du citoyen toutefois, la réglementation protège la vie privée mieux qu’avant. D’autant que, ces dernières années, minimiser le risque d&#x27;un bad buzz lié à une violation de la vie privée est peut-être devenu plus important pour les sociétés que les contraintes techniques que le RGPD engendre et a été le catalyseur de bonnes pratiques pour éviter ce type de risque.</p><h3>Le travail de datacraft</h3><p>Des membres du Club <a href="https://datacraft.paris/">datacraft</a> se sont réunis tout au long de l&#x27;année 2021 pour apporter des réponses concrètes sur les questions d’IA éthique. C’est ainsi qu’ils ont abouti à la rédaction d’une <a href="https://datacraft.paris/project/trustworthy-ai-charter/">charte</a> pour formaliser les fondements d’une IA de confiance et mettre un accent particulier sur sa dimension éthique. Un groupe de data scientists issus d’entreprises (Antoine Isnardy de Danone, Théo Alves Da Costa d’Ekimetrics...), de labos de recherche (Nathan Noiry de Télécom Paris, Eliot Moll de l’Inria...), ou freelances en résidence datacraft (Yann Girard d’HephIA...) a aussi travaillé sur la <a href="https://github.com/datacraft-paris/ethical-ai-toolkit">cartographie</a> des outils open source du marché pour limiter les biais dans les modèles d’intelligence artificielle (tels que, entre autres, les biais sociaux comme les biais sur le genre, l’âge, …).<br/>
<!-- -->Il est toujours possible d&#x27;apporter votre contribution à ce travail en <a href="https://datacraft.paris/join-us/">rejoignant datacraft</a> !</p>]]></content>
        <author>
            <name>Stéphanie Lehuger</name>
            <uri>mailto:contact@datacraft.paris</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in AI - How a benchathon unlocked our knowledge]]></title>
        <id>AI-Fairness-et-benchathon</id>
        <link href="https://drkapichu.github.io/blog/AI-Fairness-et-benchathon"/>
        <updated>2022-04-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Benchmark of Python packages for AI fairness]]></summary>
        <content type="html"><![CDATA[<hr/><h1>Fairness in AI - How a benchathon unlocked our knowledge</h1><p>Let&#x27;s all board on a journey to the land of AI fairness that we, a group of private players (Danone, Ekimetrics, datacraft), researchers (Telecom Paris, Inria), and students (Université de Cergy), partnered to uncover fairness &amp; ethics in Artificial Intelligence from a practical standpoint. Through this journey the group tried to tackle the following challenge: “how should a Data Scientist concretely react when exposed to fairness concerns?”.  </p><p>We started with a series of workshops to discuss the existing tools and methods, assess the needs and define an <a href="https://datacraft-paris.github.io/trustworthyai/">ethical charter</a>. This led to the writing of a first popularisation article, in French, which you can find <a href="./biais-humains-et-algorithmes">here</a>.<br/>
<!-- -->Then we continued our journey with a benchathon - definition below - in which the group participated to get a quick and documented opinion of an already rich fairness/ethical ecosystem. It first explains how the concept of benchathon accelerated our practical grasp of the topic, and then explores the first conclusions drawn about the fairness ecosystem. You can find the code developped during this benchathon on the <a href="https://github.com/datacraft-paris/Fairness-Benchathon">datacraft&#x27;s GitHub repo</a>.</p><h2>Benchathon as an innovation catalyst</h2><p>At this stage of our “fairness journey”, we had a decent high level understanding of what fairness could imply in real life. It was then the right time to start acting concrete: try and derive a pragmatic methodology, even if it implied implementing our own routines.</p><p>To that end, a very first step was to make sure we’d not reinvent the wheel, and we’d plainly benefit from existing open source contributions. This happened during a one-day benchathon. If you work in tech, you may already be familiar with the following two notions:</p><ul><li>Benchmark: <em>gathering and comparing qualitative information about how an activity is conducted through people, processes, and technology</em> (Source: <a href="https://www.apqc.org/blog/what-are-four-types-benchmarking">https://www.apqc.org/blog/what-are-four-types-benchmarking</a>)  </li><li>Hackathon: <em>[short]<!-- --> event <!-- -->[...]<!-- --> in which computer programmers and others involved in software development <!-- -->[...]<!-- --> collaborate intensively on software projects</em> (Source: <a href="https://en.wikipedia.org/wiki/Hackathon">https://en.wikipedia.org/wiki/Hackathon</a>)  </li></ul><p>Hackathons usually involve teams that compete on the “same topic” for 2 to 3 days. Because we were limited in time - 1 day, rather than focusing all on the same “thing”, we decided to make the most out of the presence of 9 data scientists: we shared and split between us the technical analysis of several fairness open source libraries - <a href="https://github.com/Trusted-AI/AIF360">AIF360</a>, <a href="https://github.com/MAIF/shapash">Shapash</a>, <a href="https://github.com/dssg/aequitas">Aequitas</a>, <a href="https://pair-code.github.io/what-if-tool/">What if tool</a>, <a href="https://fairlearn.org/">Fairlearn</a>. Hence the concept of benchathon.</p><p>Even though all of us were entitled as “Data Scientists”, we all came from different structures, different backgrounds, and different (coding) habits. That diversity definitely triggered (and still does) great discussions and perspectives along the initiative. Still, an important step during the benchathon was to settle on an interpretation grid that would make the outcome as reusable and general as possible, and as unbiased as possible - in line with the topic then :). A few criteria were identified:</p><table><tr><th>   <strong>Criteria</strong>                  </th><th>    <strong>Description</strong>                                                                                                             </th><th>   <strong>Scale</strong>           </th>  </tr><tr><td align="left" bgcolor="white"> Installation  </td><td align="left" bgcolor="white"> How easy is it to get started?                                                                               </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Usability     </td><td align="left" bgcolor="white"> How easy to use is the API?                                                                                  </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Documentation </td><td align="left" bgcolor="white"> How well documented is the library?                                                                          </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Completeness  </td><td align="left" bgcolor="white"> Does the library perform everything it is supposed to?                                                       </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Reliability   </td><td align="left" bgcolor="white"> Does the library seem reliable? (code quality, tests, …)                                                     </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Legitimacy    </td><td align="left" bgcolor="white"> Is the library popular within the community? (number of stars on GitHub, latest commit, number of issues, …) </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Future        </td><td align="left" bgcolor="white"> Gut instinct - would you trust it and use it in real projects?                                               </td><td align="left" bgcolor="white"> Y/N </td>  </tr><tr><td align="left" bgcolor="white"> Weaknesses    </td><td align="left" bgcolor="white"> What is currently missing?                                                                                   </td><td align="left" bgcolor="white"> N/A </td>  </tr></table><p>That being set, what was important was also to pace the day, so that despite the fact that small groups worked independently, we always kept an overall coherence and dynamics. It meant:</p><ul><li>Mini sprints of 1,5 hours</li><li>At the end of each mini sprint, a quick roundtable to share insights or blocking points, and get challenged by the whole group</li><li>Lunch break altogether: everyone brought something to share. This was a great moment of conviviality. It would even seem that a new datacraft initiative was born at this very moment, stay tuned!</li><li>At the end of the day, wrap up session during which each group made a demo of the library it spent the day on, and made sure to fill out above-mentioned criteria. The latter was especially important because this is what helps us today to have a concrete reference that every one can refer to.</li></ul><p>Taking a step back, below are a few takeaways:</p><ul><li>This benchathon was extremely productive: in the matter of only a day, our practical grasp of the fairness/ethical ecosystem clearly passed a milestone (see next section).</li><li>All people around the table had a developer background, and the same objective - namely, uncovering the fairness topic from a technical &amp; practical standpoint. It helped to get started fairly quickly, and proved that this format was a great fit for that audience and purpose.</li><li>One mistake we made was not to invest enough time beforehand in mapping the main open source libraries available in the AI community. It turns out we missed what would become our GO TO in the future: <a href="https://github.com/ModelOriented/DALEX">Dalex</a>.</li></ul><h2>Highlight of 6 ethical/fairness libraries</h2><p>If you are further interested in the exhaustive findings on the five libraries that were studied during the benchathon, we summarized our conclusions in this <a href="https://github.com/datacraft-paris/Fairness-Benchathon/blob/master/docs/Benchathon-summary.pdf">file</a>. The following section aims at providing a (subjective) summary of these libraries, in increasing relevance order, with respect to fairness / ethics.</p><h3>Shapash</h3><p><a href="https://github.com/MAIF/shapash">Open source library</a> developed by MAIF - a French insurance actor, and Quantmetry - a French AI consultancy, that mainly focuses on interpretability (no built-in fairness-oriented feature). It acts as a layer on top of the usual interpretability toolbox (feature importance, SHAP values, …). It comes with a very decent web interface, high quality code, and a great community/documentation. It also provides an audit report of the project (from data prep to modeling, to exploratory analysis).</p><p>In a nutshell: great project, but not that relevant (yet?) for fairness topics.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Shapash.jpg" alt="Screenshot of Shapash." title="Screenshot of Shapash."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of Shapash. <a target="_blank" rel="noopener noreferrer" href="https://github.com/MAIF/shapash">Credits</a>.</div><h3>What if tool</h3><p><a href="https://pair-code.github.io/what-if-tool/">Open source interface</a> developed by Google. It mainly aims at conducting counterfactual analysis (“what would be the machine learning model prediction if we changed the value of that particular attribute, like the sex e.g.?”). It comes with a decent web interface, especially to deal with unstructured data like images. Documentation is however not handy to deal with.</p><p>In a nutshell: great interface. However, counterfactual analysis is only one (important) feature among the different aspects related to fairness, which in turn does not justify a lock-in with that specific tool.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/What_if_tool.jpg" alt="Screenshot of What-if tool." title="Screenshot of What-if tool."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of What-if tool. <a target="_blank" rel="noopener noreferrer" href="https://github.com/pair-code/what-if-tool">Credits</a>.</div><h3>Aequitas</h3><p><a href="http://aequitas.dssg.io/">Aequitas</a> is a bias and audit toolkit developed by Carnegie Mellon University. It aims at spotting unfair allocation compared to population repartition or wrong decisions about certain groups of people. It comes with a web interface (which we could not make work) and a Python library to help compute fairness metrics. Documentation is decent, especially their representation of the <a href="http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/">&quot;fairness tree&quot;</a>, which helps to navigate the (many and ambiguous) fairness metrics, depending on the use case.</p><p>In a nutshell: Aequitas is a tool that has been available for quite some time now, but that does not benefit from a living community. To be kept under the radar (or contribute to!).</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Aequitas.jpg" alt="Screenshot of Aequitas web application." title="Screenshot of Aequitas web application."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of Aequitas web application. <a target="_blank" rel="noopener noreferrer" href="http://aequitas.dssg.io/">Credits</a>.</div><h3>Fairlearn</h3><p><a href="https://fairlearn.org/">Fairlearn</a> is an open source library maintained by diverse contributors (from Microsoft, Zalando, …). It aims at tackling each step of the fairness value chain. It implements fairness metrics, of which you have a summary below:</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Fairlearn-1.jpg" alt="Figure complemented by datacraft&#x27;s initiative" title="Figure complemented by datacraft&#x27;s initiative"/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Credits: <a href="#reference">[1]</a> - complemented by datacraft’s ethical initiative.</div><p>Fairlearn implements mitigation techniques:</p><ul><li>Pre-processing methods: alter a training set before training a model (example in fairlearn: removing sensitive correlations)</li><li>In-processing method: train a model (or a sequence of models) accounting for fairness constraints (example in fairlearn: exponentiated gradient <a href="#reference">[2]</a>)</li><li>Post-processing methods: alter predictions to account for fairness constraints, once a model is trained (example in fairlearn: threshold optimization post processing algorithm <a href="#reference">[3]</a>)</li></ul><p>It also tries to go beyond the usual binary classification problem, which is the usual go-to when uncovering the fairness topic (e.g. giving a try at regression). However, making our way through the “get started” procedure or the documentation - yet well designed and appealing, was no easy task. Note that the library also comes with nice dashboards that allow, among others, model comparison.</p><p>In a nutshell: promising and active library for fairness topics. Accessibility could be improved. To definitely keep an eye on (or contribute to!).</p><p><img src="./img/2022-04-01-AIFairness-benchathon/Fairlearn-2.jpg" alt="Screenshot of the Fairlearn dashboard." title="Screenshot of the Fairlearn dashboard."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of the Fairlearn dashboard. <a target="_blank" rel="noopener noreferrer" href="https://opendatascience.com/how-to-assess-ai-systems-fairness-and-mitigate-any-observed-unfairness-issues/">Credits</a>.</div><h3>AIF360</h3><p><a href="http://aif360.mybluemix.net/">AIF360</a> is an open source library developed by IBM. From our perspective, and before doing this initiative, this library was considered as the go-to for tackling fairness topics. It comes with an online tool, implements a wide range of mitigation techniques:</p><ul><li>Pre-processing methods among which reweighting <a href="#reference">[4]</a>, or learning fair representations <a href="#reference">[5]</a></li><li>In-Processing methods: grid search reduction <a href="#reference">[6, 7]</a></li><li>Post-processing methods: equalized odds postprocessing <a href="#reference">[8, 9]</a></li></ul><p>It also benefits from a wide community, and comes with a user-friendly web interface. </p><p>However rich in terms of features / mitigation techniques, the documentation is quite poor (it is not unusual to go and directly look into the source code to get answers). Besides, (useful) snippets of code are disseminated in various Jupyter notebooks, which slows down the appropriation. Last, some choices regarding data representation (formatting) and/or object declaration/instantiation (like the main explainer object, which is quite verbose), led us to troubles when trying to get used to the library.</p><p>In a nutshell: AIF360 is a very rich and mature ecosystem. Accessibility is however currently an obstacle to its full exploitation.</p><p><img src="./img/2022-04-01-AIFairness-benchathon/AIF360.jpg" alt="Screenshot of AIF360." title="Screenshot of AIF360."/></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em"> Screenshot of AIF360. <a target="_blank" rel="noopener noreferrer" href="https://aif360.mybluemix.net/">Credits</a>.</div><h2>Conclusion</h2><p>At this point in time, we had discovered very interesting libraries, some of them backed by great communities and capabilities. Still, some open points remained that we thought would be worth investing time on:</p><ul><li>There was no clear winner: each library came with pros and cons. An ideal tool should be able to combine the best of each.</li><li>All those tools were very much focused (and still are) on the tooling, namely implementing a wide set of mitigation techniques or fairness dashboards. However, we were still missing a systematic framework for tackling fairness topics, that not only would make practical tools available, but that would also provide the associated reasoning: what question should a data scientist ask themselves? In which situation? Who should take part in this or that sensitive decision with respect to the model, …?</li></ul><p>This is what will be tackled in the third article of this series. We’ll introduce Dalex, another library that will be used as a foundation to derive (our interpretation of) the whole reasoning when exposed to fairness / ethical concerns.</p><h2>Reference</h2><p>[1]<!-- --> - Credits: Data Robot – Trusted AI 102: A Guide to Building Fair and Unbiased AI Systems<br/>
<!-- -->[2]<!-- --> - <a href="https://arxiv.org/abs/1803.02453">Agarwal et al. (2018)  A Reductions Approach to Fair Classification</a><br/>
<!-- -->[3]<!-- --> - <a href="https://arxiv.org/pdf/1610.02413.pdf">M. Hardt, E. Price, N. Srebro (2018) - Equality of Opportunity in Supervised Learning</a><br/>
<!-- -->[4]<!-- --> - F. Kamiran and T. Calders, &quot;Data Preprocessing Techniques for Classification without Discrimination,&quot; Knowledge and Information Systems, 2012<br/>
<!-- -->[5]<!-- --> - R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, &quot;Learning Fair Representations.&quot; International Conference on Machine Learning, 2013<br/>
<!-- -->[6]<!-- --> - <a href="https://arxiv.org/abs/1803.02453">A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach (2018) - A Reductions Approach to Fair Classification - International Conference on Machine Learning</a><br/>
<!-- -->[7]<!-- --> - <a href="https://arxiv.org/abs/1905.12843">A. Agarwal, M. Dudik, and Z. Wu (2019) - Fair Regression: Quantitative Definitions and Reduction-based Algorithms - International Conference on Machine Learning</a><br/>
<!-- -->[8]<!-- --> - M. Hardt, E. Price, and N. Srebro, &quot;Equality of Opportunity in Supervised Learning&quot; Conference on Neural Information Processing Systems, 2016.<br/>
<!-- -->[9]<!-- --> - G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, &quot;On Fairness and Calibration,&quot; Conference on Neural Information Processing Systems, 2017.</p>]]></content>
        <author>
            <name>Antoine Isnardy</name>
            <uri>mailto:antoine.isnardy@danone.com</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A list of possible styles (stolen everywhere)]]></title>
        <id>html</id>
        <link href="https://drkapichu.github.io/blog/html"/>
        <updated>2022-03-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Ceci est une aide-mémoire pour liste les possibilités de styles dans du MarkDown]]></summary>
        <content type="html"><![CDATA[<p>Ceci est un lien vers un site qui liste les balises HTML avec des exemples (!!) : <a href="https://developer.mozilla.org/fr/docs/Web/HTML/Element/Img">https://developer.mozilla.org/fr/docs/Web/HTML/Element/Img</a></p><p>Essentially stupid thing you can find anywhere on the web because they&#x27;re basics. But you&#x27;re lazy man!!</p><h1>Ceci est une liste avec un titre</h1><h2>Etre un bénévole Data For Good</h2><p>Mais en pratique être bénévole cela veut dire : </p><ul><li>Faire partie d&#x27;une communauté tech engagée</li><li>Participer aux saisons d&#x27;accélération</li><li>... Bref, Data For Good est une communauté libre et indépendante, tu peux proposer ce que tu veux !</li></ul><p>Pour plus de détails tu peux lire la page <a href="https://dataforgood.slite.com/p/channel/F9UR6bhuYCPAtvfLDje8Zc/notes/t1KTZaDgs">suivante</a>. </p><hr/><h2>Le Slack Data For Good</h2><p>Slack est un outil de messagerie communautaire, c&#x27;est aujourd&#x27;hui le coeur de la communauté où se passe les discussions entre volontaires, l&#x27;organisation autour des projets, et où se partagent évènements, offres d&#x27;emploi, liens et actualités de l&#x27;association. Pour rejoindre le Slack, c&#x27;est simple il suffit de remplir le questionnaire ci-dessus. </p><p><img src="./img/2022-03-02-CSSStyle/slack.png" alt="slack"/></p><hr/><h1>Internal links</h1><p>Visitez la page <a href="/blog">🔥 Blog</a> pour découvrir nos accélérations depuis 2014.</p><hr/><h1>Un bouton cliquable qui redirigent vers un site extérieur</h1><div style="text-align:center;margin-bottom:20px"><a href="https://airtable.com/shrPjA75ckEgQdPUF" target="_blank" class="button button--secondary button--lg button-home"> Rejoindre la communauté - 5min ⏱</a></div><iframe id="inlineFrameExample" title="Inline Frame Example" width="300" height="200" src="https://www.openstreetmap.org/export/embed.html?bbox=-0.004017949104309083%2C51.47612752641776%2C0.00030577182769775396%2C51.478569861898606&amp;layer=mapnik"></iframe><hr/><h1>Include an audio reader</h1><figure><figcaption>Listen to the T-Rex:</figcaption><audio controls="" src="roar.wav" type="audio/wav">Your browser does not support the <code>audio</code> element.</audio></figure><audio controls="" src="Kalimba.mp3">Your browser does not support the <code>audio</code> element.</audio><hr/><h1>Un lecteur intégré de vidéos (Youtube)</h1><iframe width="85%" height="500px" src="https://www.youtube.com/embed/0xtPrTo-13o" alt="Présentation de datacraft par Isabelle" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><hr/><h1>Faire des blocs tips, infos,</h1><p>:::tip On peut mettre un titre ici (pas obligatoire)</p><p><strong>La charte va au-delà du cadre législatif</strong> afin de promouvoir l’utilisation éthique des données et de prévenir de potentiels scandales liés aux données et à l’intelligence artificielle. L’objectif est de faire grandir la responsabilité individuelle et collective des data scientists en suscitant une réflexion et des échanges sur l’impact social de leur activité professionnelle.</p><p>:::</p><p>:::info</p><p><strong>La charte va au-delà du cadre législatif</strong> afin de promouvoir l’utilisation éthique des données et de prévenir de potentiels scandales liés aux données et à l’intelligence artificielle. L’objectif est de faire grandir la responsabilité individuelle et collective des data scientists en suscitant une réflexion et des échanges sur l’impact social de leur activité professionnelle.</p><p>:::</p><p>:::info Data For Good</p><p>La communauté Data for Good compte plus de 2500 volontaires qui consacrent plusieurs heures par semaine au service de projets d&#x27;intérêt général.</p><p>Vous êtes Data Scientists/Analyst/Engineers, Developers, UX/UI designer, ou Project Manager ? <a href="https://airtable.com/shrPjA75ckEgQdPUF">Rejoignez-nous</a> !</p><p>:::</p><p>:::tip Rejoindre la communauté Data For Good</p><p>Pour rejoindre la communauté, il vous suffit de remplir ce <a href="https://airtable.com/shrPjA75ckEgQdPUF">questionnaire</a> !<br/>
Il vous sera donné un accès au <a href="#le-slack-data-for-good">Slack</a> qui est le coeur de la communauté.</p><p>:::</p><hr/><h1>Faire des capsules dépliantes</h1><details><summary><p>Principe #4 - Ne pas collecter ou utiliser de <strong>données inutilement personnelles et/ou sensibles</strong>.</p></summary><div><p><em>Etape projet (2): Je collecte ou je dispose de données</em></p><p>... et une « donnée sensible » ?</p><p>L’<a href="https://www.cnil.fr/fr/reglement-europeen-protection-donnees/chapitre2#Article9">article 9 du Règlement Général sur la Protection des Données (RGPD)</a> prévoit que « le traitement des données à caractère personnel qui révèle l&#x27;origine raciale ou ethnique, les opinions politiques, les convictions religieuses ou philosophiques ou l&#x27;appartenance syndicale, ainsi que le traitement des données génétiques, des données biométriques aux fins d&#x27;identifier une personne physique de manière unique, des données concernant la santé ou des données concernant la vie sexuelle ou l&#x27;orientation sexuelle d&#x27;une personne physique sont interdits ».</p></div></details><p>Ici un peu de texte.</p><hr/><h1>Une image et du texte côte à côte.... Ne marche pas !!</h1><section class="light-green"><div class="container main-section"><h1>L&#x27;association Data For Good</h1><div class="row"><div class="col col--6"><img src="./img/events.jpg" alt="dfg-demoday" style="width:100%;margin-bottom:20px"/></div><div class="col col--6" style="text-align:left;align-content:center"><p>Data For Good est une association loi 1901 (<i>100% bénévole, 100% open-source, 100% citoyenne</i>) créée en 2014 qui rassemble une communauté de <b>2700+ volontaires</b> tech (Data Scientists, Data Analysts, Data Engineers, Developers, UX/UI Designers, Product &amp; Project Owners) souhaitant mettre leurs compétences au profit d&#x27;associations, d&#x27;ONG, et de l&#x27;ESS - et de s&#x27;engager pour l&#x27;intérêt général.</p><p>Nous réalisons chaque année des <b>saisons d&#x27;accélération où une dizaine de projets sont accompagnés par les bénévoles sur des thématiques sociales, sociétales et environnementales</b>. Nous avons ainsi accompagné, accéléré et co-construits plus de 100 projets depuis 2014.</p><p>Nous sommes également fervents <b>critiques des risques et des dérives de la technologie</b>, faire partie de la communauté est aussi s&#x27;engager pour une technologie sobre et respectueuse des enjeux sociaux et environnementaux, et accepter que la technologie n&#x27;est pas la solution à tous les problèmes.</p></div></div></div></section><hr/><h1>Different kinds of links</h1><ul><li>Airtable : <a href="https://airtable.com/privacy">https://airtable.com/privacy</a> </li><li>Mailchimp : <a href="https://mailchimp.com/legal/privacy/">https://mailchimp.com/legal/privacy/</a> </li><li>Slack : <a href="https://slack.com/trust/privacy/privacy-policy">https://slack.com/trust/privacy/privacy-policy</a></li></ul>]]></content>
        <author>
            <name>Julien Guyot</name>
            <uri>https://datacraft.paris/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This is a test for the actions]]></title>
        <id>testou</id>
        <link href="https://drkapichu.github.io/blog/testou"/>
        <updated>2022-02-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[C'est un test pour faire fonctionner l'action GitHub]]></summary>
        <content type="html"><![CDATA[<p>(setq markdown-css-paths &#x27;(&quot;./custom_blog.css&quot;))</p><p>My fucking test</p><p>This is a fucking new post with GitHub automatic Actions!</p><p>.</p><p>.</p><p>.</p><p><strong>Cet article est payant. TOutefois je l&#x27;ai sur mon ordi, dans les documents de l&#x27;égypto, mais il est particulièrement long et structuré d&#x27;une manière que je ne pourrais pas reproduire en markdown.</strong></p><p><strong>Que faire ?</strong>
.</p><p>.</p><p>.</p><hr/><h1>Un titre</h1><p><img src="./img/2022-02-28-GitHubActions/Gozilla6.jpg" alt="image" title="**Some text**"/></p><p>And here I write some text to have something to commit!!!</p><p>This is a new day, a new year, a new life.
Another brick in the wall..
Tada</p><link href="./custom_blog.css" rel="stylesheet"/><p class="my_style">Ceci est un test </p><p class="red">red fucking text</p><p>&lt;my_style&gt;this is a test&lt;/my_style&gt;</p><script src="https://gist.github.com/ollytheninja/8498790.js"></script><hr/><abbr> this is a test </abbr>]]></content>
        <author>
            <name>Julien Guyot</name>
            <uri>https://datacraft.paris/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comment veiller à ce que les biais humains n’imprègnent pas les algorithmes ?]]></title>
        <id>biais-humains-et-algorithmes</id>
        <link href="https://drkapichu.github.io/blog/biais-humains-et-algorithmes"/>
        <updated>2022-02-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article publié dans Usbek & Rica le 18 février 2022]]></summary>
        <content type="html"><![CDATA[<hr/><div style="margin-top:1em;margin-bottom:2em"><div class="warning" style="font-size:28px;color:#69337A;padding:1.0em"> Stephanie Lehuger, Thinker et Entrepreneur, membre du <u>club datacraft</u>, qui réfléchit aux questions éthiques soulevées par l’IA, nous explique dans cette tribune de quels outils et méthodes nous disposons actuellement pour qu’un préjugé humain ne se retrouve pas dans l’algorithme d’une intelligence artificielle.</div></div><p>Tous les humains ont des biais cognitifs, c’est inévitable. Et les data scientists sont des humains, donc ils sont fatalement sujets aux biais, comme tout le monde. Pour établir des connaissances, les data scientists analysent des données. Et ces données, si elles sont mal choisies, donnent de mauvais résultats. Ainsi, un biais cognitif se transforme en biais de donnée qui se transforme ensuite en biais algorithmique.</p><p>On peut dire qu’un algorithme fonctionne comme une recette de cuisine où les ingrédients seraient les données et la recette le code : si les ingrédients (les données) sont de mauvaise qualité, avec des biais par exemple, le résultat ne peut qu’être décevant. La plupart du temps, les biais proviennent des données et cela se produit de deux manières.</p><p>En premier lieu, ils peuvent être le résultat d’une mauvaise collecte. Imaginons par exemple qu’on cherche à déterminer le loyer moyen que paient les gens qui louent leur logement. Si les data scientists sont parisiens et récupèrent la base de données de leur ville, ils vont obtenir un résultat élevé par rapport à la moyenne nationale. Il sera biaisé par les loyers de Paris.</p><p>La transmission d’un biais s’effectue donc au travers des données choisies (la « data »). Si les data scientists n’ont pas conscience que les loyers sont moins élevés dans les villes de taille moyenne et en zone rurale que dans les grandes villes et qu’ils entraînent un algorithme à prédire le prix du loyer sur ces données-là, alors ses prédictions seront biaisées aussi. Le biais d’une IA peut provenir à l’origine d’un biais cognitif humain, qui se transmet dans les données choisies qui sont biaisées, puis elles influencent ensuite les résultats en s’étant transformées en un biais algorithmique.</p><div style="margin-top:1em;margin-bottom:2em"><div class="warning" style="font-size:24px;color:#69337A;border-left:solid #805AD5 4px;padding:0.7em"> S’il existe une discrimination des femmes dans une entreprise, se baser sur les données passées pour évaluer le potentiel d’une candidate, même plus brillante qu’un concurrent masculin, lui sera défavorable</div></div><p>En deuxième lieu, les biais peuvent émaner d’une situation déjà biaisée et qu’un algorithme pourrait amplifier. Comme une intelligence artificielle qui baserait son apprentissage sur des données historiquement biaisées. Si, depuis toujours, il existe une discrimination des femmes dans une entreprise, se baser sur les données passées pour évaluer le potentiel d’une candidate, même plus brillante qu’un concurrent masculin, lui sera défavorable. Si, historiquement, les femmes sont peu représentées, l’algorithme pourra en déduire de manière erronée qu’elles ont un profil moins désirable.</p><h2> Quelques exemples de biais communs </h2><p>Un biais typique qu’il faut tenter d’éviter est le biais des survivants. Par exemple, quand on constate que des bâtiments de plus de cent ans sont encore debout, on a l’impression que la « construction d’antan » était de meilleure qualité qu’aujourd’hui. Pourtant, quand on y réfléchit, la quasi-totalité de ce qui a été construit depuis l’invention de la construction s’est en fait écroulée ou a été démolie, donc ces bâtiments « survivants » sont des exceptions.</p><div style="margin-top:1em;margin-bottom:2em"><div class="warning" style="font-size:24px;color:#69337A;border-left:solid #805AD5 4px;padding:0.7em"> Les data scientists doivent éviter le biais du survivant qui consisterait à tirer des conclusions sur la base d’une population incomplète</div></div><p>Lors de l’étude de données par des data scientists, il leur faut éviter le biais du survivant qui consisterait à tirer des conclusions sur la base d’une population incomplète, comportant uniquement les éléments ayant « survécu », qui sont en fait des exceptions, plutôt que des cas représentatifs.</p><p>En France, des « antivax » ont été victimes du paradoxe de Simpson. Ils ont assuré à tort sur les réseaux sociaux que les non-vaccinés ne saturent pas les services de réanimation du pays, en s’appuyant sur des données de la Drees mal interprétées. Leur erreur principale est d’avoir regardé les chiffres bruts au lieu des pourcentages. Il y a en effet neuf fois plus de vaccinés que de non-vaccinés en France. </p><p>Alors que les non-vaccinés sont très minoritaires, ils sont surreprésentés à l’hôpital, avec 63 % des admissions en soins critiques. Si on regarde les chiffres absolus, on peut avoir l’impression que les deux populations sont en nombre équilibrés dans les hôpitaux, mais ce serait oublier de regarder la proportion de chacune dans la population générale. C’est ainsi que plusieurs enquêtes récentes menées sur des échantillons d’hôpitaux ont conclu que les non-vaccinés représentaient entre 70 % et 90 % dans les services de réanimation.</p><h2> Comment les data scientists corrigent les biais ? </h2><p>Une fois qu’on a identifié pourquoi et comment les biais posent un problème aux data scientists, on va s’intéresser à ce que les data scientists font, ne font pas, et devraient faire pour limiter les risques liés à ces biais.</p><h3> 1. Prendre conscience du problème et se poser les bonnes questions </h3><p>Pour prendre conscience du problème des biais cognitifs, les data scientists ont accès à différents types de ressources. Ils peuvent par exemple commencer à s’informer par le biais d’une charte, comme la charte éthique élaborée au sein de datacraft. Ils peuvent par ailleurs analyser le contenu des référentiels d’évaluation de l’IA de confiance, comme celui de Labelia Labs (ex-Substra Foundation) ou celui du LNE. Les serments établissent également une liste pertinente de critères d’une IA responsable, comme le font Tech pledge et Holberton-Turing Oath. Enfin, il existe des outils pratiques comme la checklist de Data Science éthique deon, accessible en ligne de commande.</p><p>Il est important d’avoir un esprit critique sur son propre travail quand on est data scientist. Si on ne devait choisir que 3 questions à se poser absolument, voici ce que je propose :</p><ul><li>S’engager à faire une pause pour s’interroger sur toutes les conséquences de son travail, qu’elles soient voulues ou non;</li><li>Contrôler les conséquences de son travail dans le temps;</li><li>Tendre vers l’autorégulation à l’aide de référentiel d’évaluation, de certification avec audit), en complément des « 7 points de vigilance » soulignés par la Commission européenne.</li></ul><h3> 2. Mesurer les biais </h3><p>Après avoir pris conscience de la potentielle existence de biais, la seconde étape consiste à définir des métriques appropriées afin de les mesurer convenablement. Le choix des métriques dépend alors essentiellement de ce que l’on cherche à contrôler. Aequitas est une boîte à outils open source pour auditer les biais, créée par le Center for Data Science and Public Policy de l’Université de Chicago. </p><p>Elle permet de vérifier les prédictions des outils d’évaluation des risques basés sur l’apprentissage automatique afin de comprendre les différents types de biais et de prendre des décisions éclairées sur le développement et le déploiement de ces systèmes. Le « fairness tree » aide à choisir la bonne métrique. Là comme ailleurs, il convient d’être attentif aux choix réalisés puisqu’il existe un nouveau biais possible. En effet, il faut avoir conscience que, en choisissant une métrique, on écarte toutes les autres.</p><div style="margin-top:1em;margin-bottom:2em"><div class="warning" style="font-size:24px;color:#69337A;border-left:solid #805AD5 4px;padding:0.7em"> Tout choix concernant une population étudiée devient moral en data science</div></div><p>Des métriques faciles d’accès pour les data scientists sont également mises en œuvre. Une équipe au sein de datacraft a réalisé une cartographie de 5 « fairness open source libraries » lors d’un benchathon : AIF360, Shapash, Aequitas, What if tool, Fairlearn.</p><p>Tout choix concernant une population étudiée devient moral en data science. En dehors des très grands groupes qui ont conscience des risques réputationnels forts auxquels ils sont soumis s’ils ne font pas attention aux biais et à leurs conséquences, cela reste principalement une question qui tient du ressort individuel dans les autres entreprises.</p><p>Même si ce n’est pas obligatoire légalement, il revient donc aux data scientists d’être moralement critiques sur leurs choix de données. Tout comme il est nécessaire d’être prudent pour ne pas introduire de biais dans les algorithmes qu’ils développent.</p><h3> 3. Limiter les risques de biais </h3><p>Il existe de nombreuses méthodes pour réduire les biais, que l’on peut diviser en trois grandes familles selon que l’intervention du praticien se situe avant, pendant ou après l’entraînement de l’algorithme. Avant l’entraînement, ces méthodes consistent à transformer les données à disposition, par exemple en les repondérant.</p><p>Concrètement, on peut revoir la pondération du nombre de personnes dans un jeu de données pour s’assurer qu’il y a autant d’hommes que de femmes et ainsi éviter un biais de genre. Pendant l’entraînement, il s’agit d’incorporer des contraintes d’équité à satisfaire, en complément des objectifs de performance classique. Enfin, les méthodes dites de post-traitement consistent à modifier les décisions des algorithmes, par exemple en favorisant les sous-groupes discriminés.</p><div style="margin-top:1em;margin-bottom:2em"><div class="warning" style="font-size:24px;color:#69337A;border-left:solid #805AD5 4px;padding:0.7em"> Pour qu’une entreprise soit à la fois juste et profitable, toutes les parties prenantes doivent échanger pour parvenir à des compromis acceptables</div></div><p>En diminuant les biais d’un côté, on diminue généralement la performance des algorithmes de l’autre : on parle du fairness-accuracy tradeoff. Pour qu’une entreprise soit à la fois juste et profitable, toutes les parties prenantes doivent échanger pour parvenir à des compromis acceptables. Les data scientists, les décideurs business ou encore les équipes de gouvernance sont impliqués dans ce processus complexe afin d’aboutir à un arbitrage. Une fois la décision prise, l’algorithme déployé et mis en production, il est indispensable de mettre en place une politique de supervision en temps réel (monitoring) afin de détecter de possibles changements de comportement du modèle.</p><p>Pour résumer, afin d’éviter de transmettre des biais humains à une intelligence artificielle, les data scientists doivent faire preuve d’esprit critique vis-à-vis de leurs possibles préjugés inconscients quand ils sélectionnent leurs données et construisent leurs algorithmes. Il n’y a malheureusement pas de méthode miracle qui marche à tous les coups. Un modèle repose sur des hypothèses dépendantes d’un contexte, elles seront donc différentes pour chaque problème, sans qu’un modèle magique fonctionne pour tous.</p>]]></content>
        <author>
            <name>Stéphanie Lehuger</name>
            <uri>mailto:contact@datacraft.paris</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few Shot Learning - application de la méthode iPET]]></title>
        <id>draft</id>
        <link href="https://drkapichu.github.io/blog/draft"/>
        <updated>2022-02-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Draft of the first blog]]></summary>
        <content type="html"><![CDATA[<p>(<a href="https://numenta.com/blog/2019/08/30/case-for-sparsity-in-neural-networks-part-1-pruning">source</a> de l&#x27;image de présentaion)</p><hr/><h2>Qu’est ce que le Few Shot Learning (FSL) ? - titre alternatif : Sujet du jour : le Few Shot Learning (FSL)</h2><p>Il est bien connu que la puissance des méthodes de Machine Learning supervisées, et plus particulièrement de Deep Learning avec les réseaux de neurones, depuis le début des années 2000, a reposé sur la constitution de <strong>grands jeux de données labellisés</strong>. Deux éléments sont importants ici : ‘grands’ et ‘labellisés’.</p><p>Pour le premier point, ça représente par exemple des milliers, voire des millions d’images pour la Computer Vision et des millions d’ensembles de phrases pour le NLP. Concernant le second point, il signifie qu’au cours de son apprentissage, l’ordinateur compare son évaluation des données avec le label qu’un intervenant humain a associé à chaque donnée.</p><p>Dans le cas du <strong>Few Shot Learning (FSL)</strong>, les chercheurs veulent créer des méthodes capables d’apprendre avec peu de données, i.e. des dizaines ou des centaines, ce qui représente un gain de temps et d’énergie, tout en conservant des performances équivalentes aux modèles traditionnels bien sûr. C’est pourquoi en français on parle d’<strong>apprentissage frugal</strong>. Toutefois, en pratique les méthodes de FSL prennent un modèle traditionnel, pré-entraîné sur un grand nombre de données, et elles le spécialisent sur le cas d’usage via une courte phase d’apprentissage sur le petit jeu de données à disposition ; c’est du fine-tuning. Mais en plus, le Few Shot c’est une méthode qui va au-delà des méthodes traditionnelles, elle permet de faire du semi-supervisé, c’est ce qu’on va voir avec le cas d’usage.</p><h2>Le cas d’usage - titre alternatif : C’est quoi le problème ?!</h2><p>Ekimetrics s’est intéressé à l’apprentissage frugal pour exploiter les énormes jeux de données des petits commentaires quotidiens sur internet, avec une problématique de gain de temps… De la frugalité avec des énormes jeux de données ? On vous explique !</p><p>Mieux que le seul nombre d’étoiles d’un restaurant ou d’un hôtel, il s’agit de prendre en compte les avis dans les tweets, les posts, les brèves… qui sont par essence des données non labellisées et de les exploiter. L’annotation humaine de ces avis est inenvisageable. Ça coûterait trop cher, ça prendrait trop de temps, et il faudrait recommencer tous les jours pour suivre l’évolution du sentiment. En l&#x27;occurrence, pour la recherche d’Ekimetrics, le sujet d’étude porte sur des commentaires de restaurants.</p><p>Mais si la machine était capable d’évaluer les commentaires, à 2 Gigahertz, tout de suite le problème serait réglé. C’est là que le Few Shot, en utilisant la méthode PET, peut devenir utile.</p><p>Dans la suite, nous vous présentons la méthode PET, comment l’utiliser dans le cadre du FSL et enfin, comment Ekimetrics l’utilise sur les avis des consommateurs.</p><h2>PET qu’est ce que c’est ?</h2><p><strong>PET</strong> est l’acronyme de ‘<strong>Pattern Exploiting Training</strong>’. La méthode repose sur un ensemble fixe et prédéfini de <strong>patterns</strong> et de <strong>verbalizers</strong> et un <strong>Pre-trained Language Model</strong> a.k.a. <strong>PLM</strong>. Les patterns sont les phrases à trou (“It was…”, “Just…!”, “All in all, it was…”, “In summary, the restaurant is…”) et les verbalizers sont les mots qui peuvent compléter ces phrases et auxquels sont associées des notes chiffrées. On commence à retrouver les nombres que l’ordinateur aime tant !</p><p>Concrètement, reprenons notre exemple des évaluations des restaurants, la méthode consiste à :</p><ul><li>prendre un commentaire,</li><li>y associer aléatoirement un pattern,</li><li>soumettre le tout au PLM qui va le compléter en choisissant un verbalizer.</li></ul><p><img src="./img/2022-02-04-MindshakeTime/PET.png" alt="image" title="Schema of a basic PET"/>{:.image-left}</p><p>Par exemple (voir Fig. 1), avec le commentaire “Best pizza ever!”, on construit la phrase à trou : “Best pizza ever! It was … .” que le PLM va compléter avec ‘great’ avec une confiance de 0.8, sachant que ce mot est noté +1.</p><h2>FSL + PET : première application aux avis internet</h2><p>Revenons à la masse brute des avis des consommateurs sur internet. <strong>PET est la méthode</strong> pour associer une note à un commentaire, le <strong>FSL est le moyen</strong> de traiter automatiquement tout le jeu de données, et le travail de l’algorithme se fait en deux étapes.</p><p>Dans un premier temps, on labellise un petit nombre de commentaires, une centaine par exemple, ce qui signifie qu’on associe une paire pattern plus verbalizer à ces commentaires, et on finetune le PLM avec cette centaine. Puis, une fois le PLM spécialisé, on le laisse labelliser tout le reste du jeu de données, automatiquement. Ça en fait une méthode semi-supervisée d’analyse de sentiment des commentaires.</p><p>Cependant, cette application basique présente des limites. D’une part, le verbalizer donné par le PLM peut ne pas être le plus adapté au commentaire et, d’autre part, c’est très ambitieux de spécialiser le PLM une fois sur une centaine d’exemples pour ensuite en traiter des dizaines de milliers ou plus. C’est pourquoi les chercheurs ont développé une méthode de distillation qui augmente la robustesse de PET, c’est la méthode <strong>iPET : iterative PET</strong>.</p><h2>i(terative)PET : une méthode de distillation astucieuse</h2><p>Une image peut valoir mille mots…</p><p><img src="./img/2022-02-04-MindshakeTime/iPET.png"/></p><p>… Mais quelques mots seront quand même nécessaires pour expliquer cette image !</p><p>Tout d’abord, le schéma de gauche sur la figure présente l’adaptation de PET qui permet d’obtenir le label le plus adapté au commentaire… en moyenne. En effet, il s’agit ‘simplement’ de <strong>faire travailler des méthodes PET indépendantes en parallèle</strong> (trois sur le schéma). Les trois cellules ont le même PLM au départ, et elles travaillent sur les mêmes commentaires, mais avec des patterns différents. Dans la phase d’entraînement sur les données labellisées, les PLMs se spécialisent différemment. Puis, durant la phase de travail, pour un même commentaire ils produisent des <strong>paires pattern-verbalizers</strong> (appelées <strong>PVPs</strong> sur le schéma) indépendamment les uns des autres ; possiblement les mêmes, mais pas avec les mêmes probas. Enfin, <strong>en sortie</strong> ces (trois) labels sont utilisés pour calculer <strong>un soft-label</strong>, i.e. un <strong>label moyen</strong>.</p><p>Ensuite, sur la droite est présenté le caractère itératif de la méthode iPET. Elle consiste à diviser le jeu labellisés sur plusieurs itérations (indiquées par les exposants allant de 0 à k) et à diviser encore à chaque itération entre plusieurs méthodes parallèles (indiquées par les indices allant de 0 à 4). Mais attention, chacun des quatre modèles ici fait du soft-labelling comme présenté à gauche de la figure, c’est-à-dire qu’ils contiennent plusieurs méthodes en parallèle.</p><p>Donc, si l’on suppose que l’on part pour trois itérations, l’information labellisée est distillée de la manière suivante. À l’itération 0 sur le schéma, on prend un tiers des données labellisées, et on fournit un quart de ces données à chaque modèle pour le finetuner, avant de prendre un tiers des données à labelliser et d’en fournir un quart à chaque modèle pour soft-labellisation. Ce qui constitue la fin de la première itération.</p><p>À la deuxième itération - itération 1 sur le schéma, on commence à nouveau par un phase de fine-tuning, mais avec un jeu de données labellisées constitué pour partie des données annotées par un être humain (le deuxième tiers), et pour partie de données soft-labellisées. Toutefois, on fait attention à ce qu’un modèle ne s’entraîne pas avec des données qu’il a lui-même soft-labellisé, pour éviter qu’il renforce ses biais… on distille ! Par exemple sur le schéma, à l’itération 1, le jeu d’entraînement T fourni au modèle 4, i.e. T14, est constitué de données soft-labellisées par les modèles 1 et 2, en plus des données annotées par l’humain. Puis on prend le deuxième tiers de données à annoter, on en fournit un quart à chaque modèle pour soft-labellisation et on finit la deuxième itération.</p><p>Pour la troisième itération, vous avez compris le principe je pense…   </p><p>À la fin, les millions de commentaires sont plutôt bien soft-labellisés, à la vitesse de la machine et au coût de l’électricité, tout est prêt pour un classifieur sur le schéma d’Ekimetrics et je vous ai expliqué tous les termes entourés sur la figure et présenté toutes les étapes. </p><h2>Avantages, inconvénients, limites et améliorations.</h2><p>Nous avons déjà vu certains des avantages. Internet est une place sur laquelle il y a pléthore d’avis en tout genre : films, restaurants, hôtels, produits de grande consommation, lieux divers… Annoter ces données serait un travail coûteux et sans fin, nous l’avons dit. L’approche iPET permet d’automatiser cette étape, à la vitesse de l’ordinateur et quel que soit le cas d’étude.</p><p>Du point de vue des performances, Ekimetrics a indiqué avoir une précision de 88% avec seulement 50 données labellisées au départ, et même 84% avec 10 données labellisées !! En comparaison, les modèles supervisés peuvent atteindre des précisions de 99%, mais au prix d’un énorme travail de pré-traitement. C’est donc un pas conceptuel de plus dans la réduction de la supervision.</p><p>Toutefois, le domaine d’application se restreint à des données textuelles assez courtes d’une part. Et d’autre part, la charge de travail est déplacée vers une bonne conceptualisation du cas d’étude. Les résultats sont extrêmement dépendants de la formulation des patterns et des choix de verbalizers (i.e. choix du prompting). Ceux-ci impliquent une grande variabilité qui n’est pas maîtrisée. De plus le PLM utilisé - un modèle BERT dans le cas d’Ekimetrics, cache des inconnues sur le corpus qui a servi à son entraînement, son domaine d’applicabilité, ses paramètres. On touche là à une limite dans laquelle l’IA n’est plus tout à fait de l’open science.</p><hr/><h1>Notes de Xavier que je n&#x27;ai pas mises</h1><h2>LIMITES et PISTES D&#x27;AMÉLIORATIONS</h2><p>PLM ou Foundation modèle avec quelles données a-t-il été entraîné ???</p><p>Que donnerait l’utilisation de plusieurs PLM ?</p><p>une amélioration de ces approches est proposé dans le papier <a href="https://arxiv.org/pdf/2103.11955.pdf">https://arxiv.org/pdf/2103.11955.pdf</a>.</p>]]></content>
        <author>
            <name>Kapichu</name>
            <uri>mailto:julien.guyot@datacraft.paris</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[My first GitHub blog! (strongly inspired by Ekimetrics'...)]]></title>
        <id>welcome</id>
        <link href="https://drkapichu.github.io/blog/welcome"/>
        <updated>2021-10-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Welcome to the first post! Short text, but sexy... People are eager to know more.]]></summary>
        <content type="html"><![CDATA[<h1>And here is the content of my first blog...</h1><h2>Welcome to our technology website!</h2><p>We have been working in the Data Science industry for 15 years and are now the biggest pure player in Europe with 250+ data profiles. We have been benefiting from the open source community for a while, and we want to give back to the community by sharing insights on what we&#x27;ve learned over the years:</p><ul><li><a href="/blog">Blog</a> - read articles on various Data topics: from industrialization on cloud platforms to exotic Deep Learning algorithms</li><li><a href="/opensource">Open source contributions</a> - browse our own open source contributions (Python libraries, code snippets)</li></ul><p>💌 After reading behind the scenes of the Data Science Company, feel free to <a href="mailto:inno@ekimetrics.com">send us a email</a> for any questions or feedbacks! </p><h2>About Ekimetrics</h2><p>Ekimetrics is the first pure player in Data Science in Europe. We operate from Paris, London, New York and Hong Kong with 250+ Data Scientists, Data Engineers, Full Stack Developers, strategy consultants and UX designers. </p><p>We help companies steer their data opportunity, build data capabilities, and deploy actionable solutions, to power up marketing and operational performance, as well as (re)energizing business models. Our primary focus is to deliver immediate business gains, while guaranteeing sustainable data capital for our clients.</p>]]></content>
        <author>
            <name>Kapichu</name>
            <uri>mailto:julien.guyot@datacraft.paris</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L’artisanat de la science des données avec datacraft]]></title>
        <id>datacraft-binaire</id>
        <link href="https://drkapichu.github.io/blog/datacraft-binaire"/>
        <updated>2021-03-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article publié dans binaire le 23/03/2021]]></summary>
        <content type="html"><![CDATA[<p>Cet article a été initialement publié le 23/03/2021 dans <a href="https://www.lemonde.fr/blog/binaire/2021/03/23/lartisanat-de-la-science-des-donnees-avec-datacraft/">binaire</a>, blog créé en janvier 2014 dans le journal <a href="https://www.lemonde.fr/blog/binaire/a-propos/">Le Monde</a> à l’initiative de Serge Abiteboul et de plusieurs collègues de la Société Informatique de France, afin de communiquer sur ce qu’est vraiment l’informatique en tant que science et technique.</p><p>.</p><p>.</p><p>.</p><p><strong>Job : Rendre visible les légendes des images.</strong></p><p>.</p><p>.</p><p>.</p><hr/><h1>L’artisanat de la science des données avec datacraft</h1><p><strong>Image here</strong>
<img src="./img/2021-03-23-binaire/Hiba.png" alt="image" title="**Courteousy of Hiba Kalache, therefore the most profound though is a beating heart (bannière du site de datacraft)**"/>
<strong>il manque une légende pour cette image</strong></p><p><strong>Datacraft</strong>, c’est quoi ce « machin » ? On est à Sorbonne Université <strong>[1]</strong>, dans le Sorbonne Center for Artificial Intelligence, sur le campus de Jussieu, un haut lieu des sciences. Pourtant, ce n’est pas un labo universitaire, même si cela y ressemble. Ça tient du club, un peu du fablab. C’est un espace de cotravail apprenant où on travaille vraiment en commun, plus que dans un espace de cotravail classique. Officiellement, c’est une startup. En fait, ce n’est pas facile à classifier, ce qui est pour moi assez positif dans le monde de la science des données qui se réinvente en permanence.</p><p>J’ai été tenté de dire que c’était un « temple des données » tant les données sont au centre des préoccupations de tous et toutes dans ce lieu. Mais non, les données ne sont pas adorées ici, elles sont questionnées, challengées. On vous parle ici de leur mise au service des entreprises et de la société, de « responsabilité sociale des données ».</p><p>En fait, la vraie valeur, il faut la chercher dans le nom de l’entreprise, datacraft, en français « l’artisanat de la science des données » (traduction personnelle). C’est tellement plus joli qu’en anglais, même si c’est certainement moins vendeur. Avec datacraft, nous sommes bien dans l’artisanat, dans un savoir-faire spécifique, hors contexte industriel de masse. Nous sommes pile poil dans le compagnonnage en sciences des données, dans l’idée de se former en faisant, en échangeant, en bénéficiant de conseils d’experts.</p><p>Je pense qu’un tel compagnonnage est particulièrement bien adapté à la science des données. En 2014, dans un rapport pour le gouvernement <strong>[2]</strong>, nous parlions de la nécessité de booster les formations aux sciences des données, en insistant sur le caractère indispensable de projets « les yeux dans les yeux, de données en vraie grandeur ». Depuis, de telles formations ont vu le jour et les entreprises ont souvent maintenant leurs data scientists. Mais ceux-ci souffrent d’être isolés, de ne pas pouvoir partager leurs questionnements, leurs expériences. L’image du geek qui bosse seul dans son coin est à des kilomètres de la réalité de l’informatique – on travaille le plus souvent en équipe – et tout particulièrement dans la science des données. Un beau projet en science des données met typiquement en jeu des compétences variées que l’on trouve rarement chez une personne unique : gestion de données, big data, machine learning, compétence métier, etc.</p><p><strong>Image here</strong>
<img src="./img/2021-03-23-binaire/Atelier.png" alt="image" title="**©datacraft, atelier computer vision au service de l’imagerie médicale**"/>
<strong>il manque une légende pour cette image</strong></p><p>Les data scientists des entreprises adhérentes à datacraft peuvent venir travailler dans un espace de cotravail où ils rencontreront des data scientists, leurs homologues d’autres entreprises et des experts résidence. Il ne s’agit pas juste de partager de beaux bureaux et du café.  Ils peuvent par exemple dans des ateliers pratiques échanger des idées, apprendre, et partager. Et ce contexte permet aux idées d’infuser entre des domaines différents.</p><p>Par exemple, datacraft a organisé un atelier avec l’INSEP (l‘Institut national du sport, de l’expertise et de la performance) autour de l’utilisation de données dans le sport de haut niveau. Il s’agissait d’arriver à construire la meilleure équipe selon le contexte, les adversaires, la météo, etc. Il était difficile de prévoir l’intérêt des ingés de Vinci Autoroutes sur ce sujet, pourtant, ils ont apporté une expertise précieuse.</p><p><strong>Image here</strong>
<img src="./img/2021-03-23-binaire/INSEP.png" alt="image" title="**Image: https://pixabay.com/users/clker-free-vector-images-3736/**"/>
<strong>il manque une légende pour cette image</strong></p><p>Pas de bol, datacraft s’est lancée en février 2020, pas le meilleur moment pour un concept basé sur un lieu de rencontre physique. Les membres ont initié des projets autour de la santé et de l’éducation, pour aider la société dans un temps de crise sanitaire grave. Je me serais aussi attendu à ce qu’ils découvrent les avantages considérables du travail à distance, d’une certaine inutilité de la rencontre physique. Pas du tout, Isabelle Hilali, fondatrice et pédégère de datacraft, explique : « Pour moi, la dimension physique est essentielle, et j’aimerais revenir dès que possible au présentiel car il est important de garder du lien. » Et quand j’insiste sur les avantages du distanciel, elle précise : « Il faut aussi le plaisir du travail. Il y a moins de plaisir à collaborer à distance. »</p><p>Quand on met des gens brillants ensemble, les initiatives fleurissent. Des membres se regroupent pour former des consortiums et répondre à des appels à projets ambitieux auxquels ils n’auraient pas les moyens de répondre individuellement. Ils mettent en place des formations, des espaces d’échanges dans des domaines spécifiques comme les ressources humaines ou les aspects légaux des applications de la science des données.</p><p>J’ai parlé de datacraft à des collègues chiliens. Leur réaction : un tel club serait encore plus indispensable au Chili où les data scientists des entreprises sont encore plus isolés qu’en France. Je pense que c’est vrai pour de nombreux pays, datacraft devrait donc s’exporter ? J’ai posé la question : ils ouvrent une base au Maroc en 2022. À quand le Chili ?</p><p>Postscriptum : Quand je m’enthousiasme pour une startup dans binaire, il se trouve parfois un de nos très chers lecteurs pour questionner mon objectivité, m’accuser d’avoir des amis dans la startup, d’y avoir investi, voire de me faire payer pour la pub. Et bien non rien de tout cela. J’ai trouvé que c’était une idée géniale et j’ai voulu la raconter.</p><p><a href="https://fr.wikipedia.org/wiki/Serge_Abiteboul">Serge Abiteboul</a>, Inria et ENS, Paris</p><hr/><p><strong>[1]</strong> Sorbonne Université est une université française située à Paris. Elle a été créée le 1er janvier 2018 par regroupement des universités Paris-Sorbonne (Paris-IV) et Pierre-et-Marie-Curie (Paris-VI), elles-mêmes créées en 1970 et héritières de l’université de Paris fondée en 1896.</p><p><strong>[2]</strong> Serge Abiteboul, François Bancilhon, François Bourdoncle, Stephan Clemencon, Colin De La Higuera, et al. L’émergence d’une nouvelle filière de formation : data scientists », 2014 <a href="https://hal.inria.fr/hal-01092062">https://hal.inria.fr/hal-01092062</a></p>]]></content>
        <author>
            <name>Serge Abiteboul</name>
            <uri>https://www.inria.fr/fr/serge-abiteboul-1</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification des mains de scribes assistée par l’intelligence artificielle]]></title>
        <id>egyptologie</id>
        <link href="https://drkapichu.github.io/blog/egyptologie"/>
        <updated>2021-03-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article publié dans Archeologia Magazine le 04/03/2021]]></summary>
        <content type="html"><![CDATA[<p>Cet article a été initialement publié le 04/03/2021 dans <a href="https://www.archeologia-magazine.com/numero-596/egypte-dernieres-decouvertes/egypte-dernieres-decouvertes.53184.php#article_53184">Archeologia Magazine</a>, magazine payant d&#x27;archéologie.</p><p>.</p><p><strong>Mettre les images avec les légendes.</strong></p><p>.</p><hr/><h1>CLASSIFICATION DES MAINS DE SCRIBES ASSISTÉE PAR L’INTELLIGENCE ARTIFICIELLE</h1><p><strong>Depuis 2019, L’Ifao et Sorbonne Université mènent conjointement un programme de recherches (ÉCRITURES – Pour une archéologie et une anthropologie des écritures de l’Égypte ancienne) afin de mieux comprendre les usages des différentes graphies égyptiennes et les acteurs impliqués. Si les textes de la vie courante (administration, lettres, littérature, sciences, textes magiques et rituels) étaient inscrits en hiératique, l’écriture principale des scribes égyptiens, une cursive dérivée des hiéroglyphes, ces derniers demeuraient limités à des usages monumentaux et sacrés.</strong></p><h1>Identifier les mains pour connaître les lettrés</h1><p>Les scribes, les auteurs et plus généralement les praticiens de l’écriture en Égypte ancienne, restent mal connus d’autant que leurs manuscrits sur papyrus ou sur ostraca (tessons de poterie ou morceaux de calcaire taillés inscrits) furent le plus souvent anonymes. Une des tâches des égyptologues consiste donc à examiner les styles individuels d’écriture pour rapprocher entre eux des documents issus d’une même main. Les outils de la paléographie aident à établir des comparaisons entre la forme de certains signes afin de regrouper des textes possiblement tracés par une même personne. Mais les caractéristiques à prendre en compte sont nombreuses (forme générale du signe, nombre de traits, taille, dynamisme de l’écriture, mise en page, régularité...) et constituent autant d’aspects difficiles à combiner et à comparer, pour l’œil et l’esprit humains, lorsque le nombre de documents se multiplie. </p><h1>L&#x27;apport du Deep Learning</h1><p>C’est là que les outils d’intelligence artificielle, habilement mis en œuvre, peuvent s’avérer décisifs. Ce programme de recherche s’est donc associé au Sorbonne Centre of Artificial Intelligence et à datacraft afin explorer les solutions que le deep learning (ou réseau de neurones) peut apporter. Une première</p><p>expérience a ainsi été montée à partir de documents de scribes de l’époque ramesside (XIII e -XI e siècles avant notre ère). Des jeux de données provenant du British Museum, du Museo Egizio de Turin et de l’Institut français d’archéologie orientale, constitués de photos numériques d’ostraca et de papyrus publiés, ont été collectés. Réalisée avec l’équipe Data science de Vinci Autoroutes qui travaille régulièrement avec datacraft, une étape de préparation des données a été nécessaire avant de présenter ces images numériques au réseau de neurones. Grâce au logiciel de Vinci, les égyptologues ont annoté les documents dont les scribes-rédacteurs étaient connus avec certitude. Il fut ensuite possible de classer automatiquement les images non-annotées grâce au réseau de neurones, ce dernier identifiant si telle ou telle image appartient à une main déjà connue. Une autre voie explorée se fonde plus directement sur les signes égyptiens utilisés dans les documents non classés. Elle consiste à utiliser le réseau de neurones pour regrouper les textes dont les signes d’une écriture similaire, ce qui permettra à terme d’identifier de nouveaux scribes potentiels…</p><p><strong>Chloé Ragazzoli</strong>, Sorbonne Université, <strong>Florence Albert</strong>, Ifao, <strong>Xavier Lioneton</strong>, datacraft, <strong>Amir Nakib</strong>, Vinci, dans le cadre d’une collaboration au sein du club datacraft</p>]]></content>
        <author>
            <name>Chloé Ragazzoli - Florence Albert - Amir Nakib - Xavier Lioneton</name>
            <uri>https://datacraft.paris/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[datacraft - un club dédié à la data science et l’Intelligence Artificielle]]></title>
        <id>datacraft</id>
        <link href="https://drkapichu.github.io/blog/datacraft"/>
        <updated>2021-02-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article publié dans Visionary le 15/02/2021]]></summary>
        <content type="html"><![CDATA[<p>Cet article a été initialement publié le 15/02/2021 dans <a href="https://visionarymarketing.com/fr/2021/02/datacraft-data-science-et-ia/?mc_cid=a430616615&amp;mc_eid=0bd4a33d6d">Visionary</a>, site d&#x27;infos des marketeurs et innovateurs visionnaires depuis 1996.</p><p>.</p><p>.</p><p>.</p><p><strong>Job : Voir si possible d&#x27;inclure l&#x27;audio de l&#x27;interview d&#x27;Isabelle.</strong></p><p><strong>Job : Travailler la présentation pour montrer que c&#x27;est une vidéo.</strong></p><p><strong>Job : Rendre visible les légendes des images.</strong></p><p><strong>Job : Changer le style des citations.</strong></p><p>.</p><p>.</p><p>.</p><hr/><h1>Datacraft : un club dédié à la data science et l’Intelligence Artificielle</h1><p>Un club Data Science et IA ? Là où beaucoup veulent absolument qu’on remplace les humains par des robots, les experts de l’IA démontrent la supériorité des échanges humains. Car il y avait un besoin d’échange dans la communauté de la data science et <a href="https://www.linkedin.com/in/isabelle-hilali-82b5111/">Isabelle Hilali</a>, CEO et fondatrice de Datacraft, que <a href="https://visionarymarketing.com/fr/2016/09/big-data-sante-combinaison-necessaire/">nous avions déjà interviewée</a> ici il y a quelques années, l’avait pressenti. Elle n’a pas hésité à lancer son club data science en plein milieu de la crise du Covid et a démontré, même en ces temps difficiles que tout est possible. Elle a démontré également que la nécessité de se parler, y compris pour les experts de l’IA et de la data science, est plus forte que jamais. Retour sur la création d’un club hors du commun, où se dessine collaborativement le futur de vos logiciels. </p><h2>Datacraft : un club data science et IA installé au cœur de La Sorbonne</h2><p><img src="./img/2021-02-15-datacraft/datacraft-scai.png" alt="image" title="J’ai rencontré Isabelle Hilali dans ses locaux du SCAI (Sorbonne Center for Artificial Intelligence) qui héberge Datacraft, le club de la data science et de l’IA qu’elle a créé"/>
<strong>il manque une légende pour cette image</strong></p><blockquote><p>&quot;J’ai observé que l’univers de la data science et des data scientists est un domaine sur lequel il faut apprendre tout le temps et où tout va extrêmement vite.&quot;
Isabelle Hilali – Datacraft</p></blockquote><p>« En data science, il est vraiment compliqué d’être à la pointe en termes de compétences. C’est un univers où l’on a besoin de partager. On a toujours l’image du geek qui est seul derrière son micro, mais en fait, si on veut être bon, il faut croiser les données et être imaginatif » explique Isabelle.</p><p>C’est un univers sur lequel il y a beaucoup de liberté puisque ce sont des métiers très demandés alors que trop peu de bonnes compétences sont disponibles. Les data scientists peuvent donc quelque peu « imposer » la façon dont ils ont envie de travailler. Et ils ont envie de travailler de manière flexible.</p><blockquote><p>&quot;L’idée, c’était d’avoir un lieu qui donne envie de collaborer&quot;</p></blockquote><p>Le Centre d’intelligence artificielle de la Sorbonne a été imaginé pour permettre de se retrouver pour collaborer. C’est un lieu ouvert. Et c’est là que Datacraft s’est implanté, entre la tour de Jussieu et le jardin des plantes.</p><iframe width="85%" height="500px" src="https://www.youtube.com/embed/0xtPrTo-13o" alt="Présentation de datacraft par Isabelle" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><h2>Un portrait robot du data scientist ? Impossible tant ce mot recouvre des réalités différentes</h2><p>J’aimerais bien faire un portrait robot de la <a href="https://visionarymarketing.com/fr/2015/07/data-scientist/">data scientist</a>, mais il n’y en a pas beaucoup pour l’instant … Alors j’ai commencé par le data scientist au sens large.</p><blockquote><p>&quot;Il n’y a pas de portrait robot du data scientist. Ce mot est utilisé un peu à tort et à travers. Il recouvre différentes réalités&quot;</p></blockquote><p>Il y a également le data ingénieur, le data scientist, le data analyst. Il y a besoin de préparer les données, de développer des modèles, d’intégrer aussi tout cela dans le système d’information.</p><p>Certains ont une formation assez classique, une grande école d’ingénieurs ou une grande école de math et d’informatique. Mais d’autres ont fait de la musique et ensuite se sont formés à la datascience.</p><p><img src="./img/2021-02-15-datacraft/datacraft-scai.png" alt="image" title="Même masqués, les ateliers datacraft de data science sont un moment privilégié d’échange et de travail"/>
<strong>il manque une légende pour cette image</strong></p><p>Le bon data scientist, qui va être performant dans une organisation, doit être curieux. Il doit être ouvert à l’organisation dans laquelle il est, pour comprendre la situation et éviter de construire des modèles abstraits qui ne serviront à rien. Il doit comprendre les problématiques en allant chercher des données, en faisant de l’apprentissage machine, en développant des modèles, en s’interrogeant sur la meilleure façon d’aider à la connaissance, à la prise de décision.</p><blockquote><p>&quot;Pour moi, le bon data scientist est quelqu’un d’ouvert et de collaboratif, et qui a des compétences en mathématiques et en informatique, ou au moins la capacité à travailler sur ces sujets&quot;</p></blockquote><h2>La nécessité d’apprendre dans le domaine de la data science</h2><p>Le besoin principal sur lequel j’avais envie de travailler, était la nécessité d’apprendre en permanence.</p><p>Il existe bien des formations en ligne, des formations physiques, des livres, des communautés pour faire de la recherche. Mais il n’existait pas de lieu physique où partager les choses.</p><p><strong>J’ai été assez inspirée par le modèle du compagnonnage</strong>, c’est une chouette façon d’apprendre, avec les autres, en le revisitant un peu, car dans notre modèle, chacun apprend aux autres, il n’y a pas de maître ni d’élève. Chacun est maître sur un bout de sujet.</p><p>Je me suis dit qu’il faudrait qu’il y ait des lieux qui permettent aux gens un échange de bonnes pratiques et qui soient pensés pour ça. Et en même temps qui développent la collaboration et la réflexion sur ce qu’on fait, quelles limites on veut donner quand on développe de l’intelligence artificielle.</p><blockquote><p>&quot;Notre volonté avec Datacraft était de créer ce réseau de lieux pour se retrouver entre experts pour un échange de bonnes pratiques&quot;</p></blockquote><h2>Le lancement de Datacraft, le club data science et IA</h2><p>J’ai lancé DataCraft en janvier 2020, avec le groupe Accor et l’Insep, les deux premiers membres qui m’ont fait confiance, et dans de supers locaux dans le Marais.</p><p>Tout cela a pris forme début février avec l’ouverture des locaux, avec également notre système de résidence où l’on accueille à la fois des chercheurs, et des freelances qui font partie de la communauté, qui ne paient pas d’adhésion, mais qui donnent du temps à la communauté.</p><p>Et puis tout a fermé un mois plus tard avec le confinement …</p><p>Le concept étant de mettre les gens ensemble et d’avoir cette complémentarité avec ce qui existe en digital, j’ai pensé qu’il allait falloir fermer.</p><p>Et puis finalement, on avait encore davantage besoin de cet échange de bonnes pratiques, de cette solidarité entre experts et de cette créativité.</p><p>La communauté a énormément grossi. Nous sommes passés de 80 a plus de 500 en fin de confinement, en fédérant des gens qui avaient envie de s’entraider.</p><h2>Un club data science : comment ça marche ?</h2><p>Par exemple, un membre va communiquer sur le développement d’une application en Python, alors qu’il a l’habitude de faire sur <a href="https://help.adobe.com/en_US/air/build/index.html">Adobe Air</a>, et poser la question si d’autres membres ont déjà fait cela pour un type d’applications, pour un tableau de bord par exemple.</p><p>La demande est lancée dans la communauté, des membres vont répondre et au lieu que ce soit deux personnes qui se parlent, on en profite pour organiser un atelier, virtuellement pendant le Covid, où les gens vont partager leurs bonnes pratiques. Souvent, à la fin de l’atelier, un atelier suivant se dessine de par les échanges qui ont eu lieu sur par exemple une bibliothèque que les membres n’avaient jamais pensé à utiliser comme ça.</p><p>Puis, il y a eu le premier déconfinement et nous avons recommencé à faire des ateliers en physique.</p><p>Nous avons organisé un atelier, par exemple, sur les données du sport de haut niveau, avec des problématiques telles qu’aider un entraîneur à optimiser son équipe, à choisir l’équipe qui sera la plus performante en fonction des adversaires, en fonction de données, de météo ou de tout cet environnement.</p><p>C’est un prétexte. Sur un projet comme ça, des membres vont venir pendant deux jours travailler ensemble, des gens de l’Insep, de Vinci Autoroutes, d’un labo pharmaceutique, d’une petite startup qui va travailler sur des données des réseaux sociaux, par exemple. Et tous ces gens vont travailler ensemble pendant deux jours.</p><p>Contrairement à un hackathon, notre objectif n’est pas de faire un prototype au bout de deux jours qui souvent, en outre, n’est pas très bon.</p><blockquote><p>&quot;Notre seule volonté est qu’à la fin des deux jours ou de la demi-journée, les participants repartent en se disant  » C’est génial, j’ai appris telle chose »&quot;</p></blockquote><p>C’est ça notre objectif, un échange de bonnes pratiques, où même les « super experts » apprennent quelque chose.</p><p>Cela va bien au delà bien sûr. De nombreux partenariats se nouent, qu’on n’aurait jamais imaginé. Un partenariat entre Vinci Autoroutes et le sport de haut niveau par exemple.</p><p>Ou encore une startup qui travaille sur les données des réseaux sociaux en santé, qui va découvrir une expertise complémentaire chez un membre de Datacraft, et envisager de monter un gros projet européen sur les fakenews médicales.</p><p>Ou encore Danone, par exemple, qui est en train de rédiger sa <a href="https://visionarymarketing.com/fr/glossaire/marketing-ethique/">charte</a> sur l’utilisation responsable des données, qui la réalise avec d’autres membres qui l’ont déjà fait pour l’écrire ensemble.</p><h2>L’avenir pour Datacraft, le club de la data science</h2><p>L’idée était d’avoir un lieu qui donne envie de collaborer et de se poser des questions sur la façon dont on travaille la data, quelle responsabilité on a envers la société.</p><p>L’idée était aussi d’avoir dans Paris un lieu avec un jardin, des plantes, qui favorise cette collaboration et cette réflexion.</p><p>Nous sommes au Centre d’intelligence artificielle de la Sorbonne, et cela fait énormément de sens. Le Centre d’intelligence artificielle de la Sorbonne a été imaginé pour permettre à tout l’écosystème de Sorbonne universités, le Museum d’histoire naturelle, l’IRCAM en musique, la fac de médecine, de se retrouver pour collaborer.</p><p>C’est un lieu ouvert où les entreprises sont les bienvenues. Etre hébergés ici, à côté du Jardin des Plantes, est complètement en phase avec nos valeurs.</p><p><strong>Dans le futur, il y aura d’autres bases datacraft</strong> qui seront toutes imaginées autour de ce concept de collaboration et de responsabilité.</p><h2>Le Covid : crise ou opportunité ?</h2><p>Cela a été aussi une source de créativité pour nous, Cela nous a permis de faire des choses à distance, et d’avoir, par exemple, des personnes en Ouganda qui nous ont demandé de participer à un atelier. Nous n’aurions pas pu faire ce genre de choses aussi rapidement.</p><p>Et puis, ça nous a montré ce besoin de solidarité et d’échange de bonnes pratiques.</p><blockquote><p>&quot;Même si on espère bien sûr que ça durera pas trop longtemps, cette période a finalement encore renforcé nos valeurs&quot;</p></blockquote>]]></content>
        <author>
            <name>Yann Gourvennec</name>
            <uri>https://visionarymarketing.com/fr/author/yann-gourvennec/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Téléconsultation - une porte d’entrée dans le parcours de soins ?]]></title>
        <id>Teleconsultation</id>
        <link href="https://drkapichu.github.io/blog/Teleconsultation"/>
        <updated>2020-11-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article publié dans Pharmaceutiques le 27/11/2020]]></summary>
        <content type="html"><![CDATA[<p>Cet article a été initialement publié le 27/11/2020 dans <a href="https://pharmaceutiques.com/actualites/nouvelles-technologies/teleconsultation-une-porte-dentree-dans-le-parcours-de-soins/">Pharmaceutiques</a> journal spécialisé dans les secteurs de la santé et du médicament.</p><hr/><h1>Téléconsultation : une porte d’entrée dans le parcours de soins ?</h1><h2>Une étude menée par la plateforme de télémédecine Qare, en partenariat avec datacraft et Ekimetrics, analyse avec précision 300 000 téléconsultations réalisées en janvier 2019 et juillet 2020.</h2><p>Quel est le profil des utilisateurs de la téléconsultation ? Et qui a profité en priorité de l’assouplissement du cadre règlementaire décidé en mars dernier en phase d’entrée dans la pandémie de Covid19 ? Au moment où les syndicats de professionnels de santé discutent, dans le cadre conventionnel, des nouvelles modalités de la télémédecine, il apparait essentiel de bien connaitre la structure de ” consommation” de cette nouvelle forme de recours aux soins. C’est dans ce but que la plateforme Qare, l’un des principaux leaders de la téléconsultation, a mené une enquête détaillée, auprès de 2050 praticiens, 180 150 patients, et sur la base de 300 000 téléconsultations. « Notre enquête porte sur une période large, de janvier 2019 à juillet 2020, précise le Dr Julie Salomon, directrice médicale adjointe de Qare. Il faut également tenir compte du fait qu’il ne nous a pas été possible de distinguer les profils de consommation avant et après l’arrivée de la pandémie. » En raison de l’explosion du nombre de téléconsultations à partir de mars, par rapport à la période précédente, la comparaison statistique n’aurait pas été pertinente. Par ailleurs, les données proviennent des téléconsultations opérées par Qare : or, l’entreprise a fait son entrée sur le marché dès janvier 2017, par le biais d’une convention avec l’ARS des Français de l’étranger. « Nous avons initié l’outil avec les Français installés hors de France, avant de pouvoir développer ce qui est notre coeur de métier, l’offre de soin pour les français du territoire national, ajoute Julie Salomon. Cet engagement, antérieur au cadre règlementaire posé en janvier 2019, explique la diversité de notre panel d’utilisateurs.» </p><h2>Des utilisateurs jeunes, un besoin aigu de santé</h2><p>Ces réserves étant posées, l’étude livre des résultats inédits et particulièrement intéressants. Pour la réaliser, Qare s’est appuyé sur l’expertise de deux partenaires : datacraft, une startup avec un modèle de Club qui permet un échange de bonnes pratiques entre experts de la donnée, et le leader européen en data science Ekimetrics. « Le premier enseignement, c’est que la téléconsultation est une solution principalement utilisée par une population jeune et plutôt urbaine, note Soline Aubry, lead du projet pour Ekimetrics. Les patients ont en moyenne 35 ans, contre 41 ans pour l’ensemble de la population française. Et 60% vivent dans l’une des dix plus grandes zones urbaines. » Le mode de consommation est également révélateur : le pic de téléconsultation se situe plutôt tôt, avant 10 heures du matin, et plutôt en début de semaine. « Cela traduit sans doute un besoin de prise en charge rapide, dès l’ouverture des cabinets, analyse Julie Salomon. Le phénomène était moins marqué durant la phase de confinement. » Quels médecins sont consultés en priorité ? D’abord des médecins généralistes, pour 63% des consultations. Les 37% de consultation chez les spécialistes s’effectuent en priorité les dermatologues, les pédiatres et les psychiatres, trois spécialités en forte tension démographique.</p><h2>40% de fidélisation au médecin téléconsulté</h2><p>Utilisée majoritairement pour des affections dites ”courantes” (cystite, rhume, gastro-entérite, état grippal…), la téléconsultation (chez Qare) est un outil, selon les auteurs de l’étude, pour offrir une réponse en urgence face à un besoin de santé aigu. Et il permet, « dans 99% des cas », d’éviter un déplacement inutile chez le médecin. « C’est une donnée importante, car elle laisse entendre que la téléconsultation pourrait permettre de désengorger les cabinets et de faciliter la prise en charge des soins non programmés », estime Julie Salomon. D’autres chiffres révélés par Ekimetrics montrent par ailleurs que si 86% des patients et des médecins ne se connaissaient pas au moment du premier rendez-vous, 40% des patients ont pris le second rendez-vous avec le même médecin. « La téléconsultation peut donc être l’occasion pour les patients d’entrer dans le parcours de soins » ajoute Isabelle Hilali, fondatrice et dirigeante de datacraft.</p><h2>L’enjeu des soins chroniques</h2><p>Première étape d’une étude au long cours, cette enquête sera prolongée, dans les mois à venir, par de nouveaux questionnements. Qui consulte quelles spécialités ? Comment se profilent les parcours de téléconsultation en fonction des typologies de territoires ? L’usage de la téléconsultation décolle-t-il dans les zones de faible densité géographique ? Et les seniors et les malades chroniques s’y engagent-ils ? Enfin, comment la pandémie modifie-t-elle les modalités de recours à la consultation à distance ? Les enjeux d’avenir de la télémédecine se profilent derrière ces questions : pour s’imposer définitivement dans les usages des patients et les pratiques des médecins, elle devra nécessairement contribuer à la fluidité des parcours de soins pour les malades chroniques.</p>]]></content>
        <author>
            <name>Hervé Réquillart</name>
            <uri>https://www.linkedin.com/in/herve-requillart-info-sant%C3%A9/</uri>
        </author>
    </entry>
</feed>