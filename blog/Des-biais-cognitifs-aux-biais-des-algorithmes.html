<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="datacraft blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="datacraft blog Atom Feed"><title data-react-helmet="true">Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ? | datacraft blog</title><meta data-react-helmet="true" property="og:title" content="Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ? | datacraft blog"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="twitter:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="description" content="IA et biais (1/3) - L&#x27;humain, une source du problème."><meta data-react-helmet="true" property="og:description" content="IA et biais (1/3) - L&#x27;humain, une source du problème."><meta data-react-helmet="true" property="og:url" content="https://drkapichu.github.io//blog/Des-biais-cognitifs-aux-biais-des-algorithmes"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="IA de confiance,biais,éthique,data"><link data-react-helmet="true" rel="icon" href="/img/datacraft_logo.png"><link data-react-helmet="true" rel="canonical" href="https://drkapichu.github.io//blog/Des-biais-cognitifs-aux-biais-des-algorithmes"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/Des-biais-cognitifs-aux-biais-des-algorithmes" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/Des-biais-cognitifs-aux-biais-des-algorithmes" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.29cd2889.css">
<link rel="preload" href="/assets/js/runtime~main.23ef1fa4.js" as="script">
<link rel="preload" href="/assets/js/main.200c38ba.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/opensource">Open Source</a></div><div class="navbar__items navbar__items--right"><a href="https://datacraft.paris/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">datacraft website</a><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Newsletter</a><a href="https://github.com/datacraft-paris" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_RC3s">Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ?</h1><div class="margin-vert--md"><p>IA et biais (1/3) - L&#x27;humain, une source du problème.</p><time datetime="2022-04-08T00:00:00.000Z" class="blogPostDate_IAgm">April<!-- --> <!-- -->8<!-- -->, <!-- -->2022<!-- --> <!-- --> · <!-- -->7<!-- --> min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="mailto:contact@datacraft.paris" target="_blank" rel="noreferrer noopener">Stéphanie Lehuger</a></h4><small class="avatar__subtitle">Thinker et entrepreneuse</small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/img/blog/pikachu.png"></div></header><section class="markdown blog-article-custom"><hr><header><h1>Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ?</h1></header><h2 class="anchor anchorWithStickyNavbar_y2LR" id="pourquoi-les-biais-posent-des-problèmes-éthiques-en-intelligence-artificielle-">Pourquoi les biais posent des problèmes éthiques en intelligence artificielle ?<a class="hash-link" href="#pourquoi-les-biais-posent-des-problèmes-éthiques-en-intelligence-artificielle-" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="nous-avons-tous-des-biais">Nous avons tous des biais<a class="hash-link" href="#nous-avons-tous-des-biais" title="Direct link to heading">​</a></h3><p>Imaginez que vous faîtes vos courses avec une personne aveugle. Vous arrivez devant un rayon de bananes et la personne vous demande ce que vous voyez. Qu’allez-vous lui décrire ? Il y a des bananes ? Il y a des bananes avec de petits autocollants dessus ? Il y a environ 30 bananes ? Il y a peu de chances pour que vous disiez qu’il y a des bananes qui sont de couleur jaune. Le jaune est une information typique pour une banane et on a tendance à ne pas mentionner les évidences. C&#x27;est une forme de biais, ou d&#x27;angle mort cognitif. Et nous avons tous des <a href="https://fr.wikipedia.org/wiki/Biais_cognitif" target="_blank" rel="noopener noreferrer">biais cognitifs</a>, c’est humain.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pourquoi-les-biais-dune-ia-sont-un-problème-">Pourquoi les biais d’une IA sont un problème ?<a class="hash-link" href="#pourquoi-les-biais-dune-ia-sont-un-problème-" title="Direct link to heading">​</a></h3><p>Nos biais humains sont un problème quand on les transmet à une intelligence artificielle. C’est un problème aussi bien éthique qu’en termes de performance du logiciel basé sur ces biais. Concrètement, en utilisant la technique de l’apprentissage machine, on va entraîner l’intelligence artificielle à reconnaître des <a href="https://www.blog.google/technology/ai/new-course-teach-people-about-fairness-machine-learning/" target="_blank" rel="noopener noreferrer">bananes</a> en lui montrant des images de bananes. Si toutes les images que je lui montre sont des bananes jaunes, le jour où l’intelligence artificielle va voir une banane verte, elle ne va pas penser que c’est une banane. Il y a des chances qu’elle l’associe plutôt à une autre chose verte qu’elle a déjà vue, comme un concombre par exemple.</p><p>Éthiquement, c’est un problème. Parce que, quand on parle de bananes, cela paraît anodin. Mais cela peut parfois conduire à des conséquences graves. En effet, même un humain bienveillant a des préjugés inconscients. Parfois, sans s’en rendre compte, des data scientists peuvent transmettre leurs biais au travers des données qu’ils sélectionnent pour construire des logiciels. Prenons l’exemple d’un système de vidéosurveillance automatisée utilisé par la police. La Chine est friande de ces systèmes prédictifs pour interpeler des suspects, comme l’a dénoncé <a href="https://www.hrw.org/news/2021/11/24/mass-surveillance-fuels-oppression-uyghurs-and-palestinians" target="_blank" rel="noopener noreferrer">Human Rights Watch</a> concernant l’usage du système de surveillance de masse IJOP (Integrated Joint Operation Platform). Imaginons de manière caricaturale que le logiciel a appris à reconnaître des personnes à partir d’un jeu de données composé à 90% de portraits d&#x27;hommes à la peau blanche. L’algorithme n’étant pas entraîné à reconnaître des femmes à la peau noire, il sera alors complètement inefficace sur cette population et pourrait mener à des erreurs judiciaires s’il était utilisé. Ainsi, le <a href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" target="_blank" rel="noopener noreferrer">logiciel de prédiction des récidives</a> COMPAS utilisé par des juges américains pour les assister dans leur verdict a surestimé systématiquement le risque de récidive des détenus afro-américains tandis que celui des blancs a été très sous-estimé.</p><p>Un algorithme fonctionne comme une recette de cuisine : avec des ingrédients, les données, et une recette, le code. Même si c’est la partie la plus visible, la réussite de la recette ne dépend pas tant du code que de la qualité des ingrédients utilisés, les données. S’ils sont de mauvaise qualité, quelle que soit la recette, le succès ne pourra pas être au rendez-vous. Les résultats des algorithmes ne dépendent donc pas que de la manière dont les programmeurs les ont écrits. Les biais ont des origines diverses. Les biais d&#x27;acquisition de données par exemple sont liés à la manière dont les données sont collectées. Par exemple, si on estime la moyenne mensuelle de clients d’un hôtel dans une station de ski en se basant uniquement sur les chiffres des mois d’hiver, on va surestimer le résultat. Le contexte est essentiel dans l’acquisition de données parce que, si la période de l&#x27;année à laquelle on collecte des données n&#x27;est pas représentative de l&#x27;année entière, le résultat sera faussé. Nos biais représentent un problème quand ils entraînent des conséquences discriminatoires mais ils sont le plus souvent positifs. En effet, dans le cas normal, nos biais sont aussi ce qui nous permet d&#x27;avancer et de prendre des décisions rapides en environnement inconnu, transmettant ainsi des informations bénéfiques aux algorithmes.</p><p>C’est donc à la fois un problème éthique et un problème de performance des algorithmes. Imaginons que la police décide du quartier où elle patrouille sur les suggestions d’une intelligence artificielle. Si le logiciel dispose de l’historique d’actes de délinquance de deux quartiers, l‘un avec un petit peu plus de délinquance que l’autre, il va orienter les policiers vers le quartier où il y en a eu un peu plus dans le passé. En arrivant dans ce quartier, si les policiers constatent une infraction, ils vont approvisionner la base d’apprentissage de nouvelles données. Alors que, s’ils avaient été dans l’autre quartier, ils auraient peut-être aussi constaté une infraction. Mais maintenant, l’intelligence artificielle est biaisée parce qu’elle considère que ce quartier connaît plus de délinquance. Avec ce biais, les algorithmes peuvent ainsi former ce qu’on appelle des “boucles de rétroaction” par lesquelles stéréotypes, <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction" target="_blank" rel="noopener noreferrer">discriminations</a> et inégalités se renforcent mutuellement, contribuant ainsi à cristalliser durablement des situations d’inégalité. En plus de ne pas être performante, l’intelligence artificielle n’est alors pas non plus éthique. C’est ainsi qu’un <a href="https://www.courthousenews.com/audit-finds-lapd-predictive-policing-programs-lack-oversight/" target="_blank" rel="noopener noreferrer">audit critique</a> du logiciel de police prédictive de la ville de Los Angeles, PredPol, a amené à abandonner son usage.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="personne-nest-à-labri-de-risques-éthiques">Personne n’est à l’abri de risques éthiques<a class="hash-link" href="#personne-nest-à-labri-de-risques-éthiques" title="Direct link to heading">​</a></h3><p>Les médias rapportent les cas les plus sensationnalistes, comme celui d’Amazon par exemple. Un <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" target="_blank" rel="noopener noreferrer">programme informatique</a> discriminait les femmes à l&#x27;embauche parce que les informations sur lesquelles l&#x27;intelligence artificielle avait basé son apprentissage étaient des effectifs historiquement masculins de développeurs informatiques.</p><p>Il existe un risque pour les professionnels qui créent des intelligences artificielles de se sentir éloignés de ces sujets sensationnalistes et, ne se sentant pas concernés, de ne pas faire attention aux biais dans leur travail. Par exemple, les équipes de Twitter qui ont travaillé sur l’algorithme qui choisit quelle partie d’une photo s’affiche en aperçu sur Twitter n’ont pas bien contrôlé les biais dans leur travail. Leur logiciel recadrait simplement l’image d’un utilisateur pour donner un aperçu pertinent de l&#x27;image. Pourtant, les <a href="https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm" target="_blank" rel="noopener noreferrer">utilisateurs ont remarqué</a> que, sur une photo avec plusieurs personnes, ce recadrage se focalisait plus sur les personnes blanches que les personnes noires. En effet, quand on se base sur les images présentes sur internet, les personnes blanches sont en moyenne plus mises en avant que les personnes noires.<br>
<!-- -->Il est donc recommandé à toute personne travaillant sur une intelligence artificielle de toujours s’interroger sur les biais possibles qu’elle est susceptible d’y introduire, aussi peu risqué que le projet puisse paraître.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="la-solution-proposée-par-la-commission-européenne-aux-problèmes-déthique-de-lia">La solution proposée par la Commission européenne aux problèmes d’éthique de l’IA<a class="hash-link" href="#la-solution-proposée-par-la-commission-européenne-aux-problèmes-déthique-de-lia" title="Direct link to heading">​</a></h3><p>Aujourd’hui, une des rares entités qui régule l’éthique est la Commission européenne avec une <a href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" target="_blank" rel="noopener noreferrer">proposition de loi qui réglemente l’intelligence artificielle</a>. A l’instar de la mise en œuvre du règlement général sur la protection des données (RGPD), de nombreuses entreprises se plaignent que la Commission européenne complique la vie des ingénieurs parce qu’elle ne comprend pas les implications techniques de ce qu’elle ordonne. Du point de vue du citoyen toutefois, la réglementation protège la vie privée mieux qu’avant. D’autant que, ces dernières années, minimiser le risque d&#x27;un bad buzz lié à une violation de la vie privée est peut-être devenu plus important pour les sociétés que les contraintes techniques que le RGPD engendre et a été le catalyseur de bonnes pratiques pour éviter ce type de risque.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="le-travail-de-datacraft">Le travail de datacraft<a class="hash-link" href="#le-travail-de-datacraft" title="Direct link to heading">​</a></h3><p>Des membres du Club <a href="https://datacraft.paris/" target="_blank" rel="noopener noreferrer">datacraft</a> se sont réunis tout au long de l&#x27;année 2021 pour apporter des réponses concrètes sur les questions d’IA éthique. C’est ainsi qu’ils ont abouti à la rédaction d’une <a href="https://datacraft.paris/project/trustworthy-ai-charter/" target="_blank" rel="noopener noreferrer">charte</a> pour formaliser les fondements d’une IA de confiance et mettre un accent particulier sur sa dimension éthique. Un groupe de data scientists issus d’entreprises (Antoine Isnardy de Danone, Théo Alves Da Costa d’Ekimetrics...), de labos de recherche (Nathan Noiry de Télécom Paris, Eliot Moll de l’Inria...), ou freelances en résidence datacraft (Yann Girard d’HephIA...) a aussi travaillé sur la <a href="https://github.com/datacraft-paris/ethical-ai-toolkit" target="_blank" rel="noopener noreferrer">cartographie</a> des outils open source du marché pour limiter les biais dans les modèles d’intelligence artificielle (tels que, entre autres, les biais sociaux comme les biais sur le genre, l’âge, …).<br>
<!-- -->Il est toujours possible d&#x27;apporter votre contribution à ce travail en <a href="https://datacraft.paris/join-us/" target="_blank" rel="noopener noreferrer">rejoignant datacraft</a> !</p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/ia-de-confiance">IA de confiance</a><a class="margin-horiz--sm" href="/blog/tags/biais">biais</a><a class="margin-horiz--sm" href="/blog/tags/ethique">éthique</a><a class="margin-horiz--sm" href="/blog/tags/data">data</a></div></footer></article><div class="margin-vert--xl"><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/AI-Fairness-et-benchathon2"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->Fairness in AI - How a benchathon unlocked our knowledge</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/AI-Fairness-et-benchathon"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Fairness in AI - How a benchathon unlocked our knowledge<!-- --> »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#pourquoi-les-biais-posent-des-problèmes-éthiques-en-intelligence-artificielle-" class="table-of-contents__link toc-highlight">Pourquoi les biais posent des problèmes éthiques en intelligence artificielle ?</a><ul><li><a href="#nous-avons-tous-des-biais" class="table-of-contents__link toc-highlight">Nous avons tous des biais</a></li><li><a href="#pourquoi-les-biais-dune-ia-sont-un-problème-" class="table-of-contents__link toc-highlight">Pourquoi les biais d’une IA sont un problème ?</a></li><li><a href="#personne-nest-à-labri-de-risques-éthiques" class="table-of-contents__link toc-highlight">Personne n’est à l’abri de risques éthiques</a></li><li><a href="#la-solution-proposée-par-la-commission-européenne-aux-problèmes-déthique-de-lia" class="table-of-contents__link toc-highlight">La solution proposée par la Commission européenne aux problèmes d’éthique de l’IA</a></li><li><a href="#le-travail-de-datacraft" class="table-of-contents__link toc-highlight">Le travail de datacraft</a></li></ul></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items"><li class="footer__item"><a href="https://datacraft.paris/about-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/assets/js/runtime~main.23ef1fa4.js"></script>
<script src="/assets/js/main.200c38ba.js"></script>
</body>
</html>