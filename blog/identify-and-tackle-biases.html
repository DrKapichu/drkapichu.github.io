<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="datacraft blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="datacraft blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-124520099-9","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script><title data-react-helmet="true">Fairness in AI - Identify and tackle biases | datacraft blog</title><meta data-react-helmet="true" property="og:title" content="Fairness in AI - Identify and tackle biases | datacraft blog"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="twitter:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="description" content="IA et biais (3/3) - ICI UN DESCRIPTION"><meta data-react-helmet="true" property="og:description" content="IA et biais (3/3) - ICI UN DESCRIPTION"><meta data-react-helmet="true" property="og:url" content="https://drkapichu.github.io//blog/identify-and-tackle-biases"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="keyword1,keyword2,keyword3"><link data-react-helmet="true" rel="icon" href="/img/datacraft_logo.png"><link data-react-helmet="true" rel="canonical" href="https://drkapichu.github.io//blog/identify-and-tackle-biases"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/identify-and-tackle-biases" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/identify-and-tackle-biases" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.aa5ca896.css">
<link rel="preload" href="/assets/js/runtime~main.2f015b7f.js" as="script">
<link rel="preload" href="/assets/js/main.0f62061b.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/opensource">Open Source</a></div><div class="navbar__items navbar__items--right"><a href="https://datacraft.paris/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">datacraft website</a><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Newsletter</a><a href="https://github.com/datacraft-paris" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_RC3s">Fairness in AI - Identify and tackle biases</h1><div class="margin-vert--md"><p>IA et biais (3/3) - ICI UN DESCRIPTION</p><time datetime="2022-05-20T00:00:00.000Z" class="blogPostDate_IAgm">May<!-- --> <!-- -->20<!-- -->, <!-- -->2022<!-- --> <!-- --> · <!-- -->12<!-- --> min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="mailto:eliot.moll@inria.fr" target="_blank" rel="noreferrer noopener">Eliot Moll</a></h4><small class="avatar__subtitle">POSTE À L&#x27;INRIA</small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/img/blog/pikachu.png"></div></header><section class="markdown blog-article-custom"><hr><p>Welcome to the third article of a series that aims to share how a group of private players (Danone, Ekimetrics, Datacraft) and researchers (Telecom Paris, Inria), partnered to uncover fairness &amp; ethics in Artificial Intelligence from a practical standpoint. The group tried to tackle the following challenge: “how should a Data Scientist concretely react when exposed to fairness concerns?”. If you are interested to understand how this initiative kick started, have a look <a href="https://datacraft-paris.github.io/blog/Des-biais-cognitifs-aux-biais-des-algorithmes" target="_blank" rel="noopener noreferrer">here</a>.</p><p>These workshops led to the creation of an <a href="https://datacraft-paris.github.io/trustworthyai/accountability.html" target="_blank" rel="noopener noreferrer">AI ethics charter</a>, a <a href="http://tbd/" target="_blank" rel="noopener noreferrer">benchmark of (free/open-source) tools available on the market</a> <span style="background-color:red;color:green">INSERT RIGHT LINK HERE</span> and a hands-on experiment to discover and mitigate biases in a prediction model. This article focuses on the last part but feel free to check out links for the first two ones.</p><p><strong>TL;DR</strong>: This article focuses its analysis on algorithmic biases. Even if observed biases mostly come from biases in input data it is usually easier to observe and mitigate these biases by working with outcomes of machine learning models.</p><p><img alt="Photo by Tingey Injury Law Firm on Unsplash" src="/assets/images/img1-32ffeafb6dcf59ea333c91bd13d3d9dc.jpg" title="Photo by Tingey Injury Law Firm on Unsplash."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Photo by <a href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Tingey Injury Law Firm</a> on <a href="https://unsplash.com/s/photos/justice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener noreferrer">Unsplash</a>.</div><p>Fairness is a very general concept whose meaning depends strongly on the context. In this article, we will focus on algorithmic biases which may be responsible for discriminative treatments on a subgroup of a population.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="origin-of-biases">Origin of biases<a class="hash-link" href="#origin-of-biases" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="input-data">Input data<a class="hash-link" href="#input-data" title="Direct link to heading">​</a></h3><p>In most cases, the input data used by a machine learning (ML) algorithm during its training is the main cause of the inequities observed in the algorithm’s outcomes.</p><p>Indeed, the data collection process can often lead to bias in the sense that the training sample is not representative of the population. Let us give two examples of these kinds of biases:  </p><ul><li>Selection Bias: Tracked data might not be representative of the true reality, such as data from people agreeing to answer a survey may not represent your entire population</li><li>Measure Bias: Systematic measurement error (technical or not) like a camera contrast calibration which may not return the same degree of details from all skin colors.</li></ul><p>Moreover, if the collected data are representative, it is only representative of the current state of the world, which, as you may know, is full of biases. Many unfair judgements such as racism and misogyny are still perpetuated, consciously or unconsciously, in some of our behaviors.</p><p>It is crucial for people technically in charge to be aware of all potential bias in order to check, control or alter data before any use.</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="illustration-with-a-dataset">Illustration with a dataset<a class="hash-link" href="#illustration-with-a-dataset" title="Direct link to heading">​</a></h4><p>Let’s use a biased dataset about <a href="https://www.kaggle.com/uciml/german-credit" target="_blank" rel="noopener noreferrer">banking credit</a>. It contains clients characteristics (savings, age, gender, etc.) and their classification as “at risk” or “not at risk” to default on their loan repayments. The risk assessment comes from an individual decision of a bank officer.</p><p>As a banking analyst and based on information just above, one might be afraid of the presence of social and cognitive biases in this data. For some reasons (ethical, legal, corporate branding, etc.) the bank (through its analysts) wants to be sure to have no bias according to age or gender in the decision to grant a credit.</p><p>As a first step, audit/exploration of data is mandatory. A lot of tools and products are available on the market to explore, visualize and dig into data. We decided to use an open-source tool that can be plugged in a python notebook: <a href="https://pair-code.github.io/facets/" target="_blank" rel="noopener noreferrer">Facets</a> (from Google). Its interactive interface allows to display distributions, aggregated values and also intersect variables to discover cross-variable effects.</p><p><img alt="Example of a Facets (Google) visualization. Risk value splitted by Age and Gender." src="/assets/images/img2-e4eed43b24001f036ebad8131a4a1070.png" title="Example of a Facets (Google) visualization. Risk value splitted by Age and Gender."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Example of a Facets (Google) visualization. Risk value splitted by Age and Gender.</div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="models-predictions">Models predictions<a class="hash-link" href="#models-predictions" title="Direct link to heading">​</a></h3><p>It is usually easier to measure biases in model outcomes. Indeed, measuring biases inside a high dimensional dataset remains a challenge which gives rise to several research papers each year, while the outcome of an algorithm is usually a number which lends itself well to a statistical analysis. Still, the belief of the community is that a bias in the data should reflect in a bias in the outcomes.</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="fairness-metrics">Fairness Metrics<a class="hash-link" href="#fairness-metrics" title="Direct link to heading">​</a></h4><p>To check if a model is biased or not, some new metrics (instead of traditional model performance metrics) are needed. There are plenty of fairness-related metrics, but here are the 3 most commonly used ones:  </p><ul><li>Statistical/Demographic Parity: Groups have an equal probability of being positively predicted.</li><li>Equal Odds: Groups have an equal True Positive Rate (probability of a true positive to be predicted positively among all true positives) and equal False Positive Rate (probability of a true negative to be predicted positively among all true negatives)</li><li>Equal Opportunity: Groups have an equal probability of a False Negative Rate (probability of a true positive to be predicted negatively among all true positives)</li></ul><p>During workshops, we mainly used a python package called “Dalex” to measure unfairness of results. Since this tool was not mentioned in the benchmark, let’s start by a quick presentation.</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="dalex">Dalex<a class="hash-link" href="#dalex" title="Direct link to heading">​</a></h4><p>The <a href="https://github.com/ModelOriented/DALEX" target="_blank" rel="noopener noreferrer">Dalex</a> package is an open-source library developed in R and python by <a href="https://www.mi2.ai/" target="_blank" rel="noopener noreferrer">MI²DataLab</a>, a research group in machine learning, based in the Warsaw University of Technology. Both libraries have been maintained by the team since their creation in 2018.</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="how-to-use-dalex-python-version">How to use Dalex (python version)<a class="hash-link" href="#how-to-use-dalex-python-version" title="Direct link to heading">​</a></h4><p>The dalex package is quite easy to use and can be added to any existing machine learning project (under experimentation, or even in production). It is an on-top library providing explanations and measures of existing models. For instance, it interfaces particularly well with scikit-learn models.</p><p>In our case, it provides 2 highly relevant modules:  </p><ul><li><strong>Explainer</strong>: This object encapsulates a model and its data. It can already provide some measures about the model and its performance (model type, set of predicted values, precision, f1-score, etc.). The use is as quick and easy as shown below.</li></ul><div class="codeBlockContainer_J+bg language-python theme-code-block"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">import</span><span class="token plain"> dalex</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">explainer_object </span><span class="token operator">=</span><span class="token plain"> dalex</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">Explainer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">fitted model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    explanatory_columns_train_set</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> target_column_train_set</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><ul><li><strong>Fairness Object</strong>: This object (based on the Explainer one) allows to measure bias between some specified groups. These groups, also called “protected groups”, have to be manually defined. It is an easy task when protected groups are built on a qualitative variable (e.g. men VS women) but it’s more tricky when it is built on a quantitative variable (e.g. age). For the second case, the threshold to create two groups from a quantitative variable has to be set by the analyst (no automatic optimisation is done by Dalex to define the best threshold). So far, there are about 10 fairness metrics that are implemented to compare protected groups (including the 3 presented previously).</li></ul><div class="codeBlockContainer_J+bg language-python theme-code-block"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">fairness_object </span><span class="token operator">=</span><span class="token plain"> explainer_object</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">model_fairness</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    protected </span><span class="token operator">=</span><span class="token plain"> protected_variable</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    privileged </span><span class="token operator">=</span><span class="token plain"> protected_variable_value_used_as_reference</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>It’s quite utapian to think it’s possible to achieve a perfect fairness according to multiple metrics (or even just one). It is more realistic to ensure a relatively equal treatment with an acceptance margin. Dalex provides this relative analysis with a by-default 20% margin (to learn more about this practice and this threshold, see the <a href="https://en.wikipedia.org/wiki/Disparate_impact" target="_blank" rel="noopener noreferrer">Disparate Impact page</a>).</p><p>Based on the practical example, let’s visualize differences of treatment between groups. Let’s remind us all that the sensitive variables in the example are gender and age. Since the gender only contains 2 distinct values, the two natural groups from this variable are already set up. However the age variable is quantitative. By fixing an arbitrary threshold at 25 years old, it’s now possible to have 4 combined groups (gender and age): “0(male) old”, “1(female) young”, “1(female) old” and “0(male) young”. Among these groups, the one suspected of being favored is “male old” and will be used as reference to compute relative ratio. Below, a result according to 3 fairness metrics returned by Dalex.</p><p><img alt="An example of 3 fairness metrics returned by Dalex" src="/assets/images/img3-052d97c369c16db1f187a97352cc005c.png" title="An example of 3 fairness metrics returned by Dalex"></p><p>Example of plot available in Dalex. This represents the relative treatment (compared to the reference group “male_old”) according to 3 fairness metrics for the 3 other groups. A bar in a green area means there is no bias (at a 20% level) for the fairness metric between the corresponding group and the reference group “male_old” for the trained decision tree model.</p><p>Thanks to Dalex, a quick view on the previous plot allows to highlight some bias issues in the treatment of young males and females compared to older males (reference group). Now that biases have been discovered, how can we deal with them? </p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="mitigation-of-biases">Mitigation of biases<a class="hash-link" href="#mitigation-of-biases" title="Direct link to heading">​</a></h2><p>In a perfect world, data collection pipelines should be done regarding the possibilities of bias. However, most of the time, that’s already too late since data is already collected when the analyst has to create a prediction model. At this point, it’s still possible to unbias results by altering the input data (pre-processing), constraining the model (in-processing) or altering predictions (post-processing). All these solutions are possible (to a certain extent) with Dalex.</p><p>To keep this presentation as short and popularized as possible, lines of code for this part will not be displayed but notebooks are availables at the end.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pre-processing-treatment">Pre-processing treatment<a class="hash-link" href="#pre-processing-treatment" title="Direct link to heading">​</a></h3><p>The pre-processing step is the modification of the input data to improve the fairness-performance of the model.</p><p>Two ways of dealing with this kind of treatment are implemented in Dalex:</p><ul><li><strong>Resampling observations</strong>: Resampling commonly ensures a better balanced relative presence of each group.</li><li><strong>Weighting observations</strong>: Applying weights on observation allows to reduce or increase the impact of each observation to obtain a better consideration of minority cases.</li></ul><p><img alt="Effect of a uniform resampling." src="/assets/images/img4-acb02f9d8db768023ef55c0356c4b438.png" title="Effect of a uniform resampling."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Effect of a uniform resampling before the training (blue) compared to the initial results (green) according to 3 fariness metrics.</div><p>Effect of a uniform resampling before the training (blue) compared to the initial results (green) according to 3 fariness metrics.</p><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 class="anchor anchorWithStickyNavbar_y2LR" id="in-processing-treatment">In-processing treatment<a class="hash-link" href="#in-processing-treatment" title="Direct link to heading">​</a></h2><p>The classic way to train a ML model consists in learning parameters of an algorithm (e.g. SVM, RF…) so as to minimize a loss function $\mathcal{L}<em>{perf}$that accounts for the performance. In-processing mitigations of bias techniques consists in incorporating to the loss function another term criteria, $$\mathcal{L}</em>{fair}$$, , that also takes into account fairness constraints. Usually, the resulting loss function writes</p><p>[<!-- --> \mathcal{L} = \mathcal{L}<em>{perf} + \lambda \mathcal{L}</em>{fair}, <!-- -->]</p><p>where $\lambda&gt;0$ is a hyperparameter controlling the fairness-accuracy tradeoff (the larger it is, the more fair is the model). For instance, $\mathcal{L}<em>{fair}$ can be chosen as the absolute difference between false positive among males and false positive among females. Another promising line of search consists in taking $\mathcal{L}</em>{fair}$ as the opposite loss of a classifier designed to predict the gender, which results in so-called adversarial sensitive label removal techniques.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="post-processing-treatment">Post-processing treatment<a class="hash-link" href="#post-processing-treatment" title="Direct link to heading">​</a></h2><p>The last possibility is to alter outcomes of a model. This idea is to change some results to improve the fairness of the model with the least possible degradation of the model performance (f1-score, accuracy, etc.).</p><p>In a classication case (like the risk scoring), predictions close to the change of class (between “at risk” and “not at risk”) changes may be done in favor of the disadvantaged group or in disfavor of the advantaged one to rebalance the fairness-performance without perturbing the overall logic of the trained model.</p><p>Dalex offers 1 post-processing solution called “ROC-pivot” (in this context the name “ROC” might may you think about the ROC curve, but it’s not linked). It does basically what was explained just above. The “close to the change” has to be manually defined as margin value (usually a small value like 5 or 10% of the cutoff value between the two groups).</p><p><img alt="Effect of the Dalex ROC-pivot method." src="/assets/images/img5-6123915b606ded7ab9c16075afca0634.png" title="Effect of the Dalex ROC-pivot method."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Effect of the Dalex ROC-pivot method (blue) compared to the initial results (green) according to 3 fairness metrics..</div><p>Effect of the Dalex ROC-pivot method (blue) compared to the initial results (green) according to 3 fairness metrics.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>Studying biases in machine learning is a growing field in the AI landscape and it will probably be a non-negligible part of the deployment process as soon as the European AI Act will be in effect.</p><p>Some tools like Dalex or <a href="https://datacraft-paris.github.io/blog/AI-Fairness-et-benchathon2" target="_blank" rel="noopener noreferrer">those presented in the first article of this series</a> will probably be part of the Data Scientist toolkit as much as any machine learning library.</p><p>Before being backed into a corner, Data Scientists should first get familiar with the fairness field. As we saw, dealing with them is not easy and there is no magic recipe to mitigate an observed bias. Then, they should start thinking about efficient processes to deal with ethical questions since it will probably lead to cross-fields discussions with new stakeholders in the company (legal department, public relations department, etc.).</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="links--references">Links &amp; References<a class="hash-link" href="#links--references" title="Direct link to heading">​</a></h2><p><a href="https://github.com/datacraft-paris/ethical-ai-toolkit/blob/main/notebooks/ethical_ai.ipynb" target="_blank" rel="noopener noreferrer">Datacraft workshop notebook</a> (for full code)</p><p><a href="https://github.com/ModelOriented/DALEX" target="_blank" rel="noopener noreferrer">Dalex Github Repo</a></p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/tag-1">tag1</a><a class="margin-horiz--sm" href="/blog/tags/tag-2">tag2</a><a class="margin-horiz--sm" href="/blog/tags/tag-3">tag3</a><a class="margin-horiz--sm" href="/blog/tags/tag-4">tag4</a></div></footer></article><div class="margin-vert--xl"><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/automl-frameworks-and-libraries"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->An exploration of autoML frameworks and libraries for a possible workshop</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/sense4data-kubernetes"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Un petit mémo à propos de Kubernetes<!-- --> »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#origin-of-biases" class="table-of-contents__link toc-highlight">Origin of biases</a><ul><li><a href="#input-data" class="table-of-contents__link toc-highlight">Input data</a></li><li><a href="#models-predictions" class="table-of-contents__link toc-highlight">Models predictions</a></li></ul></li><li><a href="#mitigation-of-biases" class="table-of-contents__link toc-highlight">Mitigation of biases</a><ul><li><a href="#pre-processing-treatment" class="table-of-contents__link toc-highlight">Pre-processing treatment</a></li></ul></li><li><a href="#in-processing-treatment" class="table-of-contents__link toc-highlight">In-processing treatment</a></li><li><a href="#post-processing-treatment" class="table-of-contents__link toc-highlight">Post-processing treatment</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#links--references" class="table-of-contents__link toc-highlight">Links &amp; References</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items"><li class="footer__item"><a href="https://datacraft.paris/about-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/assets/js/runtime~main.2f015b7f.js"></script>
<script src="/assets/js/main.0f62061b.js"></script>
</body>
</html>