<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.14">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="datacraft blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="datacraft blog Atom Feed"><title data-react-helmet="true">Fairness in AI - How a benchathon unlocked our knowledge | datacraft blog</title><meta data-react-helmet="true" property="og:title" content="Fairness in AI - How a benchathon unlocked our knowledge | datacraft blog"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="twitter:image" content="https://drkapichu.github.io//img/datacraft-team-3.JPG"><meta data-react-helmet="true" name="description" content="Benchmark of Python packages for AI fairness"><meta data-react-helmet="true" property="og:description" content="Benchmark of Python packages for AI fairness"><meta data-react-helmet="true" property="og:url" content="https://drkapichu.github.io//blog/AI-Fairness-et-benchathon"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" name="keywords" content="fairness,ethicalai,Aequitas,Shapash,aif360,Dalex,fairlearn,What if tool"><link data-react-helmet="true" rel="icon" href="/img/datacraft_logo.png"><link data-react-helmet="true" rel="canonical" href="https://drkapichu.github.io//blog/AI-Fairness-et-benchathon"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/AI-Fairness-et-benchathon" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://drkapichu.github.io//blog/AI-Fairness-et-benchathon" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.3166031a.css">
<link rel="preload" href="/assets/js/runtime~main.ecb5232a.js" as="script">
<link rel="preload" href="/assets/js/main.2f4129da.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/datacraft_logo_full.png" alt=" " class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/opensource">Open Source</a></div><div class="navbar__items navbar__items--right"><a href="https://datacraft.paris/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">datacraft website</a><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Newsletter</a><a href="https://github.com/datacraft-paris" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper"><div class="container container-wide margin-vert--lg"><div class="row"><div class="col col--2"></div><main class="col col--8"><article><header><h1 class="margin-bottom--sm blogPostTitle_RC3s">Fairness in AI - How a benchathon unlocked our knowledge</h1><div class="margin-vert--md"><p>Benchmark of Python packages for AI fairness</p><time datetime="2022-04-01T00:00:00.000Z" class="blogPostDate_IAgm">April<!-- --> <!-- -->1<!-- -->, <!-- -->2022<!-- --> <!-- --> · <!-- -->11<!-- --> min read</time></div><div class="avatar margin-vert--md"><div class="avatar__intro"><h4 class="avatar__name">Written by <a href="mailto:antoine.isnardy@danone.com" target="_blank" rel="noreferrer noopener">Antoine Isnardy</a></h4><small class="avatar__subtitle">Lead Data Scientist at Danone</small></div></div><div class="margin-vert--md"><img class="img-blog-header" src="/img/blog/AIFairness-benchathon2.jpg"></div></header><section class="markdown blog-article-custom"><hr><header><h1>Fairness in AI - How a benchathon unlocked our knowledge</h1></header><p>Let&#x27;s all board on a journey to the land of AI fairness that we, a group of private players (Danone, Ekimetrics, datacraft), researchers (Telecom Paris, Inria), and students (Université de Cergy), partnered to uncover fairness &amp; ethics in Artificial Intelligence from a practical standpoint. Through this journey the group tried to tackle the following challenge: “how should a Data Scientist concretely react when exposed to fairness concerns?”.  </p><p>We started with a series of workshops to discuss the existing tools and methods, assess the needs and define an <a href="https://datacraft-paris.github.io/trustworthyai/" target="_blank" rel="noopener noreferrer">ethical charter</a>. This led to the writing of a first popularisation article, in French, which you can find <a href="/blog/biais-humains-et-algorithmes">here</a>.<br>
<!-- -->Then we continued our journey with a benchathon - definition below - in which the group participated to get a quick and documented opinion of an already rich fairness/ethical ecosystem. It first explains how the concept of benchathon accelerated our practical grasp of the topic, and then explores the first conclusions drawn about the fairness ecosystem. You can find the code developped during this benchathon on the <a href="https://github.com/datacraft-paris/Fairness-Benchathon" target="_blank" rel="noopener noreferrer">datacraft&#x27;s GitHub repo</a>.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="benchathon-as-an-innovation-catalyst">Benchathon as an innovation catalyst<a class="hash-link" href="#benchathon-as-an-innovation-catalyst" title="Direct link to heading">​</a></h2><p>At this stage of our “fairness journey”, we had a decent high level understanding of what fairness could imply in real life. It was then the right time to start acting concrete: try and derive a pragmatic methodology, even if it implied implementing our own routines.</p><p>To that end, a very first step was to make sure we’d not reinvent the wheel, and we’d plainly benefit from existing open source contributions. This happened during a one-day benchathon. If you work in tech, you may already be familiar with the following two notions:</p><ul><li>Benchmark: <em>gathering and comparing qualitative information about how an activity is conducted through people, processes, and technology</em> (Source: <a href="https://www.apqc.org/blog/what-are-four-types-benchmarking" target="_blank" rel="noopener noreferrer">https://www.apqc.org/blog/what-are-four-types-benchmarking</a>)  </li><li>Hackathon: <em>[short]<!-- --> event <!-- -->[...]<!-- --> in which computer programmers and others involved in software development <!-- -->[...]<!-- --> collaborate intensively on software projects</em> (Source: <a href="https://en.wikipedia.org/wiki/Hackathon" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Hackathon</a>)  </li></ul><p>Hackathons usually involve teams that compete on the “same topic” for 2 to 3 days. Because we were limited in time - 1 day, rather than focusing all on the same “thing”, we decided to make the most out of the presence of 9 data scientists: we shared and split between us the technical analysis of several fairness open source libraries - <a href="https://github.com/Trusted-AI/AIF360" target="_blank" rel="noopener noreferrer">AIF360</a>, <a href="https://github.com/MAIF/shapash" target="_blank" rel="noopener noreferrer">Shapash</a>, <a href="https://github.com/dssg/aequitas" target="_blank" rel="noopener noreferrer">Aequitas</a>, <a href="https://pair-code.github.io/what-if-tool/" target="_blank" rel="noopener noreferrer">What if tool</a>, <a href="https://fairlearn.org/" target="_blank" rel="noopener noreferrer">Fairlearn</a>. Hence the concept of benchathon.</p><p>Even though all of us were entitled as “Data Scientists”, we all came from different structures, different backgrounds, and different (coding) habits. That diversity definitely triggered (and still does) great discussions and perspectives along the initiative. Still, an important step during the benchathon was to settle on an interpretation grid that would make the outcome as reusable and general as possible, and as unbiased as possible - in line with the topic then :). A few criteria were identified:</p><table><tr><th>   <strong>Criteria</strong>                  </th><th>    <strong>Description</strong>                                                                                                             </th><th>   <strong>Scale</strong>           </th>  </tr><tr><td align="left" bgcolor="white"> Installation  </td><td align="left" bgcolor="white"> How easy is it to get started?                                                                               </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Usability     </td><td align="left" bgcolor="white"> How easy to use is the API?                                                                                  </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Documentation </td><td align="left" bgcolor="white"> How well documented is the library?                                                                          </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Completeness  </td><td align="left" bgcolor="white"> Does the library perform everything it is supposed to?                                                       </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Reliability   </td><td align="left" bgcolor="white"> Does the library seem reliable? (code quality, tests, …)                                                     </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Legitimacy    </td><td align="left" bgcolor="white"> Is the library popular within the community? (number of stars on GitHub, latest commit, number of issues, …) </td><td align="left" bgcolor="white"> 1-5 </td>  </tr><tr><td align="left" bgcolor="white"> Future        </td><td align="left" bgcolor="white"> Gut instinct - would you trust it and use it in real projects?                                               </td><td align="left" bgcolor="white"> Y/N </td>  </tr><tr><td align="left" bgcolor="white"> Weaknesses    </td><td align="left" bgcolor="white"> What is currently missing?                                                                                   </td><td align="left" bgcolor="white"> N/A </td>  </tr></table><p>That being set, what was important was also to pace the day, so that despite the fact that small groups worked independently, we always kept an overall coherence and dynamics. It meant:</p><ul><li>Mini sprints of 1,5 hours</li><li>At the end of each mini sprint, a quick roundtable to share insights or blocking points, and get challenged by the whole group</li><li>Lunch break altogether: everyone brought something to share. This was a great moment of conviviality. It would even seem that a new datacraft initiative was born at this very moment, stay tuned!</li><li>At the end of the day, wrap up session during which each group made a demo of the library it spent the day on, and made sure to fill out above-mentioned criteria. The latter was especially important because this is what helps us today to have a concrete reference that every one can refer to.</li></ul><p>Taking a step back, below are a few takeaways:</p><ul><li>This benchathon was extremely productive: in the matter of only a day, our practical grasp of the fairness/ethical ecosystem clearly passed a milestone (see next section).</li><li>All people around the table had a developer background, and the same objective - namely, uncovering the fairness topic from a technical &amp; practical standpoint. It helped to get started fairly quickly, and proved that this format was a great fit for that audience and purpose.</li><li>One mistake we made was not to invest enough time beforehand in mapping the main open source libraries available in the AI community. It turns out we missed what would become our GO TO in the future: <a href="https://github.com/ModelOriented/DALEX" target="_blank" rel="noopener noreferrer">Dalex</a>.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="highlight-of-6-ethicalfairness-libraries">Highlight of 6 ethical/fairness libraries<a class="hash-link" href="#highlight-of-6-ethicalfairness-libraries" title="Direct link to heading">​</a></h2><p>If you are further interested in the exhaustive findings on the five libraries that were studied during the benchathon, we summarized our conclusions in this <a href="https://github.com/datacraft-paris/Fairness-Benchathon/blob/master/docs/Benchathon-summary.pdf" target="_blank" rel="noopener noreferrer">file</a>. The following section aims at providing a (subjective) summary of these libraries, in increasing relevance order, with respect to fairness / ethics.</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="shapash">Shapash<a class="hash-link" href="#shapash" title="Direct link to heading">​</a></h3><p><a href="https://github.com/MAIF/shapash" target="_blank" rel="noopener noreferrer">Open source library</a> developed by MAIF - a French insurance actor, and Quantmetry - a French AI consultancy, that mainly focuses on interpretability (no built-in fairness-oriented feature). It acts as a layer on top of the usual interpretability toolbox (feature importance, SHAP values, …). It comes with a very decent web interface, high quality code, and a great community/documentation. It also provides an audit report of the project (from data prep to modeling, to exploratory analysis).</p><p>In a nutshell: great project, but not that relevant (yet?) for fairness topics.</p><p><img alt="Screenshot of Shapash." src="/assets/images/Shapash-2a18e60b585020d550a6d7cba9e7eaa7.jpg" title="Screenshot of Shapash."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Screenshot of Shapash. <a href="https://github.com/MAIF/shapash" target="_blank" rel="noopener noreferrer">Credits</a>.</div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="what-if-tool">What if tool<a class="hash-link" href="#what-if-tool" title="Direct link to heading">​</a></h3><p><a href="https://pair-code.github.io/what-if-tool/" target="_blank" rel="noopener noreferrer">Open source interface</a> developed by Google. It mainly aims at conducting counterfactual analysis (“what would be the machine learning model prediction if we changed the value of that particular attribute, like the sex e.g.?”). It comes with a decent web interface, especially to deal with unstructured data like images. Documentation is however not handy to deal with.</p><p>In a nutshell: great interface. However, counterfactual analysis is only one (important) feature among the different aspects related to fairness, which in turn does not justify a lock-in with that specific tool.</p><p><img alt="Screenshot of What-if tool." src="/assets/images/What_if_tool-1bf8f4b04ca7846adc517306b1dd5f4a.jpg" title="Screenshot of What-if tool."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Screenshot of What-if tool. <a href="https://github.com/pair-code/what-if-tool" target="_blank" rel="noopener noreferrer">Credits</a>.</div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="aequitas">Aequitas<a class="hash-link" href="#aequitas" title="Direct link to heading">​</a></h3><p><a href="http://aequitas.dssg.io/" target="_blank" rel="noopener noreferrer">Aequitas</a> is a bias and audit toolkit developed by Carnegie Mellon University. It aims at spotting unfair allocation compared to population repartition or wrong decisions about certain groups of people. It comes with a web interface (which we could not make work) and a Python library to help compute fairness metrics. Documentation is decent, especially their representation of the <a href="http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/" target="_blank" rel="noopener noreferrer">&quot;fairness tree&quot;</a>, which helps to navigate the (many and ambiguous) fairness metrics, depending on the use case.</p><p>In a nutshell: Aequitas is a tool that has been available for quite some time now, but that does not benefit from a living community. To be kept under the radar (or contribute to!).</p><p><img alt="Screenshot of Aequitas web application." src="/assets/images/Aequitas-9c7a24656682c403120ea61fe07e2753.jpg" title="Screenshot of Aequitas web application."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Screenshot of Aequitas web application. <a href="http://aequitas.dssg.io/" target="_blank" rel="noopener noreferrer">Credits</a>.</div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="fairlearn">Fairlearn<a class="hash-link" href="#fairlearn" title="Direct link to heading">​</a></h3><p><a href="https://fairlearn.org/" target="_blank" rel="noopener noreferrer">Fairlearn</a> is an open source library maintained by diverse contributors (from Microsoft, Zalando, …). It aims at tackling each step of the fairness value chain. It implements fairness metrics, of which you have a summary below:</p><p><img alt="Figure complemented by datacraft&amp;#39;s initiative" src="/assets/images/Fairlearn-1-48ac6dab8d7e909d170a3d54c9eca17e.jpg" title="Figure complemented by datacraft&#x27;s initiative"></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Credits: <a href="#reference">[1]</a> - complemented by datacraft’s ethical initiative.</div><p>Fairlearn implements mitigation techniques:</p><ul><li>Pre-processing methods: alter a training set before training a model (example in fairlearn: removing sensitive correlations)</li><li>In-processing method: train a model (or a sequence of models) accounting for fairness constraints (example in fairlearn: exponentiated gradient <a href="#reference">[2]</a>)</li><li>Post-processing methods: alter predictions to account for fairness constraints, once a model is trained (example in fairlearn: threshold optimization post processing algorithm <a href="#reference">[3]</a>)</li></ul><p>It also tries to go beyond the usual binary classification problem, which is the usual go-to when uncovering the fairness topic (e.g. giving a try at regression). However, making our way through the “get started” procedure or the documentation - yet well designed and appealing, was no easy task. Note that the library also comes with nice dashboards that allow, among others, model comparison.</p><p>In a nutshell: promising and active library for fairness topics. Accessibility could be improved. To definitely keep an eye on (or contribute to!).</p><p><img alt="Screenshot of the Fairlearn dashboard." src="/assets/images/Fairlearn-2-9d99856c58780c027ecfa7acc9ae0480.jpg" title="Screenshot of the Fairlearn dashboard."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Screenshot of the Fairlearn dashboard. <a href="https://opendatascience.com/how-to-assess-ai-systems-fairness-and-mitigate-any-observed-unfairness-issues/" target="_blank" rel="noopener noreferrer">Credits</a>.</div><h3 class="anchor anchorWithStickyNavbar_y2LR" id="aif360">AIF360<a class="hash-link" href="#aif360" title="Direct link to heading">​</a></h3><p><a href="http://aif360.mybluemix.net/" target="_blank" rel="noopener noreferrer">AIF360</a> is an open source library developed by IBM. From our perspective, and before doing this initiative, this library was considered as the go-to for tackling fairness topics. It comes with an online tool, implements a wide range of mitigation techniques:</p><ul><li>Pre-processing methods among which reweighting <a href="#reference">[4]</a>, or learning fair representations <a href="#reference">[5]</a></li><li>In-Processing methods: grid search reduction <a href="#reference">[6, 7]</a></li><li>Post-processing methods: equalized odds postprocessing <a href="#reference">[8, 9]</a></li></ul><p>It also benefits from a wide community, and comes with a user-friendly web interface. </p><p>However rich in terms of features / mitigation techniques, the documentation is quite poor (it is not unusual to go and directly look into the source code to get answers). Besides, (useful) snippets of code are disseminated in various Jupyter notebooks, which slows down the appropriation. Last, some choices regarding data representation (formatting) and/or object declaration/instantiation (like the main explainer object, which is quite verbose), led us to troubles when trying to get used to the library.</p><p>In a nutshell: AIF360 is a very rich and mature ecosystem. Accessibility is however currently an obstacle to its full exploitation.</p><p><img alt="Screenshot of AIF360." src="/assets/images/AIF360-9deb604be90f0ffe7f55d7bc381fcdbc.jpg" title="Screenshot of AIF360."></p><div style="text-align:center;margin-left:9em;margin-right:9em;margin-bottom:5em">Screenshot of AIF360. <a href="https://aif360.mybluemix.net/" target="_blank" rel="noopener noreferrer">Credits</a>.</div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>At this point in time, we had discovered very interesting libraries, some of them backed by great communities and capabilities. Still, some open points remained that we thought would be worth investing time on:</p><ul><li>There was no clear winner: each library came with pros and cons. An ideal tool should be able to combine the best of each.</li><li>All those tools were very much focused (and still are) on the tooling, namely implementing a wide set of mitigation techniques or fairness dashboards. However, we were still missing a systematic framework for tackling fairness topics, that not only would make practical tools available, but that would also provide the associated reasoning: what question should a data scientist ask themselves? In which situation? Who should take part in this or that sensitive decision with respect to the model, …?</li></ul><p>This is what will be tackled in the third article of this series. We’ll introduce Dalex, another library that will be used as a foundation to derive (our interpretation of) the whole reasoning when exposed to fairness / ethical concerns.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="reference">Reference<a class="hash-link" href="#reference" title="Direct link to heading">​</a></h2><p>[1]<!-- --> - Credits: Data Robot – Trusted AI 102: A Guide to Building Fair and Unbiased AI Systems<br>
<!-- -->[2]<!-- --> - <a href="https://arxiv.org/abs/1803.02453" target="_blank" rel="noopener noreferrer">Agarwal et al. (2018)  A Reductions Approach to Fair Classification</a><br>
<!-- -->[3]<!-- --> - <a href="https://arxiv.org/pdf/1610.02413.pdf" target="_blank" rel="noopener noreferrer">M. Hardt, E. Price, N. Srebro (2018) - Equality of Opportunity in Supervised Learning</a><br>
<!-- -->[4]<!-- --> - F. Kamiran and T. Calders, &quot;Data Preprocessing Techniques for Classification without Discrimination,&quot; Knowledge and Information Systems, 2012<br>
<!-- -->[5]<!-- --> - R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, &quot;Learning Fair Representations.&quot; International Conference on Machine Learning, 2013<br>
<!-- -->[6]<!-- --> - <a href="https://arxiv.org/abs/1803.02453" target="_blank" rel="noopener noreferrer">A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach (2018) - A Reductions Approach to Fair Classification - International Conference on Machine Learning</a><br>
<!-- -->[7]<!-- --> - <a href="https://arxiv.org/abs/1905.12843" target="_blank" rel="noopener noreferrer">A. Agarwal, M. Dudik, and Z. Wu (2019) - Fair Regression: Quantitative Definitions and Reduction-based Algorithms - International Conference on Machine Learning</a><br>
<!-- -->[8]<!-- --> - M. Hardt, E. Price, and N. Srebro, &quot;Equality of Opportunity in Supervised Learning&quot; Conference on Neural Information Processing Systems, 2016.<br>
<!-- -->[9]<!-- --> - G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, &quot;On Fairness and Calibration,&quot; Conference on Neural Information Processing Systems, 2017.</p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/benchathon">benchathon</a><a class="margin-horiz--sm" href="/blog/tags/fairness">fairness</a><a class="margin-horiz--sm" href="/blog/tags/aif-360">aif360</a><a class="margin-horiz--sm" href="/blog/tags/fairlearn">fairlearn</a><a class="margin-horiz--sm" href="/blog/tags/ethicalai">ethicalai</a></div></footer></article><div class="margin-vert--xl"><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/Des-biais-cognitifs-aux-biais-des-algorithmes"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« <!-- -->Comment nos préjugés deviennent un problème éthique pour une intelligence artificielle ?</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/html"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">A list of possible styles (stolen everywhere)<!-- --> »</div></a></div></nav></div></main><div class="col col--2"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#benchathon-as-an-innovation-catalyst" class="table-of-contents__link toc-highlight">Benchathon as an innovation catalyst</a></li><li><a href="#highlight-of-6-ethicalfairness-libraries" class="table-of-contents__link toc-highlight">Highlight of 6 ethical/fairness libraries</a><ul><li><a href="#shapash" class="table-of-contents__link toc-highlight">Shapash</a></li><li><a href="#what-if-tool" class="table-of-contents__link toc-highlight">What if tool</a></li><li><a href="#aequitas" class="table-of-contents__link toc-highlight">Aequitas</a></li><li><a href="#fairlearn" class="table-of-contents__link toc-highlight">Fairlearn</a></li><li><a href="#aif360" class="table-of-contents__link toc-highlight">AIF360</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#reference" class="table-of-contents__link toc-highlight">Reference</a></li></ul></div></div></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items"><li class="footer__item"><a href="https://datacraft.paris/about-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Who we are ?</a></li><li class="footer__item"><a href="http://eepurl.com/hfkB9z" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe to our newsletter</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/assets/js/runtime~main.ecb5232a.js"></script>
<script src="/assets/js/main.2f4129da.js"></script>
</body>
</html>